"""
Application Interactive d'Anonymisation de Graphes Sociaux
Bas√©e sur la th√®se "Anonymisation de Graphes Sociaux" par NGUYEN Huu-Hiep

Application Streamlit avec s√©lection de m√©thodes et explications d√©taill√©es
"""

import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
import random
from copy import deepcopy
import io
import pandas as pd
from scipy.optimize import minimize
from scipy.sparse import csr_matrix
from method_details import ATTACKS_AND_GUARANTEES
from definitions_and_attacks import (
    ANONYMIZATION_DEFINITIONS,
    ATTACKS_DICTIONARY,
    GRAPH_PROPERTIES,
    CONCRETE_ATTACK_EXAMPLES
)
from thesis_references import (
    THESIS_REFERENCES,
    format_thesis_reference,
    get_method_references
)

# Configuration de la page
st.set_page_config(
    page_title="Anonymisation de Graphes Sociaux",
    page_icon="üîí",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Style CSS personnalis√©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 20px;
    }
    .method-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .metric-box {
        background-color: #e8f4f8;
        padding: 15px;
        border-radius: 5px;
        border-left: 5px solid #1f77b4;
    }
    .math-formula {
        background-color: #fff3cd;
        padding: 10px;
        border-radius: 5px;
        font-family: 'Courier New', monospace;
    }
</style>
""", unsafe_allow_html=True)


class GraphAnonymizer:
    """Classe pour anonymiser des graphes sociaux - VERSION √âQUILIBR√âE"""

    def __init__(self, graph):
        self.original_graph = graph.copy()
        self.n = graph.number_of_nodes()
        self.m = graph.number_of_edges()

    def random_add_del(self, k=20):
        """Random Add/Del optimis√© pour effet visible"""
        G = self.original_graph.copy()
        added = 0
        attempts = 0
        max_attempts = k * 100

        while added < k and attempts < max_attempts:
            u, v = random.sample(list(G.nodes()), 2)
            if not G.has_edge(u, v):
                G.add_edge(u, v)
                added += 1
            attempts += 1

        edges = list(self.original_graph.edges())
        if len(edges) >= k:
            edges_to_remove = random.sample(edges, min(k, len(edges)))
            for u, v in edges_to_remove:
                if G.has_edge(u, v):
                    G.remove_edge(u, v)

        return G

    def random_switch(self, k=25):
        """Random Switch optimis√©"""
        G = self.original_graph.copy()
        successful_switches = 0

        for _ in range(k * 3):  # Plus de tentatives
            if successful_switches >= k:
                break

            edges = list(G.edges())
            if len(edges) < 2:
                break

            (u, w), (v, x) = random.sample(edges, 2)

            if u != v and u != x and w != v and w != x:
                if not G.has_edge(u, v) and not G.has_edge(w, x):
                    G.remove_edge(u, w)
                    G.remove_edge(v, x)
                    G.add_edge(u, v)
                    G.add_edge(w, x)
                    successful_switches += 1

        return G

    def k_degree_anonymity(self, k=2):
        """
        k-degree anonymity : Garantit qu'au moins k n≈ìuds ont le m√™me degr√©.

        ALGORITHME EN LANGAGE NATUREL :
        1. Compter combien de n≈ìuds ont chaque degr√©
        2. Pour chaque degr√© avec MOINS de k n≈ìuds :
           - FUSIONNER ce groupe avec un groupe voisin (degr√© proche)
           - Ajuster les degr√©s en ajoutant/supprimant des ar√™tes
        3. R√©p√©ter jusqu'√† ce que TOUS les groupes aient au moins k n≈ìuds

        STRAT√âGIE :
        - Groupes trop petits ‚Üí Fusionner avec degr√© voisin
        - Ajouter des ar√™tes pour augmenter les degr√©s vers le degr√© cible
        - Garantir : chaque degr√© appara√Æt au moins k fois
        """
        G = self.original_graph.copy()

        # It√©rer jusqu'√† satisfaire la contrainte
        max_iterations = 100
        for iteration in range(max_iterations):
            # Calculer la distribution actuelle des degr√©s
            degrees = dict(G.degree())
            degree_counts = Counter(degrees.values())

            # Trouver les degr√©s qui violent la contrainte k
            violating_degrees = [d for d, count in degree_counts.items() if count < k]

            if not violating_degrees:
                # Contrainte satisfaite !
                break

            # Strat√©gie : Fusionner les petits groupes avec leurs voisins
            for degree in sorted(violating_degrees):
                nodes_with_degree = [n for n, d in degrees.items() if d == degree]

                if len(nodes_with_degree) == 0:
                    continue

                # Trouver le degr√© cible (degr√© voisin le plus fr√©quent)
                all_degrees = sorted(degree_counts.keys())

                # Chercher le degr√© voisin (sup√©rieur ou inf√©rieur)
                target_degree = None
                if degree < max(all_degrees):
                    # Augmenter vers le degr√© sup√©rieur
                    target_degree = min([d for d in all_degrees if d > degree])
                elif degree > min(all_degrees):
                    # Diminuer vers le degr√© inf√©rieur (en supprimant des ar√™tes)
                    target_degree = max([d for d in all_degrees if d < degree])
                else:
                    # Dernier recours : dupliquer le degr√© en ajoutant des ar√™tes
                    target_degree = degree + 1

                if target_degree is None:
                    continue

                # Ajuster les degr√©s des n≈ìuds pour atteindre target_degree
                for node in nodes_with_degree[:]:  # Copie pour √©viter modification pendant it√©ration
                    current_degree = G.degree(node)

                    if current_degree < target_degree:
                        # AUGMENTER le degr√© en ajoutant des ar√™tes
                        edges_to_add = target_degree - current_degree

                        for _ in range(edges_to_add):
                            # Trouver un n≈ìud non connect√©
                            candidates = [n for n in G.nodes()
                                        if n != node and not G.has_edge(node, n)]

                            if candidates:
                                # Pr√©f√©rer les n≈ìuds qui ont aussi besoin d'augmenter leur degr√©
                                target_node = random.choice(candidates)
                                G.add_edge(node, target_node)
                            else:
                                break  # Pas de candidat disponible

                    elif current_degree > target_degree:
                        # DIMINUER le degr√© en supprimant des ar√™tes
                        edges_to_remove = current_degree - target_degree

                        neighbors = list(G.neighbors(node))
                        edges_to_delete = random.sample(neighbors, min(edges_to_remove, len(neighbors)))

                        for neighbor in edges_to_delete:
                            G.remove_edge(node, neighbor)

                # Recalculer pour la prochaine it√©ration
                degrees = dict(G.degree())
                degree_counts = Counter(degrees.values())

        return G

    def generalization(self, k=4):
        """
        G√©n√©ralisation par clustering avec taille minimale k.

        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        PRINCIPE DE LA G√âN√âRALISATION (k-anonymity structurelle) :
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        - Regrouper les n≈ìuds en CLUSTERS (super-n≈ìuds) de taille ‚â• k
        - Chaque cluster repr√©sente au moins k n≈ìuds indistinguables
        - Les ar√™tes deviennent des super-ar√™tes entre clusters

        GARANTIE DE PRIVACY :
        - Un attaquant ne peut identifier un n≈ìud sp√©cifique dans un cluster
        - Probabilit√© de r√©-identification ‚â§ 1/k pour chaque n≈ìud

        PARAM√àTRE k :
        - Plus k est GRAND ‚Üí Clusters plus gros ‚Üí PLUS de privacy ‚Üí MOINS d'utilit√©
        - Plus k est PETIT ‚Üí Clusters plus petits ‚Üí MOINS de privacy ‚Üí PLUS d'utilit√©
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        """
        G = self.original_graph.copy()
        n = G.number_of_nodes()

        # Nombre de clusters bas√© sur k
        # On veut environ n/k clusters de taille k
        num_clusters = max(2, n // k)  # Au moins 2 clusters

        # Utiliser un algorithme de clustering spectral pour cr√©er num_clusters
        try:
            # Essayer d'abord avec la m√©thode des communaut√©s
            from networkx.algorithms import community

            # Utiliser label propagation qui converge rapidement
            communities_generator = community.label_propagation_communities(G)
            communities = list(communities_generator)

            # Si on a trop de clusters, fusionner les plus petits
            while len(communities) > num_clusters:
                # Trouver les 2 plus petits clusters
                communities = sorted(communities, key=len)
                smallest = communities[0]
                second_smallest = communities[1]
                # Fusionner
                merged = smallest.union(second_smallest)
                communities = [merged] + communities[2:]

            # Si on a trop peu de clusters, diviser les plus gros
            while len(communities) < num_clusters and any(len(c) >= 2*k for c in communities):
                # Trouver le plus gros cluster
                communities = sorted(communities, key=len, reverse=True)
                largest = communities[0]
                if len(largest) >= 2*k:
                    # Diviser en deux
                    largest_list = list(largest)
                    mid = len(largest_list) // 2
                    part1 = set(largest_list[:mid])
                    part2 = set(largest_list[mid:])
                    communities = [part1, part2] + communities[1:]
                else:
                    break

        except Exception as e:
            # Fallback : clustering simple par degr√©
            # Regrouper les n≈ìuds par degr√© similaire
            nodes_by_degree = {}
            for node in G.nodes():
                degree = G.degree(node)
                if degree not in nodes_by_degree:
                    nodes_by_degree[degree] = []
                nodes_by_degree[degree].append(node)

            # Cr√©er des clusters de taille k
            communities = []
            current_cluster = set()
            for degree in sorted(nodes_by_degree.keys()):
                for node in nodes_by_degree[degree]:
                    current_cluster.add(node)
                    if len(current_cluster) >= k:
                        communities.append(current_cluster)
                        current_cluster = set()

            # Ajouter le dernier cluster s'il existe
            if current_cluster:
                if communities:
                    # Fusionner avec le dernier cluster
                    communities[-1] = communities[-1].union(current_cluster)
                else:
                    communities.append(current_cluster)

        # Assurer que tous les clusters ont au moins k n≈ìuds
        final_communities = []
        buffer = set()

        for community in sorted(communities, key=len, reverse=True):
            community = set(community).union(buffer)
            buffer = set()

            if len(community) >= k:
                final_communities.append(community)
            else:
                # Trop petit, mettre en buffer pour fusionner avec le prochain
                buffer = community

        # Si reste un buffer, fusionner avec le dernier cluster
        if buffer and final_communities:
            final_communities[-1] = final_communities[-1].union(buffer)
        elif buffer:
            final_communities.append(buffer)

        communities = final_communities

        # Cr√©er le super-graphe
        super_graph = nx.Graph()
        node_to_cluster = {}
        cluster_to_nodes = {}
        cluster_id = 0

        for community in communities:
            community = set(community)

            for node in community:
                node_to_cluster[node] = cluster_id

            super_graph.add_node(cluster_id, cluster_size=len(community), size=len(community),
                               nodes=list(community), internal_edges=0)
            cluster_to_nodes[cluster_id] = list(community)
            cluster_id += 1

        # Compter les ar√™tes intra et inter-clusters
        intra_edges = 0
        inter_edges = 0

        # Ajouter les super-ar√™tes
        for u, v in G.edges():
            cluster_u = node_to_cluster.get(u)
            cluster_v = node_to_cluster.get(v)

            if cluster_u is not None and cluster_v is not None:
                if cluster_u != cluster_v:
                    inter_edges += 1
                    if super_graph.has_edge(cluster_u, cluster_v):
                        super_graph[cluster_u][cluster_v]['weight'] += 1
                    else:
                        super_graph.add_edge(cluster_u, cluster_v, weight=1)
                else:
                    # Self-loops pour les ar√™tes internes
                    intra_edges += 1
                    # Incr√©menter le compteur d'ar√™tes internes du n≈ìud
                    super_graph.nodes[cluster_u]['internal_edges'] += 1
                    if super_graph.has_edge(cluster_u, cluster_u):
                        super_graph[cluster_u][cluster_u]['weight'] += 1
                    else:
                        super_graph.add_edge(cluster_u, cluster_u, weight=1)

        # Stocker les statistiques
        super_graph.graph['intra_edges'] = intra_edges
        super_graph.graph['inter_edges'] = inter_edges
        super_graph.graph['cluster_to_nodes'] = cluster_to_nodes
        super_graph.graph['node_to_cluster'] = node_to_cluster

        return super_graph, node_to_cluster

    def probabilistic_obfuscation(self, k=5, epsilon=0.3):
        """
        (k,Œµ)-obfuscation selon l'algorithme de Boldi et al. (2012)

        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        ALGORITHME DE BOLDI ET AL. (Distribution Normale Tronqu√©e)
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        Au lieu de la formule simplifi√©e (p = 1 - Œµ/k), cet algorithme utilise
        une DISTRIBUTION NORMALE TRONQU√âE R_œÉ sur [0,1] pour assigner les
        probabilit√©s, conform√©ment √† la publication originale.

        PROCESSUS :
        1. Pour chaque n≈ìud v, identifier k voisins candidats N_k(v)
        2. Tirer k valeurs de R_œÉ (normale tronqu√©e centr√©e)
        3. Normaliser pour obtenir une distribution de probabilit√©
        4. Garantir H(N_k(v)) ‚â• log(k) - Œµ

        AVANTAGE : Distribution plus r√©aliste, mod√©lise mieux l'incertitude
        INCONV√âNIENT : Toujours vuln√©rable au threshold attack ‚Üí voir MaxVar

        R√©f√©rence : Boldi et al., "Injecting Uncertainty in Graphs for Identity
        Obfuscation", VLDB 2012 (voir th√®se p.70-72)
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        """
        from scipy.stats import truncnorm

        G = self.original_graph.copy()
        prob_graph = nx.Graph()
        prob_graph.add_nodes_from(G.nodes())

        # Param√®tre de la distribution normale tronqu√©e
        # œÉ est choisi pour cr√©er une distribution centr√©e avec variance adapt√©e
        # Plus œÉ est grand, plus la distribution est √©tal√©e
        sigma = 0.15 + (epsilon / k) * 0.2  # Ajust√© en fonction de Œµ et k

        # Pr√©parer la distribution normale tronqu√©e sur [0, 1]
        # truncnorm utilise les bornes standardis√©es (a, b) = ((lower - mu)/sigma, (upper - mu)/sigma)
        a_trunc = (0 - 0.5) / sigma  # Borne inf√©rieure standardis√©e
        b_trunc = (1 - 0.5) / sigma  # Borne sup√©rieure standardis√©e

        # Pour chaque n≈ìud, on va assigner des probabilit√©s √† ses voisins candidats
        all_candidate_edges = []

        # D'abord, collecter tous les voisins candidats pour chaque n≈ìud
        for node in G.nodes():
            # Voisins existants
            existing_neighbors = list(G.neighbors(node))

            # Voisins potentiels (non-voisins)
            all_nodes = set(G.nodes())
            potential_neighbors = list(all_nodes - set(existing_neighbors) - {node})

            # S√©lectionner k voisins candidats total
            # Prioriser les voisins existants, compl√©ter avec potentiels
            num_existing = len(existing_neighbors)
            num_potential_needed = max(0, k - num_existing)

            if len(potential_neighbors) > 0 and num_potential_needed > 0:
                num_potential_to_add = min(num_potential_needed, len(potential_neighbors))
                selected_potential = random.sample(potential_neighbors, num_potential_to_add)
            else:
                selected_potential = []

            candidates = existing_neighbors + selected_potential

            # Tirer k valeurs de la distribution normale tronqu√©e
            num_candidates = len(candidates)
            if num_candidates > 0:
                # G√©n√©rer des probabilit√©s via distribution normale tronqu√©e
                raw_probs = truncnorm.rvs(a_trunc, b_trunc, loc=0.5, scale=sigma, size=num_candidates)

                # Normaliser pour avoir une distribution de probabilit√©
                # (optionnel selon l'interpr√©tation, mais aide √† l'interpr√©tation)
                prob_sum = np.sum(raw_probs)
                if prob_sum > 0:
                    normalized_probs = raw_probs / prob_sum
                else:
                    normalized_probs = np.ones(num_candidates) / num_candidates

                # Assigner les probabilit√©s aux ar√™tes
                for i, neighbor in enumerate(candidates):
                    u, v = (node, neighbor) if node < neighbor else (neighbor, node)
                    is_original = G.has_edge(node, neighbor)

                    # Utiliser les probabilit√©s brutes (non normalis√©es) pour mieux refl√©ter l'algorithme
                    # Les probabilit√©s individuelles sont dans [0,1]
                    prob = float(raw_probs[i])

                    # Ne pas dupliquer les ar√™tes
                    if not prob_graph.has_edge(u, v):
                        prob_graph.add_edge(u, v, probability=prob, is_original=is_original)

        return prob_graph

    def differential_privacy_edgeflip(self, epsilon=0.8):
        """
        EdgeFlip avec epsilon-differential privacy.

        FORMULE CORRECTE : s = 2 / (e^epsilon + 1)

        Trade-off :
        - epsilon PETIT => s GRAND => beaucoup de flips => FORTE privacy
        - epsilon GRAND => s PETIT => peu de flips => FAIBLE privacy
        """
        G = nx.Graph()
        G.add_nodes_from(self.original_graph.nodes())

        # FORMULE CORRECTE (d√©riv√©e du ratio de DP)
        s = 2 / (np.exp(epsilon) + 1)

        for u in self.original_graph.nodes():
            for v in self.original_graph.nodes():
                if u < v:
                    exists = self.original_graph.has_edge(u, v)

                    if random.random() < s/2:
                        if not exists:
                            G.add_edge(u, v)
                    else:
                        if exists:
                            G.add_edge(u, v)

        return G

    def differential_privacy_laplace(self, epsilon=1.2):
        """M√©canisme de Laplace optimis√©"""
        G = self.original_graph.copy()
        sensitivity = 1
        scale = sensitivity / epsilon

        new_graph = nx.Graph()
        new_graph.add_nodes_from(G.nodes())

        for u in G.nodes():
            for v in G.nodes():
                if u < v:
                    true_value = 1 if G.has_edge(u, v) else 0
                    noise = np.random.laplace(0, scale)
                    noisy_value = true_value + noise

                    if noisy_value > 0.5:
                        new_graph.add_edge(u, v)

        return new_graph

    def maxvar_obfuscation(self, num_potential_edges=50):
        """
        MaxVar: Variance Maximizing Scheme

        Am√©lioration de (k,Œµ)-obfuscation qui r√©sout le probl√®me de reconstruction
        par seuillage en maximisant la variance totale des degr√©s.

        ALGORITHME (Nguyen Huu-Hiep, 2016):
        1. Ajouter des ar√™tes potentielles "nearby" (distance 2, friend-of-friend)
        2. R√©soudre un programme quadratique pour assigner les probabilit√©s:
           - Minimiser: Œ£ p_i¬≤ (√©quivalent √† maximiser la variance)
           - Contrainte: Œ£ p_uv = degree(u) pour chaque n≈ìud u
        3. Les probabilit√©s r√©sultantes sont DISPERS√âES (pas concentr√©es √† 0/1)

        AVANTAGE vs (k,Œµ)-obf:
        - Pas de reconstruction par seuillage!
        - Probabilit√©s varient significativement autour de 0.5
        - Pr√©servation exacte des degr√©s attendus
        """
        G0 = self.original_graph.copy()
        n = G0.number_of_nodes()

        # Phase 1: Ajouter des ar√™tes potentielles "nearby" (distance = 2)
        potential_edges = []
        for u in G0.nodes():
            # Trouver les voisins √† distance 2 (friend-of-friend)
            neighbors_dist_2 = set()
            for neighbor in G0.neighbors(u):
                for neighbor2 in G0.neighbors(neighbor):
                    if neighbor2 != u and not G0.has_edge(u, neighbor2):
                        neighbors_dist_2.add(neighbor2)

            # Ajouter des ar√™tes potentielles vers ces voisins
            for v in neighbors_dist_2:
                if u < v:  # √âviter les doublons
                    potential_edges.append((u, v))

        # Limiter le nombre d'ar√™tes potentielles
        if len(potential_edges) > num_potential_edges:
            potential_edges = random.sample(potential_edges, num_potential_edges)

        # Cr√©er le graphe √©tendu avec ar√™tes existantes + potentielles
        all_edges = list(G0.edges()) + potential_edges
        edge_to_idx = {edge: idx for idx, edge in enumerate(all_edges)}
        m = len(all_edges)

        # Phase 2: Formulation du programme quadratique
        # Objectif: Minimiser Œ£ p_i¬≤
        # Contrainte: Œ£ p_uv = degree(u) pour chaque n≈ìud u

        # Construire la matrice de contraintes d'√©galit√© (A_eq)
        # Chaque ligne correspond √† un n≈ìud, chaque colonne √† une ar√™te
        A_eq = np.zeros((n, m))
        b_eq = np.zeros(n)

        node_to_idx = {node: idx for idx, node in enumerate(G0.nodes())}

        for node in G0.nodes():
            node_idx = node_to_idx[node]
            b_eq[node_idx] = G0.degree(node)  # Degr√© attendu = degr√© original

            # Pour chaque ar√™te touchant ce n≈ìud
            for u, v in all_edges:
                if u == node or v == node:
                    edge_idx = edge_to_idx[(u, v)]
                    A_eq[node_idx, edge_idx] = 1.0

        # Fonction objectif: f(p) = Œ£ p_i¬≤
        def objective(p):
            return np.sum(p ** 2)

        # Gradient: f'(p) = 2p
        def gradient(p):
            return 2 * p

        # Contraintes d'√©galit√©
        constraints = {'type': 'eq', 'fun': lambda p: A_eq @ p - b_eq}

        # Bornes: 0 ‚â§ p_i ‚â§ 1
        bounds = [(0.0, 1.0) for _ in range(m)]

        # Point initial: probabilit√© uniforme qui satisfait les contraintes
        # Ar√™tes existantes: prob = 1.0, ar√™tes potentielles: prob faible
        p0 = np.zeros(m)
        for idx, (u, v) in enumerate(all_edges):
            if G0.has_edge(u, v):
                p0[idx] = 0.8  # Ar√™te existante: prob √©lev√©e mais pas 1.0
            else:
                p0[idx] = 0.2  # Ar√™te potentielle: prob faible mais pas 0.0

        # R√©soudre le programme quadratique
        result = minimize(
            objective,
            p0,
            method='SLSQP',
            jac=gradient,
            bounds=bounds,
            constraints=constraints,
            options={'maxiter': 1000, 'ftol': 1e-6}
        )

        if not result.success:
            # Si l'optimisation √©choue, utiliser p0
            probabilities = p0
        else:
            probabilities = result.x

        # Phase 3: Cr√©er le graphe incertain avec les probabilit√©s optimis√©es
        prob_graph = nx.Graph()
        prob_graph.add_nodes_from(G0.nodes())

        for idx, (u, v) in enumerate(all_edges):
            prob = probabilities[idx]
            is_original = G0.has_edge(u, v)

            # Ajouter toutes les ar√™tes avec leur probabilit√©
            prob_graph.add_edge(u, v, probability=prob, is_original=is_original)

        return prob_graph


# D√©finitions des m√©thodes avec explications
METHODS = {
    "Random Add/Del": {
        "name": "Randomisation - Random Add/Del",
        "category": "1. Anonymisation par Randomisation",
        "params": {"k": 20},
        "description_short": "Ajoute k fausses ar√™tes puis supprime k vraies ar√™tes al√©atoirement",
        "description": """
### Principe en Langage Naturel

La m√©thode **Random Add/Del** est l'une des plus simples. Elle fonctionne en deux √©tapes :
1. **Ajout** : On ajoute k ar√™tes al√©atoires entre des n≈ìuds non connect√©s
2. **Suppression** : On supprime k ar√™tes existantes choisies au hasard

Cette approche cr√©e de l'incertitude en modifiant la structure du graphe de mani√®re al√©atoire.
Un attaquant qui conna√Ætrait le degr√© d'un n≈ìud ne pourra plus le retrouver avec certitude
car les degr√©s ont chang√©.

### Formalisation Math√©matique

Soit G = (V, E) le graphe original.

**Algorithme** :
```
1. Pour i = 1 √† k :
   - Choisir (u, v) ‚àà V √ó V tel que (u,v) ‚àâ E
   - E ‚Üê E ‚à™ {(u,v)}

2. Pour i = 1 √† k :
   - Choisir (u, v) ‚àà E uniform√©ment
   - E ‚Üê E \\ {(u,v)}

3. Retourner G' = (V, E)
```

**Propri√©t√©s** :
- Nombre d'ar√™tes pr√©serv√© : |E'| = |E|
- Distribution des degr√©s modifi√©e
- Pas de garantie formelle de privacy

**Complexit√©** : O(k)
        """,
        "formula": r"P(edge_{added}) = \frac{k}{|V|(|V|-1)/2 - |E|}, \quad P(edge_{removed}) = \frac{k}{|E|}",
        "privacy_level": "Faible (pas de garantie formelle)",
        "utility_preservation": "Moyenne √† Bonne"
    },

    "Random Switch": {
        "name": "Randomisation - Random Switch",
        "category": "1. Anonymisation par Randomisation",
        "params": {"k": 25},
        "description_short": "√âchange k paires d'ar√™tes en pr√©servant les degr√©s",
        "description": """
### Principe en Langage Naturel

**Random Switch** am√©liore Random Add/Del en pr√©servant une propri√©t√© importante : **les degr√©s des n≈ìuds**.

Au lieu d'ajouter/supprimer des ar√™tes ind√©pendamment, on **√©change** des ar√™tes :
- On choisit deux ar√™tes (u,w) et (v,x)
- On les remplace par (u,v) et (w,x)
- Si ces nouvelles ar√™tes n'existent pas d√©j√†

Ainsi, chaque n≈ìud conserve exactement le m√™me nombre de connexions, mais ces connexions
pointent vers d'autres n≈ìuds. C'est comme si on "r√©arrangeait" les liens sociaux sans
changer le nombre d'amis de chacun.

### Formalisation Math√©matique

**Algorithme** :
```
Pour i = 1 √† k :
  1. Choisir (u,w), (v,x) ‚àà E uniform√©ment
  2. Si u ‚â† v ‚â† w ‚â† x et (u,v) ‚àâ E et (w,x) ‚àâ E :
     - E ‚Üê E \\ {(u,w), (v,x)}
     - E ‚Üê E ‚à™ {(u,v), (w,x)}

Retourner G' = (V, E)
```

**Invariants pr√©serv√©s** :
- S√©quence de degr√©s : deg_G'(v) = deg_G(v) ‚àÄv ‚àà V
- Nombre d'ar√™tes : |E'| = |E|

**Propri√©t√© cl√©** : Les chemins et la structure globale sont modifi√©s tout en
pr√©servant les propri√©t√©s locales (degr√©s).

**Complexit√©** : O(k)
        """,
        "formula": r"deg_{G'}(v) = deg_G(v) \quad \forall v \in V",
        "privacy_level": "Faible √† Moyenne",
        "utility_preservation": "Tr√®s Bonne (degr√©s pr√©serv√©s)"
    },

    "k-degree anonymity": {
        "name": "K-Anonymisation - k-degree anonymity",
        "category": "2. K-Anonymisation",
        "params": {"k": 2},
        "description_short": "Garantit que chaque degr√© appara√Æt au moins k fois",
        "description": """
### Principe en Langage Naturel

La **k-degree anonymity** fournit une garantie formelle : chaque n≈ìud doit √™tre
**indistinguable** d'au moins k-1 autres n≈ìuds en termes de degr√©.

**Intuition** : Si un attaquant conna√Æt le degr√© d'un n≈ìud cible (ex: 5 amis),
il doit y avoir au moins k n≈ìuds avec ce m√™me degr√©. L'attaquant ne peut donc
identifier le n≈ìud cible qu'avec une probabilit√© $\\leq 1/k$.

**Exemple** : Avec k=3, si Alice a 7 amis, on s'assure qu'au moins 2 autres
personnes ont aussi 7 amis. L'attaquant ne peut pas dire laquelle est Alice.

L'algorithme ajoute des ar√™tes de mani√®re **d√©terministe** pour atteindre cette propri√©t√©.

### Formalisation Math√©matique

**D√©finition formelle** :

Un graphe G = (V, E) satisfait la k-degree anonymity si :

$$\\forall d \\in \\{\\deg(v) : v \\in V\\}, |\\{v \\in V : \\deg(v) = d\\}| \\geq k$$

C'est-√†-dire : pour tout degr√© d qui appara√Æt dans le graphe,
il doit y avoir au moins k n≈ìuds avec ce degr√©.

**Algorithme** :
```
Entr√©e : G = (V, E), k
Sortie : G' = (V, E') satisfaisant k-degree anonymity

1. Calculer la s√©quence de degr√©s D = [deg(v) : v ‚àà V]
2. Pour chaque degr√© d apparaissant moins de k fois :
   - Identifier les n≈ìuds V_d = {v : deg(v) = d}
   - Ajouter des ar√™tes pour augmenter/uniformiser les degr√©s
3. Retourner G'
```

### Heuristique Impl√©ment√©e

L'impl√©mentation utilise une **strat√©gie greedy it√©rative** qui fusionne les groupes de degr√©s trop petits :

**√âtape 1 - D√©tection des violations** :
```
R√âP√âTER jusqu'√† convergence (max 100 it√©rations):
  1. Calculer la distribution des degr√©s
  2. Identifier les degr√©s "violants" : count(d) < k
  3. SI aucun violant ‚Üí STOP (contrainte satisfaite)
  4. SINON ‚Üí Passer √† l'√©tape 2
```

**√âtape 2 - Fusion vers degr√© cible** :

Pour chaque degr√© violant d avec count(d) < k :
```
SI d < max(all_degrees):
  ‚Üí AUGMENTER vers le degr√© sup√©rieur le plus proche
  ‚Üí target_degree = min([degr√©s > d])

SINON SI d > min(all_degrees):
  ‚Üí DIMINUER vers le degr√© inf√©rieur le plus proche
  ‚Üí target_degree = max([degr√©s < d])

SINON (dernier recours):
  ‚Üí Cr√©er un nouveau groupe √† d+1
```

**√âtape 3 - Ajustement des degr√©s** :
```
Pour AUGMENTER un degr√© (current < target):
  1. Chercher candidats = [n≈ìuds NON connect√©s]
  2. S√©lection al√©atoire parmi candidats
  3. Ajouter ar√™te (node, candidat)

Pour DIMINUER un degr√© (current > target):
  1. Lister les voisins du n≈ìud
  2. √âchantillonner al√©atoirement les ar√™tes √† supprimer
  3. Supprimer les ar√™tes s√©lectionn√©es
```

**Exemple d'ex√©cution** (Karate Club, k=2) :
- Distribution originale : {1: 1, 2: 11, 3: 6, 9: 1, 10: 1, 12: 1, 16: 1, 17: 1}
  - Degr√©s violants : 1, 9, 10, 12, 16, 17 (< 2 occurrences)
- Apr√®s anonymisation : {1: 6, 2: 7, 3: 8, 4: 2, 5: 2, 8: 9}
  - ‚úÖ Tous les degr√©s $\\geq$ 2 occurrences
  - Modification : -14.1% d'ar√™tes

**Propri√©t√©s** :
- ‚úÖ **Garantit** la contrainte k-anonymity
- ‚úÖ **Minimise** les modifications (fusion vers voisin proche)
- ‚ö†Ô∏è **Non-optimal** (peut modifier plus que le minimum th√©orique)
- ‚ö†Ô∏è **Randomis√©** (r√©sultats non d√©terministes)

**Garantie de privacy** :

$$P(\\text{identit√© de } v | \\deg(v) = d) \\leq \\frac{1}{k}$$

**NP-compl√©tude** : Trouver le nombre minimum d'ar√™tes √† ajouter est NP-difficile.

**Complexit√©** : O(n¬≤) en pratique (it√©rations √ó ajustements)

---

üìñ **R√©f√©rences Th√®se:**
- **p.30** - Section 2.2: k-Anonymity pour les graphes
- **p.32** - Section 2.2.1: k-Degree Anonymity - d√©finition et algorithmes
- **p.45** - Section 2.5: Mod√®les d'attaques
        """,
        "formula": r"|\{v \in V : deg(v) = d\}| \geq k \quad \forall d",
        "privacy_level": "Moyenne √† Forte (garantie k-anonymity)",
        "utility_preservation": "Bonne"
    },

    "Generalization": {
        "name": "G√©n√©ralisation - Super-nodes",
        "category": "3. Anonymisation par G√©n√©ralisation",
        "params": {"k": 4},
        "description_short": "Regroupe les n≈ìuds en super-n≈ìuds de taille $\\geq k$",
        "description": """
### Principe en Langage Naturel

La **g√©n√©ralisation** adopte une approche radicalement diff√©rente : au lieu de modifier
les ar√™tes, on **regroupe** les n≈ìuds similaires en "super-n≈ìuds".

**Analogie** : C'est comme publier des statistiques par d√©partement plut√¥t que par personne.
- Au lieu de "Alice (Paris) connect√©e √† Bob (Lyon)"
- On dit "R√©gion √éle-de-France (10000 personnes) connect√©e √† R√©gion Auvergne-Rh√¥ne-Alpes (5000 personnes)"

**Avantages** :
- Protection maximale de l'identit√© individuelle
- R√©duction de la taille du graphe publi√©
- Chaque individu est "cach√©" dans un groupe de k personnes minimum

**Inconv√©nient** : Perte importante d'information structurelle fine.

### Algorithme de Clustering Impl√©ment√©

L'impl√©mentation utilise une approche **Label Propagation** avec ajustements pour garantir $|C_i| \\geq k$ :

**√âtape 1 - Clustering initial** :
- Utilisation de l'algorithme **Label Propagation** (propagation d'√©tiquettes)
- Chaque n≈ìud adopte l'√©tiquette la plus fr√©quente parmi ses voisins
- Converge rapidement vers des communaut√©s naturelles du graphe

**√âtape 2 - Ajustement des tailles** :
```
TANT QUE il existe des clusters < k :
  SI trop de clusters :
    ‚Üí Fusionner les 2 plus petits clusters
  SI pas assez de clusters ET un cluster ‚â• 2k :
    ‚Üí Diviser le plus gros cluster en deux
```

**Avantages de cette approche** :
- ‚úÖ Respecte la structure naturelle du graphe (communaut√©s)
- ‚úÖ Garantit la contrainte $|C_i| \\geq k$
- ‚úÖ Temps de calcul raisonnable O(n¬≤)

**Alternative possible** : Clustering spectral, k-means sur les embeddings, ou partitionnement par degr√©.

### Formalisation Math√©matique

**Mod√®le de graphe g√©n√©ralis√©** :

Soit G = (V, E) le graphe original. On cr√©e une partition P = {C‚ÇÅ, C‚ÇÇ, ..., C‚Çò}
de V telle que $|C_i| \\geq k \\; \\forall i$.

Le **super-graphe** G* = (V*, E*) est d√©fini par :
- V* = {C‚ÇÅ, C‚ÇÇ, ..., C‚Çò} (les clusters)
- $E^* = \\{(C_i, C_j) : \\exists(u,v) \\in E \\text{ avec } u \\in C_i, v \\in C_j\\}$

Chaque super-ar√™te (C·µ¢, C‚±º) a un **poids** :

$$w(C_i, C_j) = |\\{(u,v) \\in E : u \\in C_i, v \\in C_j\\}|$$

**Probabilit√© d'ar√™te dans le cluster** :

$$P(\\text{edge} | C_i, C_j) = \\frac{w(C_i, C_j)}{|C_i| \\times |C_j|}$$

**Garantie de privacy** : Un individu est cach√© parmi au moins k-1 autres
dans son cluster.

**Probl√®me d'optimisation** : Trouver la partition P qui minimise la perte
d'information tout en respectant $|C_i| \\geq k$ est NP-difficile.

**Complexit√©** : O(n¬≤) √† O(n¬≥) selon l'algorithme de clustering

---

üìñ **R√©f√©rences Th√®se:**
- **p.40** - Section 2.3: G√©n√©ralisation par super-nodes
- **p.30** - Section 2.2: Fondements de la k-anonymity
        """,
        "formula": r"G^* = (V^*, E^*) \text{ o√π } V^* = \{C_i : |C_i| \geq k\}",
        "privacy_level": "Forte (k-anonymity structurelle)",
        "utility_preservation": "Faible √† Moyenne"
    },

    "Probabilistic": {
        "name": "Probabiliste - (k,Œµ)-obfuscation",
        "category": "4. Approches Probabilistes",
        "params": {"k": 5, "epsilon": 0.3},
        "description_short": "Cr√©e un graphe incertain avec probabilit√©s sur les ar√™tes",
        "description": """
### Principe en Langage Naturel

Les approches **probabilistes** cr√©ent un "graphe incertain" o√π chaque ar√™te existe
avec une certaine **probabilit√©**.

**Id√©e cl√©** : Au lieu de publier un graphe d√©terministe (ar√™te = oui/non), on publie
des probabilit√©s. Par exemple :
- Ar√™te (Alice, Bob) : 95% de probabilit√© d'exister
- Ar√™te (Alice, Charlie) : 20% de probabilit√© d'exister

Un attaquant ne peut plus √™tre certain de rien : m√™me les vraies ar√™tes ont une incertitude.

**Mod√®le (k,Œµ)-obfuscation** :
- **k** : niveau d'anonymisation souhait√© (plus k est grand, plus de protection)
- **Œµ** : param√®tre de tol√©rance (plus Œµ est petit, plus de protection)

### Formalisation Math√©matique

**Graphe incertain** :

Un graphe incertain est un triplet GÃÉ = (V, E, p) o√π :
- V : ensemble de n≈ìuds
- E : ensemble d'ar√™tes (r√©elles + potentielles)
- p : E ‚Üí [0,1] fonction de probabilit√©

**D√©finition (k,Œµ)-obfuscation** :

Pour tout n≈ìud v ‚àà V, l'entropie de Shannon de la distribution
de probabilit√© sur les k voisins candidats doit √™tre $\\geq \\log(k) - \\varepsilon$ :

$$H(N_k(v)) = -\\sum_i p_i \\log(p_i) \\geq \\log(k) - \\varepsilon$$

o√π $N_k(v)$ sont les k n≈ìuds les plus susceptibles d'√™tre voisins de v.

### Algorithme de Boldi et al. (Distribution Normale Tronqu√©e)

**Approche impl√©ment√©e** : Au lieu d'utiliser les formules simplifi√©es ci-dessus, cette application
impl√©mente l'**algorithme original de Boldi et al. (2012)** qui utilise une **distribution normale tronqu√©e** $R_\sigma$
sur l'intervalle $[0,1]$.

**Processus de construction** :

1. **Identification des candidats** : Pour chaque n≈ìud $v$, identifier $N_k(v)$ (k voisins candidats : existants + potentiels)

2. **G√©n√©ration des probabilit√©s** : Tirer $k$ valeurs d'une distribution normale tronqu√©e $R_\sigma$ centr√©e sur $[0,1]$
   - $\sigma$ est l'√©cart-type, ajust√© en fonction de $\varepsilon$ et $k$
   - Plus $\sigma$ est grand, plus la distribution est √©tal√©e

3. **Assignation** : Assigner ces probabilit√©s tir√©es aux ar√™tes candidates

4. **V√©rification** : S'assurer que la contrainte d'entropie est respect√©e :
   $$H(N_k(v)) = -\\sum_i p_i \\log(p_i) \\geq \\log(k) - \\varepsilon$$

**Pourquoi cette approche ?**
- ‚úÖ **Plus r√©aliste** : Distribution naturelle vs formule uniforme
- ‚úÖ **Conforme √† la publication** : Suit Boldi et al. 2012
- ‚úÖ **Mod√©lisation de l'incertitude** : Probabilit√©s vari√©es au lieu de 2 valeurs fixes
- ‚ö†Ô∏è **Toujours vuln√©rable** : Au threshold attack (voir MaxVar pour la solution)

**Formules simplifi√©es (pour r√©f√©rence)** :
- Ar√™tes existantes : $p = 1 - \varepsilon/k$ (g√©n√©ralement ‚âà 0.9-1.0)
- Ar√™tes potentielles : $p = \varepsilon/(2k)$ (g√©n√©ralement ‚âà 0.0-0.1)

**Graphe d'exemple (sample graph)** :

√Ä partir de GÃÉ, on peut g√©n√©rer des graphes compatibles en √©chantillonnant :

$$G_{sample} = (V, E_{sample}) \\text{ o√π } e \\in E_{sample} \\text{ ssi } X_e \\leq p(e), X_e \\sim U[0,1]$$

**Propri√©t√©** : L'esp√©rance des degr√©s est pr√©serv√©e.

**Complexit√©** : O(|E| + k¬∑n)

### ‚ö†Ô∏è **LIMITATION CRITIQUE : Reconstruction par Seuillage**

L'impl√©mentation actuelle de (k,Œµ)-obfuscation a une **faille majeure** :

**Probl√®me** :
- Ar√™tes existantes : probabilit√© $\\approx$ 1.0
- Ar√™tes potentielles : probabilit√© $\\approx$ 0.0
- **Un attaquant peut appliquer un seuil √† 0.5 et r√©cup√©rer EXACTEMENT le graphe original!**

**Pourquoi ?** Comme mentionn√© dans la th√®se (Section 3.3.3) :
> "With small values of Œµ, re highly concentrates around zero, so existing sampled
> edges have probabilities nearly 1 and non-existing sampled edges are assigned
> probabilities almost 0. **Simple rounding techniques can easily reveal the true graph.**"

**Solution** : Utiliser **MaxVar** (voir ci-dessous) qui maximise la variance des
probabilit√©s pour √©viter cette concentration autour de 0/1.

**Utilit√© p√©dagogique** : Cette m√©thode est conserv√©e dans l'application pour
montrer l'importance de la **conception d'algorithmes** en privacy. Une formulation
math√©matique correcte ne garantit pas une impl√©mentation s√©curis√©e!

---

üìñ **R√©f√©rences Th√®se:**
- **p.70** - Section 3.3: D√©finition de la (k,Œµ)-obfuscation
- **p.72** - Section 3.3.2: Formules d'assignation des probabilit√©s
- **p.75** - Section 3.3.3: ‚ö†Ô∏è Vuln√©rabilit√© au threshold attack
        """,
        "formula": r"H(N_k(v)) = -\sum_i p_i \log(p_i) \geq \log(k) - \varepsilon",
        "privacy_level": "‚ö†Ô∏è FAIBLE (vuln√©rable au seuillage) - Voir MaxVar",
        "utility_preservation": "Bonne (esp√©rance pr√©serv√©e)"
    },

    "MaxVar": {
        "name": "Probabiliste - MaxVar (Variance Maximizing)",
        "category": "4. Approches Probabilistes",
        "params": {"num_potential_edges": 50},
        "description_short": "Graphe incertain avec probabilit√©s dispers√©es (r√©siste au seuillage)",
        "description": """
### Principe en Langage Naturel

**MaxVar** est une am√©lioration de (k,Œµ)-obfuscation qui r√©sout le **probl√®me de reconstruction par seuillage**.

**Id√©e cl√©** : Au lieu de minimiser Œµ (ce qui concentre les probabilit√©s autour de 0/1),
on **maximise la variance totale des degr√©s** tout en pr√©servant les degr√©s attendus.

**R√©sultat** : Les probabilit√©s sont **dispers√©es** autour de 0.5, rendant impossible
la reconstruction du graphe original par simple seuillage!

üí° **Note sur la distance d'√©dition** : MaxVar propose des ar√™tes "nearby" (√† distance 2) au lieu
d'ar√™tes al√©atoires, ce qui **minimise la distance d'√©dition** entre le graphe original et les graphes
√©chantillonn√©s. Cela pr√©serve mieux la structure locale du graphe (voir m√©trique "Distance d'√âdition"
dans les propri√©t√©s pour plus de d√©tails).

**Analogie** : Imaginons que vous voulez cacher quelle porte est la vraie parmi 10 portes :
- **(k,Œµ)-obf** : Porte vraie = 99% de chance, portes fausses = 1% ‚Üí **Trop √©vident!**
- **MaxVar** : Toutes les portes ont des probabilit√©s vari√©es entre 30% et 70% ‚Üí **Confusion maximale!**

### Formalisation Math√©matique

**Programme Quadratique** :

L'algorithme r√©sout un programme d'optimisation quadratique :

$$\\min \\sum_{i \\in E} p_i^2$$

Contraintes:

$$0 \\leq p_i \\leq 1 \\quad \\forall i \\in E$$

$$\\sum_{v \\in N(u)} p_{uv} = \\deg(u) \\quad \\forall u \\in V$$

o√π $E$ contient √† la fois les ar√™tes existantes ET les ar√™tes potentielles.

**Pourquoi minimiser $\\sum p_i^2$?**

La variance de la distance d'√©dition (Th√©or√®me 3.3, th√®se) est :

$$\\text{Var}[D(\\tilde{G}, G)] = \\sum_i p_i(1 - p_i) = |E_{\\text{original}}| - \\sum_i p_i^2$$

Donc **minimiser $\\sum p_i^2$** √©quivaut √† **maximiser la variance**, ce qui maximise
l'incertitude sur le graphe!

**Algorithme (3 phases)** :

**Phase 1 - Proposition d'ar√™tes "nearby"** :
```
Pour chaque n≈ìud u :
  1. Trouver les voisins √† distance 2 (friend-of-friend)
  2. Ajouter des ar√™tes potentielles vers ces voisins
  3. Limiter le nombre total d'ar√™tes potentielles
```

**Observation cl√©** : Proposer des ar√™tes "nearby" (distance 2) minimise la distorsion
structurelle tout en maximisant la confusion. C'est plus plausible qu'ajouter des ar√™tes
al√©atoires entre n≈ìuds distants!

**Phase 2 - Optimisation quadratique** :
```
1. Construire la matrice A_eq : ligne u, colonne (u,v) ‚Üí 1
2. Vecteur b_eq : degree(u) pour chaque n≈ìud u
3. R√©soudre: min Œ£ p¬≤ sous contrainte A_eq @ p = b_eq
4. Utiliser SLSQP (Sequential Least Squares Programming)
```

**Phase 3 - Publication** :
```
1. Cr√©er le graphe incertain GÃÉ = (V, E, p)
2. Publier plusieurs graphes √©chantillons G_sample
3. Chaque ar√™te e ‚àà E_sample si X_e ‚â§ p(e), X_e ~ U[0,1]
```

**Propri√©t√©s math√©matiques** :

1. **Conservation des degr√©s attendus** :
   $$\\mathbb{E}[\\deg(u) \\text{ dans } \\tilde{G}] = \\deg(u) \\text{ dans } G_0 \\quad \\forall u$$

2. **Maximisation de la variance** :
   $$\\text{Var}[D(\\tilde{G}, G)] \\text{ est maximale sous contraintes}$$

3. **R√©sistance au seuillage** :
   Les probabilit√©s NE sont PAS concentr√©es √† 0/1, donc $\\text{threshold}(\\tilde{G}, 0.5) \\neq G_0$

**Exemple num√©rique** (Karate Club) :

Ar√™te existante (0,1) :
- (k,Œµ)-obf : p = 0.95 (proche de 1.0) ‚Üí **facilement identifiable**
- MaxVar : p = 0.63 (dispers√©) ‚Üí **ambigu√´**

Ar√™te potentielle (5,12) :
- (k,Œµ)-obf : p = 0.05 (proche de 0.0) ‚Üí **facilement identifiable**
- MaxVar : p = 0.42 (dispers√©) ‚Üí **ambigu√´**

**Complexit√©** :
- Phase 1 : $O(\\sum \\deg(u)^2) \\approx O(n \\cdot d_{avg}^2)$
- Phase 2 : $O(m^2)$ pour l'optimisation quadratique
- Phase 3 : $O(m)$ pour l'√©chantillonnage

Total : **O(m¬≤)** (peut √™tre r√©duit avec partitionnement du graphe)

### Comparaison (k,Œµ)-obf vs MaxVar

| Crit√®re | (k,Œµ)-obf | MaxVar |
|---------|-----------|--------|
| **Probabilit√©s** | Concentr√©es (0/1) | Dispers√©es (0.3-0.7) |
| **R√©sistance seuillage** | ‚ùå Vuln√©rable | ‚úÖ R√©sistant |
| **Pr√©servation degr√©s** | Approximative | ‚úÖ Exacte |
| **Ar√™tes propos√©es** | Al√©atoires | ‚úÖ Nearby (distance 2) |
| **Variance** | Minimale | ‚úÖ Maximale |
| **Complexit√©** | O(m + kn) | O(m¬≤) |
| **Distance d'√©dition** | √âlev√©e (ar√™tes al√©atoires) | ‚úÖ Faible (ar√™tes nearby) |

**Trade-off** : MaxVar est plus co√ªteux en calcul mais offre de meilleures garanties
de privacy et d'utilit√©.

---

üìñ **R√©f√©rences Th√®se:**
- **p.80** - Section 3.4: MaxVar - Variance Maximizing Scheme
- **p.82** - Section 3.4.2: Formulation du programme quadratique
- **p.85** - Section 3.4.3: D√©tails d'impl√©mentation et nearby edges
        """,
        "formula": r"\min \sum_{i} p_i^2 \text{ s.t. } \sum_{v \in N(u)} p_{uv} = \deg(u)",
        "privacy_level": "Forte (r√©siste au seuillage)",
        "utility_preservation": "Excellente (degr√©s exacts + ar√™tes nearby)"
    },

    "EdgeFlip": {
        "name": "Privacy Diff√©rentielle - EdgeFlip",
        "category": "5. Privacy Diff√©rentielle",
        "params": {"epsilon": 0.8},
        "description_short": "Applique le Randomized Response Technique avec Œµ-DP",
        "description": """
### Principe en Langage Naturel

**EdgeFlip** applique le c√©l√®bre **Randomized Response Technique** (RRT) des statistiques
√† la publication de graphes.

**Intuition du RRT** (exemple classique) :
Pour une question sensible ("Avez-vous trich√© √† l'examen ?") :
- Lancez une pi√®ce en secret
- Si Face : r√©pondez la v√©rit√©
- Si Pile : r√©pondez au hasard (oui/non √† pile ou face)

R√©sultat : Votre r√©ponse a du "d√©ni plausible" mais les statistiques globales
restent calculables.

**Application √† EdgeFlip** :
Pour chaque paire de n≈ìuds (u,v) :
- Avec probabilit√© s/2 : **inverser** l'ar√™te (0‚Üí1 ou 1‚Üí0)
- Avec probabilit√© 1-s/2 : garder l'√©tat r√©el

o√π s est d√©termin√© par le param√®tre de privacy Œµ.

**Garantie Œµ-differential privacy** : La pr√©sence/absence d'une ar√™te
est prot√©g√©e avec garantie math√©matique Œµ-DP.

### Formalisation Math√©matique

**D√©finition Œµ-Differential Privacy** :

Un algorithme $\\mathcal{A}$ satisfait Œµ-DP si pour tous graphes voisins $G, G'$
(diff√©rant par une ar√™te) et pour tout output $O$ :

$$P[\\mathcal{A}(G) = O] \\leq e^\\varepsilon \\cdot P[\\mathcal{A}(G') = O]$$

Plus $\\varepsilon$ est petit, plus forte est la garantie de privacy.

**Algorithme EdgeFlip (en langage naturel)** :

Pour chaque paire de n≈ìuds possible (u, v) dans le graphe :
1. **Lancer une pi√®ce biais√©e** avec probabilit√© s/2
2. **Si pile** (probabilit√© s/2) : INVERSER l'√©tat de l'ar√™te
   - Si l'ar√™te existe ‚Üí la supprimer
   - Si l'ar√™te n'existe pas ‚Üí l'ajouter
3. **Si face** (probabilit√© 1-s/2) : GARDER l'√©tat de l'ar√™te
   - Si l'ar√™te existe ‚Üí la garder
   - Si l'ar√™te n'existe pas ‚Üí ne rien faire

Le param√®tre s d√©pend du budget privacy Œµ selon :

$$s = \\frac{2}{e^\\varepsilon + 1}$$

**Trade-off** :
- Œµ petit (0.1) ‚Üí s = 0.95 ‚Üí flip 47.5% des ar√™tes ‚Üí **forte privacy**
- Œµ grand (3.0) ‚Üí s = 0.09 ‚Üí flip 4.7% des ar√™tes ‚Üí **faible privacy**

**Algorithme EdgeFlip (pseudo-code formel)** :

```
Entr√©e : G = (V, E), Œµ
Param√®tre : s = 2 / (e^Œµ + 1)    ‚Üê FORMULE CORRECTE

Pour chaque paire (u, v) avec u < v :
  exists = (u,v) ‚àà E

  Avec probabilit√© s/2 :
    output = NOT exists   // Inverser
  Sinon :
    output = exists       // Garder

  Si output = TRUE :
    Ajouter (u,v) √† E_output

Retourner G_output = (V, E_output)
```

**Preuve de Œµ-DP** :

Pour une ar√™te (u,v) :

P[output=1 | exists=1] = 1 - s/2
P[output=1 | exists=0] = s/2

Ratio : (1 - s/2) / (s/2) = (e^Œµ + 1 - 1) / 1 = e^Œµ ‚úì

Donc EdgeFlip satisfait Œµ-edge-DP.

**Esp√©rance du nombre d'ar√™tes** :

$$\\mathbb{E}[|E_{output}|] = |E| \\cdot (1 - s/2) + (n(n-1)/2 - |E|) \\cdot s/2$$
$$\\approx n(n-1)/4 \\text{  (pour } s \\approx 1\\text{, tr√®s bruit√©)}$$

**Complexit√©** : O(n¬≤)

**Inconv√©nient** : Complexit√© quadratique limite le passage √† l'√©chelle.

---

üìñ **R√©f√©rences Th√®se:**
- **p.50** - Section 2.4: Differential Privacy pour les graphes
- **p.52** - Section 2.4.1: Edge-Level Differential Privacy
- **p.110** - Section 5: √âvaluation exp√©rimentale
        """,
        "formula": r"P[\mathcal{A}(G) = O] \leq e^\varepsilon \cdot P[\mathcal{A}(G') = O]",
        "privacy_level": "Tr√®s Forte (Œµ-differential privacy)",
        "utility_preservation": "Variable (d√©pend de Œµ)"
    },

    "Laplace": {
        "name": "Privacy Diff√©rentielle - M√©canisme de Laplace",
        "category": "5. Privacy Diff√©rentielle",
        "params": {"epsilon": 1.2},
        "description_short": "Ajoute du bruit Laplacien pour d√©cider de l'inclusion des ar√™tes",
        "description": """
### Principe en Langage Naturel

Le **M√©canisme de Laplace** est la technique fondamentale de la privacy diff√©rentielle.

**Principe g√©n√©ral** : Pour publier une statistique f(donn√©es) de mani√®re priv√©e,
on ajoute du **bruit al√©atoire** calibr√© √† la **sensibilit√©** de f.

**Pour les graphes** :
- On consid√®re chaque ar√™te potentielle (u,v)
- Valeur r√©elle : 1 si l'ar√™te existe, 0 sinon
- On ajoute du bruit Laplacien ~ Lap(Œîf/Œµ)
- On d√©cide d'inclure l'ar√™te si valeur_bruit√©e > seuil

**Intuition du bruit** : Le bruit "masque" la contribution d'une ar√™te individuelle,
rendant impossible de d√©terminer si une ar√™te sp√©cifique √©tait pr√©sente ou non.

### Formalisation Math√©matique

**M√©canisme de Laplace g√©n√©ral** :

Pour une fonction f : D ‚Üí ‚Ñù^d, le m√©canisme de Laplace est :

M(D) = f(D) + (Y‚ÇÅ, ..., Y_d)

o√π Y_i ~ Lap(Œîf/Œµ) sont ind√©pendants et Œîf est la sensibilit√© globale.

**Sensibilit√© globale** :

$$\\Delta f = \\max_{G,G' \\text{ voisins}} ||f(G) - f(G')||_1$$

Pour les graphes (edge-DP), deux graphes sont voisins s'ils diff√®rent par une ar√™te.
Donc : $\\Delta f = 1$ pour une requ√™te de type "cette ar√™te existe-t-elle ?"

**Distribution de Laplace** :

$\\text{Lap}(b)$ a la densit√© :
$$p(x|b) = \\frac{1}{2b} \\cdot \\exp\\left(-\\frac{|x|}{b}\\right)$$

- Moyenne : 0
- Variance : $2b^2$
- Plus b est grand, plus le bruit est important

**Application aux graphes** :

```
Entr√©e : G = (V, E), Œµ
Scale : b = 1/Œµ

Pour chaque paire (u, v) avec u < v :
  true_value = 1 si (u,v) ‚àà E, 0 sinon
  noise = Laplace(0, b)
  noisy_value = true_value + noise

  Si noisy_value > 0.5 :
    Ajouter (u,v) √† E_output

Retourner G_output = (V, E_output)
```

**Th√©or√®me** : Ce m√©canisme satisfait Œµ-differential privacy.

**Preuve (sketch)** :
Pour G et G' diff√©rant par une ar√™te (u‚ÇÄ, v‚ÇÄ) :

$$\\frac{P[M(G) = O]}{P[M(G') = O]} = \\exp(-\\varepsilon \\cdot |f(G)-f(G')|) \\leq e^\\varepsilon$$

car $|f(G) - f(G')| \\leq \\Delta f = 1$.

**Trade-off Œµ** :
- Œµ petit (ex: 0.1) : forte privacy, beaucoup de bruit, faible utilit√©
- Œµ grand (ex: 10) : faible privacy, peu de bruit, forte utilit√©
- Valeurs typiques : Œµ ‚àà [0.1, 10]

**Complexit√©** : O(n¬≤)
        """,
        "formula": r"M(D) = f(D) + \text{Lap}(\Delta f / \varepsilon)",
        "privacy_level": "Tr√®s Forte (Œµ-differential privacy)",
        "utility_preservation": "Variable (d√©pend de Œµ)"
    }
}


def calculate_anonymization_metrics(G_orig, G_anon):
    """Calcule des m√©triques d'anonymisation d√©taill√©es"""
    metrics = {}

    # Changements dans les ar√™tes
    if isinstance(G_anon, nx.Graph):
        orig_edges = set(G_orig.edges())
        anon_edges = set(G_anon.edges())

        added = len(anon_edges - orig_edges)
        removed = len(orig_edges - anon_edges)
        preserved = len(orig_edges & anon_edges)

        metrics['edges_added'] = added
        metrics['edges_removed'] = removed
        metrics['edges_preserved'] = preserved
        metrics['modification_rate'] = (added + removed) / (2 * len(orig_edges)) if len(orig_edges) > 0 else 0

        # Changements dans les degr√©s
        deg_orig = dict(G_orig.degree())
        deg_anon = dict(G_anon.degree())

        if set(deg_orig.keys()) == set(deg_anon.keys()):
            deg_changes = sum(abs(deg_orig[v] - deg_anon[v]) for v in deg_orig.keys())
            metrics['total_degree_change'] = deg_changes
            metrics['avg_degree_change'] = deg_changes / len(deg_orig)

            # Incorrectness (combien de n≈ìuds ont chang√© de degr√©)
            metrics['nodes_with_degree_change'] = sum(1 for v in deg_orig.keys() if deg_orig[v] != deg_anon[v])
            metrics['degree_preservation_rate'] = 1 - (metrics['nodes_with_degree_change'] / len(deg_orig))

        # M√©triques structurelles
        try:
            metrics['clustering_change'] = abs(
                nx.average_clustering(G_orig) - nx.average_clustering(G_anon)
            )
        except:
            metrics['clustering_change'] = None

        metrics['density_change'] = abs(nx.density(G_orig) - nx.density(G_anon))

    return metrics


def calculate_privacy_guarantees(G_orig, G_anon, method_key, method_params):
    """Calcule les garanties de privacy sp√©cifiques √† chaque m√©thode"""
    guarantees = {}

    if method_key == "k-degree anonymity":
        # V√©rifier la k-anonymit√© des degr√©s
        degrees = dict(G_anon.degree())
        degree_counts = Counter(degrees.values())

        k_value = method_params.get('k', 2)
        min_count = min(degree_counts.values()) if degree_counts else 0
        is_k_anonymous = min_count >= k_value

        guarantees['k_anonymity_satisfied'] = is_k_anonymous
        guarantees['min_degree_count'] = min_count
        guarantees['k_required'] = k_value
        guarantees['re_identification_risk'] = f"‚â§ 1/{min_count}" if min_count > 0 else "N/A"
        guarantees['unique_degrees'] = len(degree_counts)

    elif method_key == "Generalization":
        # M√©triques pour super-nodes
        if hasattr(G_anon, 'graph'):
            cluster_sizes = [G_anon.nodes[n].get('size', 0) for n in G_anon.nodes()]
            min_cluster_size = min(cluster_sizes) if cluster_sizes else 0
            max_cluster_size = max(cluster_sizes) if cluster_sizes else 0
            avg_cluster_size = np.mean(cluster_sizes) if cluster_sizes else 0

            intra_edges = G_anon.graph.get('intra_edges', 0)
            inter_edges = G_anon.graph.get('inter_edges', 0)
            total_edges = intra_edges + inter_edges

            guarantees['num_clusters'] = G_anon.number_of_nodes()
            guarantees['min_cluster_size'] = min_cluster_size
            guarantees['max_cluster_size'] = max_cluster_size
            guarantees['avg_cluster_size'] = f"{avg_cluster_size:.1f}"
            guarantees['intra_cluster_edges'] = intra_edges
            guarantees['inter_cluster_edges'] = inter_edges
            guarantees['intra_ratio'] = f"{intra_edges/total_edges*100:.1f}%" if total_edges > 0 else "N/A"
            guarantees['inter_ratio'] = f"{inter_edges/total_edges*100:.1f}%" if total_edges > 0 else "N/A"
            guarantees['re_identification_risk'] = f"‚â§ 1/{min_cluster_size}" if min_cluster_size > 0 else "N/A"
            guarantees['information_loss'] = f"{(1 - G_anon.number_of_nodes()/G_orig.number_of_nodes())*100:.1f}%"

    elif method_key in ["EdgeFlip", "Laplace"]:
        # Privacy diff√©rentielle
        epsilon = method_params.get('epsilon', 1.0)
        guarantees['epsilon'] = epsilon
        guarantees['privacy_budget'] = epsilon
        guarantees['privacy_level'] = "Forte" if epsilon < 1.0 else ("Moyenne" if epsilon < 2.0 else "Faible")
        guarantees['max_privacy_loss'] = f"e^{epsilon:.2f} ‚âà {np.exp(epsilon):.2f}"

        # Calculer le taux de faux positifs/n√©gatifs attendu
        if method_key == "EdgeFlip":
            s = 1 - np.exp(-epsilon)
            false_positive_rate = s/2
            false_negative_rate = s/2
            guarantees['expected_false_positive_rate'] = f"{false_positive_rate*100:.1f}%"
            guarantees['expected_false_negative_rate'] = f"{false_negative_rate*100:.1f}%"

    elif method_key == "Probabilistic":
        # (k,Œµ)-obfuscation
        k = method_params.get('k', 5)
        eps = method_params.get('epsilon', 0.3)

        guarantees['k_neighborhood'] = k
        guarantees['epsilon_tolerance'] = eps
        guarantees['min_entropy'] = f"log({k}) - {eps:.2f} ‚âà {np.log(k) - eps:.2f}"
        guarantees['uncertainty_level'] = "√âlev√©e" if eps < 0.5 else "Moyenne"
        guarantees['vulnerability'] = "‚ö†Ô∏è Reconstruction par seuillage possible!"

    elif method_key == "MaxVar":
        # MaxVar obfuscation
        num_pot = method_params.get('num_potential_edges', 50)

        guarantees['potential_edges'] = num_pot
        guarantees['optimization'] = "Programme quadratique (SLSQP)"
        guarantees['degree_preservation'] = "Exacte (E[deg(u)] = deg(u))"
        guarantees['variance_maximization'] = "‚úì Probabilit√©s dispers√©es"
        guarantees['threshold_resistance'] = "‚úì R√©siste au seuillage √† 0.5"

    elif method_key == "Random Switch":
        # Pr√©servation de la s√©quence de degr√©s
        deg_orig = sorted([d for n, d in G_orig.degree()])
        deg_anon = sorted([d for n, d in G_anon.degree()])

        degree_sequence_preserved = deg_orig == deg_anon
        guarantees['degree_sequence_preserved'] = degree_sequence_preserved
        guarantees['structural_property'] = "S√©quence de degr√©s pr√©serv√©e" if degree_sequence_preserved else "Modifi√©e"

    elif method_key == "Random Add/Del":
        # Quantifier l'incertitude introduite
        k = method_params.get('k', 20)
        total_possible_edges = G_orig.number_of_nodes() * (G_orig.number_of_nodes() - 1) // 2

        guarantees['edges_modified'] = 2 * k  # k ajout√©es + k supprim√©es (th√©orique)
        guarantees['modification_budget'] = k
        guarantees['structural_uncertainty'] = "Mod√©r√©e"

    return guarantees


def sample_from_probabilistic_graph(prob_graph):
    """
    Tire un √©chantillon de graphe d√©terministe depuis un graphe probabiliste.

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    PRINCIPE DU TIRAGE (SAMPLING) :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    Dans un graphe probabiliste (k,Œµ)-obfuscation, chaque ar√™te a une PROBABILIT√â
    d'existence. Pour cr√©er un graphe d√©terministe, on effectue un TIRAGE au sort
    pour chaque ar√™te :

    - Si prob(ar√™te) = 0.95 ‚Üí 95% de chance d'appara√Ætre dans l'√©chantillon
    - Si prob(ar√™te) = 0.10 ‚Üí 10% de chance d'appara√Ætre dans l'√©chantillon

    Ce m√©canisme permet de :
    1. Publier plusieurs graphes √©chantillons diff√©rents
    2. Cr√©er de la confusion pour l'attaquant (plusieurs graphes plausibles)
    3. Garantir que l'attaquant ne peut pas identifier le graphe original avec certitude

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    Param√®tres:
        prob_graph : networkx.Graph avec attributs 'probability' sur les ar√™tes

    Retourne:
        networkx.Graph : Graphe d√©terministe √©chantillonn√©
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """
    import random

    # Cr√©er un nouveau graphe avec les m√™mes n≈ìuds
    sampled_graph = nx.Graph()
    sampled_graph.add_nodes_from(prob_graph.nodes())

    # Pour chaque ar√™te du graphe probabiliste
    for u, v in prob_graph.edges():
        # R√©cup√©rer la probabilit√©
        prob = prob_graph[u][v].get('probability', 0.5)

        # Tirer au sort : l'ar√™te appara√Æt si random < prob
        if random.random() < prob:
            sampled_graph.add_edge(u, v)

    return sampled_graph


def plot_probabilistic_graph(prob_graph, G_orig, method_name, ax):
    """
    Visualise un graphe probabiliste avec des ar√™tes de diff√©rentes intensit√©s.

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    PRINCIPE DE LA VISUALISATION :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    Dans un graphe probabiliste (k,Œµ)-obfuscation :
    - Les ar√™tes EXISTANTES ont une probabilit√© √âLEV√âE (‚âà 1 - Œµ/k) ‚Üí FONC√âES
    - Les ar√™tes POTENTIELLES ont une probabilit√© FAIBLE (‚âà Œµ/2k) ‚Üí CLAIRES

    Cette visualisation utilise :
    1. INTENSIT√â DE COULEUR : Plus la probabilit√© est √©lev√©e, plus l'ar√™te est fonc√©e
    2. √âPAISSEUR : Les ar√™tes √† haute probabilit√© sont plus √©paisses
    3. L√âGENDE : Code couleur pour interpr√©ter les probabilit√©s

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """
    import matplotlib.cm as cm
    from matplotlib.colors import LinearSegmentedColormap

    # Position pour visualisation
    pos = nx.spring_layout(G_orig, seed=42, k=0.5, iterations=50)

    # Dessiner les n≈ìuds
    nx.draw_networkx_nodes(prob_graph, pos, ax=ax,
                          node_color='lightcyan',
                          node_size=500, alpha=0.9,
                          edgecolors='darkblue', linewidths=2)

    # Collecter les ar√™tes par probabilit√©
    edges_with_prob = []
    for u, v in prob_graph.edges():
        prob = prob_graph[u][v].get('probability', 0.5)
        is_orig = prob_graph[u][v].get('is_original', False)
        edges_with_prob.append(((u, v), prob, is_orig))

    if not edges_with_prob:
        nx.draw_networkx_labels(prob_graph, pos, ax=ax, font_size=8, font_weight='bold')
        return

    # Trier par probabilit√© pour dessiner les faibles d'abord
    edges_with_prob.sort(key=lambda x: x[1])

    # Cr√©er un colormap du clair (prob faible) au fonc√© (prob √©lev√©e)
    cmap = cm.get_cmap('RdYlGn')  # Rouge (faible) -> Jaune (moyen) -> Vert (√©lev√©)

    # Dessiner chaque ar√™te avec sa couleur et √©paisseur
    for (u, v), prob, is_orig in edges_with_prob:
        # Couleur bas√©e sur la probabilit√©
        color = cmap(prob)

        # √âpaisseur bas√©e sur la probabilit√©
        width = 0.5 + 3.5 * prob  # De 0.5 (prob=0) √† 4.0 (prob=1)

        # Style : solide pour ar√™tes originales, pointill√© pour potentielles
        style = 'solid' if is_orig else 'dotted'

        # Transparence bas√©e sur la probabilit√©
        alpha = 0.3 + 0.6 * prob  # De 0.3 √† 0.9

        nx.draw_networkx_edges(prob_graph, pos, [(u, v)], ax=ax,
                              edge_color=[color], width=width,
                              style=style, alpha=alpha)

    # Labels des n≈ìuds
    nx.draw_networkx_labels(prob_graph, pos, ax=ax, font_size=8, font_weight='bold')

    # Cr√©er une l√©gende explicative
    from matplotlib.patches import Rectangle
    from matplotlib.lines import Line2D

    legend_elements = [
        Line2D([0], [0], color=cmap(0.95), linewidth=4, label='Prob. tr√®s √©lev√©e (‚âà 95%)'),
        Line2D([0], [0], color=cmap(0.70), linewidth=3, label='Prob. √©lev√©e (‚âà 70%)'),
        Line2D([0], [0], color=cmap(0.50), linewidth=2, label='Prob. moyenne (‚âà 50%)'),
        Line2D([0], [0], color=cmap(0.30), linewidth=1.5, label='Prob. faible (‚âà 30%)'),
        Line2D([0], [0], color=cmap(0.10), linewidth=1, linestyle='dotted', label='Prob. tr√®s faible (‚âà 10%)'),
        Line2D([0], [0], color='black', linewidth=2, linestyle='solid', label='‚îÄ‚îÄ‚îÄ Ar√™tes originales'),
        Line2D([0], [0], color='black', linewidth=2, linestyle='dotted', label='¬∑¬∑¬∑ Ar√™tes potentielles'),
    ]

    ax.legend(handles=legend_elements, loc='upper right', fontsize=9, framealpha=0.9)
    ax.set_title(f'{method_name}\nGraphe Probabiliste ({prob_graph.number_of_nodes()} n≈ìuds, {prob_graph.number_of_edges()} ar√™tes)',
                fontsize=14, fontweight='bold')


def plot_graph_comparison(G_orig, G_anon, method_name, node_to_cluster=None):
    """
    Cr√©e une comparaison visuelle des graphes.

    G√®re plusieurs types de graphes :
    - Graphes classiques (Random Add/Del, Random Switch, k-anonymity)
    - Graphes probabilistes (k,Œµ)-obfuscation avec ar√™tes pond√©r√©es
    - Super-graphes (G√©n√©ralisation avec clusters)
    - Graphes diff√©rentiellement priv√©s (EdgeFlip, Laplace)
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

    # Position commune pour comparaison
    pos = nx.spring_layout(G_orig, seed=42, k=0.5, iterations=50)

    # Graphe original
    if node_to_cluster is not None:
        # Si c'est une g√©n√©ralisation, colorier par cluster
        clusters = {}
        for node, cluster in node_to_cluster.items():
            if cluster not in clusters:
                clusters[cluster] = []
            clusters[cluster].append(node)

        # G√©n√©rer des couleurs pour chaque cluster
        import matplotlib.cm as cm
        colors = cm.tab20(np.linspace(0, 1, len(clusters)))

        # Dessiner les n≈ìuds par cluster
        for idx, (cluster_id, nodes) in enumerate(clusters.items()):
            node_color = colors[idx % len(colors)]
            nx.draw_networkx_nodes(G_orig, pos, nodelist=nodes, ax=ax1,
                                  node_color=[node_color], node_size=500, alpha=0.7)

            # Encercler le cluster
            node_positions = np.array([pos[n] for n in nodes])
            if len(node_positions) > 0:
                center = node_positions.mean(axis=0)
                max_dist = np.max(np.linalg.norm(node_positions - center, axis=1))
                circle = plt.Circle(center, max_dist + 0.15, color=node_color,
                                  fill=False, linewidth=2.5, linestyle='--', alpha=0.6)
                ax1.add_patch(circle)

        # Dessiner les ar√™tes par type
        intra_edges = []
        inter_edges = []
        for u, v in G_orig.edges():
            if node_to_cluster[u] == node_to_cluster[v]:
                intra_edges.append((u, v))
            else:
                inter_edges.append((u, v))

        if intra_edges:
            nx.draw_networkx_edges(G_orig, pos, intra_edges, ax=ax1,
                                  edge_color='green', width=2, alpha=0.4,
                                  label='Intra-cluster')
        if inter_edges:
            nx.draw_networkx_edges(G_orig, pos, inter_edges, ax=ax1,
                                  edge_color='red', width=1.5, alpha=0.5,
                                  label='Inter-cluster', style='dashed')

        ax1.legend(loc='upper right')
        ax1.set_title(f'Graphe Original avec Clusters\n{G_orig.number_of_nodes()} n≈ìuds, {len(clusters)} clusters',
                      fontsize=14, fontweight='bold')
    else:
        # Affichage normal
        nx.draw_networkx_nodes(G_orig, pos, ax=ax1, node_color='lightblue',
                              node_size=500, alpha=0.9)
        nx.draw_networkx_edges(G_orig, pos, ax=ax1, edge_color='gray',
                              width=1.5, alpha=0.6)
        ax1.set_title(f'Graphe Original\n{G_orig.number_of_nodes()} n≈ìuds, {G_orig.number_of_edges()} ar√™tes',
                      fontsize=14, fontweight='bold')

    nx.draw_networkx_labels(G_orig, pos, ax=ax1, font_size=8, font_weight='bold')
    ax1.axis('off')

    # Graphe anonymis√©
    if isinstance(G_anon, nx.Graph) and G_anon.number_of_nodes() > 0:
        # Adapter la position si diff√©rent nombre de n≈ìuds
        if set(G_anon.nodes()) != set(G_orig.nodes()):
            pos_anon = nx.spring_layout(G_anon, seed=42, k=0.5, iterations=50)

            # Si c'est un super-graphe, ajuster la visualisation
            if node_to_cluster is not None and hasattr(G_anon, 'graph'):
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # VISUALISATION AM√âLIOR√âE DU SUPER-GRAPHE
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

                # Tailles proportionnelles au nombre de n≈ìuds dans chaque cluster
                node_sizes = [G_anon.nodes[n].get('size', 1) * 300 for n in G_anon.nodes()]

                # Couleurs vari√©es pour distinguer les clusters
                import matplotlib.cm as cm
                num_clusters = G_anon.number_of_nodes()
                colors_palette = cm.Set3(np.linspace(0, 1, num_clusters))

                # Dessiner les super-n≈ìuds
                for idx, node in enumerate(G_anon.nodes()):
                    nx.draw_networkx_nodes(G_anon, pos_anon, nodelist=[node], ax=ax2,
                                          node_color=[colors_palette[idx]],
                                          node_size=node_sizes[idx],
                                          alpha=0.85, edgecolors='darkblue',
                                          linewidths=2.5)

                # S√©parer les ar√™tes intra-cluster (self-loops) et inter-cluster
                intra_edges = []
                inter_edges = []
                intra_weights = []
                inter_weights = []

                for u, v in G_anon.edges():
                    weight = G_anon[u][v].get('weight', 1)
                    if u == v:  # Self-loop (ar√™tes intra-cluster)
                        intra_edges.append((u, v))
                        intra_weights.append(weight)
                    else:  # Ar√™tes inter-cluster
                        inter_edges.append((u, v))
                        inter_weights.append(weight)

                # Calculer les poids max pour normalisation
                max_intra_weight = max(intra_weights) if intra_weights else 1
                max_inter_weight = max(inter_weights) if inter_weights else 1

                # Dessiner les ar√™tes INTRA-cluster (self-loops)
                if intra_edges:
                    for (u, v), weight in zip(intra_edges, intra_weights):
                        # Dessiner un cercle autour du n≈ìud pour repr√©senter les ar√™tes internes
                        node_pos = pos_anon[u]
                        radius = 0.08 + 0.12 * (weight / max_intra_weight)
                        circle = plt.Circle(node_pos, radius, color='green',
                                          fill=False, linewidth=2 + 3*(weight/max_intra_weight),
                                          linestyle='solid', alpha=0.6)
                        ax2.add_patch(circle)

                # Dessiner les ar√™tes INTER-cluster
                if inter_edges:
                    for (u, v), weight in zip(inter_edges, inter_weights):
                        width = 1.5 + 4.5 * (weight / max_inter_weight)
                        nx.draw_networkx_edges(G_anon, pos_anon, [(u, v)], ax=ax2,
                                              width=width, alpha=0.7, edge_color='purple',
                                              style='solid')

                # Labels avec d√©tails : ID + taille + poids intra
                labels = {}
                for n in G_anon.nodes():
                    size = G_anon.nodes[n].get('size', '?')
                    intra_weight = 0
                    if G_anon.has_edge(n, n):
                        intra_weight = G_anon[n][n].get('weight', 0)
                    labels[n] = f"C{n}\n[{size}n]\n{intra_weight}i"

                nx.draw_networkx_labels(G_anon, pos_anon, labels, ax=ax2,
                                       font_size=9, font_weight='bold',
                                       bbox=dict(boxstyle='round,pad=0.3',
                                                facecolor='white', alpha=0.8,
                                                edgecolor='black'))

                # L√©gende explicative
                from matplotlib.lines import Line2D
                legend_elements = [
                    Line2D([0], [0], color='green', linewidth=3, linestyle='solid',
                          label='Ar√™tes intra-cluster (vert)'),
                    Line2D([0], [0], color='purple', linewidth=3, linestyle='solid',
                          label='Ar√™tes inter-cluster (violet)'),
                ]
                ax2.legend(handles=legend_elements, loc='upper right', fontsize=9, framealpha=0.9)

                # Titre avec statistiques compl√®tes
                total_intra = sum(intra_weights)
                total_inter = sum(inter_weights)
                ax2.set_title(f'Super-Graphe - {method_name}\n{G_anon.number_of_nodes()} clusters | {int(total_intra)} ar√™tes intra | {int(total_inter)} ar√™tes inter',
                             fontsize=13, fontweight='bold')
            else:
                # Graphe normal avec n≈ìuds diff√©rents
                nx.draw_networkx_nodes(G_anon, pos_anon, ax=ax2, node_color='lightgreen',
                                      node_size=500, alpha=0.9)
                nx.draw_networkx_edges(G_anon, pos_anon, ax=ax2, edge_color='gray',
                                      width=1.5, alpha=0.6)
                nx.draw_networkx_labels(G_anon, pos_anon, ax=ax2, font_size=8, font_weight='bold')
                ax2.set_title(f'Graphe Anonymis√© - {method_name}\n{G_anon.number_of_nodes()} n≈ìuds',
                             fontsize=14, fontweight='bold')
        else:
            pos_anon = pos

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # D√âTECTION DE GRAPHE PROBABILISTE
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # V√©rifier si le graphe a des ar√™tes avec des probabilit√©s
            has_probabilities = False
            if G_anon.number_of_edges() > 0:
                first_edge = list(G_anon.edges())[0]
                has_probabilities = 'probability' in G_anon[first_edge[0]][first_edge[1]]

            if has_probabilities:
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # VISUALISATION PROBABILISTE AM√âLIOR√âE
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                plot_probabilistic_graph(G_anon, G_orig, method_name, ax2)
            else:
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # VISUALISATION CLASSIQUE (graphes d√©terministes)
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                orig_edges = set(G_orig.edges())

                # Dessiner les n≈ìuds
                nx.draw_networkx_nodes(G_anon, pos_anon, ax=ax2, node_color='lightgreen',
                                      node_size=500, alpha=0.9)

                # Dessiner les ar√™tes par type
                preserved_edges = [(u,v) for u,v in G_anon.edges()
                                  if (u,v) in orig_edges or (v,u) in orig_edges]
                added_edges = [(u,v) for u,v in G_anon.edges()
                              if (u,v) not in orig_edges and (v,u) not in orig_edges]

                if preserved_edges:
                    nx.draw_networkx_edges(G_anon, pos_anon, preserved_edges, ax=ax2,
                                          edge_color='blue', width=1.5, alpha=0.6,
                                          style='solid', label='Ar√™tes pr√©serv√©es')
                if added_edges:
                    nx.draw_networkx_edges(G_anon, pos_anon, added_edges, ax=ax2,
                                          edge_color='red', width=1.5, alpha=0.6,
                                          style='dashed', label='Ar√™tes ajout√©es')

                nx.draw_networkx_labels(G_anon, pos_anon, ax=ax2, font_size=8, font_weight='bold')
                ax2.legend(loc='upper right')

                ax2.set_title(f'Graphe Anonymis√© - {method_name}\n{G_anon.number_of_nodes()} n≈ìuds, {G_anon.number_of_edges()} ar√™tes',
                             fontsize=14, fontweight='bold')
    else:
        ax2.text(0.5, 0.5, 'Graphe non visualisable\n(format incompatible)',
                ha='center', va='center', fontsize=12)
        ax2.set_title(f'Graphe Anonymis√© - {method_name}', fontsize=14, fontweight='bold')

    ax2.axis('off')

    plt.tight_layout()
    return fig


def plot_degree_distribution(G_orig, G_anon, method_name):
    """
    Compare les distributions de degr√©s.

    G√®re 3 cas sp√©ciaux :
    1. Graphe probabiliste ‚Üí √âchantillonner avant de calculer
    2. Super-graphe ‚Üí Tirage uniforme depuis les clusters
    3. Graphe classique ‚Üí Distribution standard
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Distribution originale
    degrees_orig = [d for n, d in G_orig.degree()]
    ax1.hist(degrees_orig, bins=range(max(degrees_orig)+2),
            alpha=0.7, color='blue', edgecolor='black', rwidth=0.8)
    ax1.set_xlabel('Degr√©', fontsize=12)
    ax1.set_ylabel('Nombre de n≈ìuds', fontsize=12)
    ax1.set_title('Distribution des degr√©s - Original', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.set_axisbelow(True)

    # Distribution anonymis√©e
    if isinstance(G_anon, nx.Graph) and G_anon.number_of_nodes() > 0:
        # CAS 1 : D√©tecter si c'est un super-graphe (g√©n√©ralisation)
        is_super_graph = False
        if G_anon.number_of_nodes() > 0:
            first_node = list(G_anon.nodes())[0]
            node_data = G_anon.nodes[first_node]
            is_super_graph = 'cluster_size' in node_data

        if is_super_graph:
            # TIRAGE UNIFORME depuis le super-graphe pour recr√©er une distribution
            degrees_reconstructed = []

            # Pour chaque cluster
            for cluster_id in G_anon.nodes():
                cluster_size = G_anon.nodes[cluster_id]['cluster_size']
                internal_edges = G_anon.nodes[cluster_id]['internal_edges']

                # Compter les ar√™tes inter-cluster pour ce cluster
                inter_edges_count = 0
                for neighbor in G_anon.neighbors(cluster_id):
                    if neighbor != cluster_id:  # Exclure self-loops
                        inter_edges_count += G_anon[cluster_id][neighbor]['weight']

                # Estimer le degr√© moyen dans ce cluster
                # Degr√© interne moyen ‚âà 2 √ó internal_edges / cluster_size
                avg_internal_degree = (2 * internal_edges) / cluster_size if cluster_size > 0 else 0

                # Degr√© externe moyen ‚âà inter_edges_count / cluster_size
                avg_external_degree = inter_edges_count / cluster_size if cluster_size > 0 else 0

                # Degr√© total moyen pour les n≈ìuds de ce cluster
                avg_degree = avg_internal_degree + avg_external_degree

                # Tirer des degr√©s avec une petite variance (¬± 20%)
                for _ in range(cluster_size):
                    # Ajouter un peu de bruit pour simuler la variabilit√©
                    degree = int(max(0, avg_degree + np.random.normal(0, avg_degree * 0.2)))
                    degrees_reconstructed.append(degree)

            ax2.hist(degrees_reconstructed, bins=range(max(degrees_reconstructed)+2) if degrees_reconstructed else [0],
                    alpha=0.7, color='orange', edgecolor='black', rwidth=0.8)
            ax2.set_xlabel('Degr√©', fontsize=12)
            ax2.set_ylabel('Nombre de n≈ìuds', fontsize=12)
            ax2.set_title(f'Distribution des degr√©s - {method_name}\n(Tirage uniforme depuis clusters)',
                         fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3, linestyle='--')
            ax2.set_axisbelow(True)

        else:
            # CAS 2 : D√©tecter si c'est un graphe probabiliste
            is_probabilistic = False
            if G_anon.number_of_edges() > 0:
                first_edge = list(G_anon.edges())[0]
                is_probabilistic = 'probability' in G_anon[first_edge[0]][first_edge[1]]

            if is_probabilistic:
                # √âCHANTILLONNER le graphe probabiliste
                G_sample = sample_from_probabilistic_graph(G_anon)
                degrees_anon = [d for n, d in G_sample.degree()]

                ax2.hist(degrees_anon, bins=range(max(degrees_anon)+2) if degrees_anon else [0],
                        alpha=0.7, color='purple', edgecolor='black', rwidth=0.8)
                ax2.set_xlabel('Degr√©', fontsize=12)
                ax2.set_ylabel('Nombre de n≈ìuds', fontsize=12)
                ax2.set_title(f'Distribution des degr√©s - {method_name}\n(√âchantillon)',
                             fontsize=14, fontweight='bold')
                ax2.grid(True, alpha=0.3, linestyle='--')
                ax2.set_axisbelow(True)

            # CAS 3 : Graphe classique
            elif set(G_anon.nodes()).issubset(set(G_orig.nodes())):
                degrees_anon = [d for n, d in G_anon.degree()]
                ax2.hist(degrees_anon, bins=range(max(degrees_anon)+2),
                        alpha=0.7, color='green', edgecolor='black', rwidth=0.8)
                ax2.set_xlabel('Degr√©', fontsize=12)
                ax2.set_ylabel('Nombre de n≈ìuds', fontsize=12)
                ax2.set_title(f'Distribution des degr√©s - {method_name}', fontsize=14, fontweight='bold')
                ax2.grid(True, alpha=0.3, linestyle='--')
                ax2.set_axisbelow(True)
            else:
                ax2.text(0.5, 0.5, 'Distribution non comparable\n(n≈ìuds diff√©rents)',
                        ha='center', va='center', fontsize=12)
                ax2.axis('off')
    else:
        ax2.text(0.5, 0.5, 'Pas de distribution\n(format non standard)',
                ha='center', va='center', fontsize=12)
        ax2.axis('off')

    plt.tight_layout()
    return fig


def simulate_degree_attack(G_orig, G_anon, target_node=0):
    """
    Simule une ATTAQUE PAR DEGR√â (Degree Attack) sur le graphe anonymis√©.

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    PRINCIPE DE L'ATTAQUE :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    L'adversaire conna√Æt le DEGR√â (nombre d'amis/connexions) d'un n≈ìud cible
    dans le graphe original et tente de le retrouver dans le graphe anonymis√©
    en cherchant les n≈ìuds ayant le m√™me degr√©.

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    EXEMPLE CONCRET (Karate Club) :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    - Alice (instructrice) a 16 amis dans le club (information publique)
    - L'attaquant cherche dans le graphe anonymis√© tous les n≈ìuds de degr√© 16
    - Si UN SEUL n≈ìud a degr√© 16 ‚Üí Alice est r√©-identifi√©e avec 100% de certitude
    - Si k n≈ìuds ont degr√© 16 ‚Üí Probabilit√© de r√©-identification = 1/k

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    M√âTRIQUES DE PRIVACY CALCUL√âES :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    1. INCORRECTNESS : Nombre de fausses suppositions de l'attaquant
       ‚Üí Plus cette valeur est √©lev√©e, meilleure est la privacy
       ‚Üí Incorrectness = k - 1 (k candidats signifie k-1 erreurs potentielles)

    2. MIN ENTROPY : log‚ÇÇ(k) bits
       ‚Üí Mesure l'incertitude de l'attaquant
       ‚Üí 0 bits = aucune privacy (1 candidat)
       ‚Üí 1 bit = privacy faible (2 candidats)
       ‚Üí 3 bits = privacy moyenne (8 candidats)
       ‚Üí 5+ bits = bonne privacy (32+ candidats)

    3. PROBABILIT√â DE R√â-IDENTIFICATION : 1/k
       ‚Üí Chance que l'attaquant devine correctement au hasard

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    PARAM√àTRES :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    G_orig : networkx.Graph
        Graphe original (avant anonymisation)
    G_anon : networkx.Graph ou autre
        Graphe apr√®s anonymisation
    target_node : int
        N≈ìud que l'attaquant cherche √† r√©-identifier (par d√©faut : n≈ìud 0)

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    RETOURNE :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    dict contenant:
        - attack_type : Type d'attaque ('Degree Attack')
        - target_node : N≈ìud cibl√©
        - target_degree : Degr√© du n≈ìud cible
        - candidates : Liste des n≈ìuds candidats dans le graphe anonymis√©
        - success : True si r√©-identification r√©ussie (1 seul candidat)
        - re_identification_probability : 1/nombre_candidats
        - incorrectness : Nombre de fausses suppositions (k-1)
        - min_entropy_bits : log‚ÇÇ(k) bits de privacy
        - explanation : Explication textuelle du r√©sultat
    """

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # INITIALISATION DES R√âSULTATS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    results = {
        'attack_type': 'Degree Attack',
        'target_node': target_node,
        'success': False,
        'candidates': [],
        'explanation': ''
    }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # V√âRIFICATION : L'attaque n'est possible que sur des graphes classiques
    # (pas sur les super-n≈ìuds de la g√©n√©ralisation)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if not isinstance(G_anon, nx.Graph):
        results['explanation'] = "‚ö†Ô∏è Attaque impossible sur ce type de graphe (super-nodes). La g√©n√©ralisation d√©truit les degr√©s individuels."
        results['incorrectness'] = float('inf')  # Privacy parfaite
        results['min_entropy_bits'] = float('inf')
        results['re_identification_probability'] = 0.0
        return results

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 1 : CONNAISSANCE DE L'ATTAQUANT
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # L'attaquant conna√Æt le degr√© du n≈ìud cible dans le graphe ORIGINAL
    # (par exemple via un profil public, un annuaire, ou sa propre connaissance)
    target_degree = G_orig.degree(target_node)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 2 : RECHERCHE DES CANDIDATS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # L'attaquant cherche TOUS les n≈ìuds du graphe anonymis√© ayant le m√™me degr√©
    candidates = [n for n in G_anon.nodes() if G_anon.degree(n) == target_degree]

    k = len(candidates)  # Nombre de n≈ìuds indistinguables (anonymity set size)

    results['candidates'] = candidates
    results['target_degree'] = target_degree

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 3 : CALCUL DES M√âTRIQUES DE PRIVACY
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    if k == 1:
        # CAS 1 : R√â-IDENTIFICATION R√âUSSIE (Privacy = 0)
        # Un seul candidat ‚Üí L'attaquant est CERTAIN de l'identit√©
        results['success'] = True
        results['re_identified_node'] = candidates[0]
        results['re_identification_probability'] = 1.0  # 100% de certitude
        results['incorrectness'] = 0  # Aucune fausse supposition possible
        results['min_entropy_bits'] = 0.0  # Aucune incertitude (log‚ÇÇ(1) = 0)
        results['explanation'] = (
            f"‚úÖ **R√â-IDENTIFICATION R√âUSSIE !**\n\n"
            f"Le n≈ìud cible {target_node} a un degr√© UNIQUE ({target_degree} connexions).\n"
            f"Un seul n≈ìud dans le graphe anonymis√© a ce degr√©.\n\n"
            f"üìä **M√©triques de Privacy** :\n"
            f"- Probabilit√© de r√©-identification : **100%** (certitude absolue)\n"
            f"- Incorrectness : **0** (aucune erreur possible)\n"
            f"- Min Entropy : **0 bits** (aucune privacy)\n\n"
            f"üî¥ **DANGER** : L'attaquant peut maintenant d√©couvrir toutes les connexions du n≈ìud {target_node} !"
        )

    elif k == 0:
        # CAS 2 : AUCUN CANDIDAT TROUV√â
        # Le degr√© a √©t√© modifi√© par l'anonymisation (randomisation, DP, etc.)
        results['success'] = False
        results['re_identification_probability'] = 0.0
        results['incorrectness'] = float('inf')  # Protection parfaite
        results['min_entropy_bits'] = float('inf')
        results['explanation'] = (
            f"‚ùå **ATTAQUE √âCHOU√âE - Aucun candidat**\n\n"
            f"Aucun n≈ìud avec degr√© {target_degree} trouv√© dans le graphe anonymis√©.\n"
            f"Le degr√© du n≈ìud cible a √©t√© modifi√© par l'anonymisation.\n\n"
            f"üìä **M√©triques de Privacy** :\n"
            f"- Probabilit√© de r√©-identification : **0%**\n"
            f"- Incorrectness : **‚àû** (impossible de deviner)\n"
            f"- Min Entropy : **‚àû bits** (privacy maximale)\n\n"
            f"üü¢ **S√âCURIT√â** : Excellente protection contre cette attaque !"
        )

    else:
        # CAS 3 : R√â-IDENTIFICATION AMBIGU√ã (k-anonymity)
        # Plusieurs candidats ‚Üí L'attaquant doit deviner parmi k n≈ìuds
        results['success'] = False
        results['re_identification_probability'] = 1.0 / k
        results['incorrectness'] = k - 1  # Nombre de fausses suppositions
        results['min_entropy_bits'] = np.log2(k)  # Bits de privacy

        # √âvaluation qualitative de la privacy
        if k >= 10:
            privacy_level = "üü¢ FORTE"
            privacy_comment = "Excellente protection"
        elif k >= 5:
            privacy_level = "üü° MOYENNE"
            privacy_comment = "Protection acceptable"
        else:
            privacy_level = "üü† FAIBLE"
            privacy_comment = "Protection limit√©e"

        results['explanation'] = (
            f"‚ö†Ô∏è **R√â-IDENTIFICATION AMBIGU√ã**\n\n"
            f"{k} n≈ìuds ont le degr√© {target_degree} dans le graphe anonymis√©.\n"
            f"L'attaquant doit deviner au hasard parmi ces {k} candidats.\n\n"
            f"üìä **M√©triques de Privacy** :\n"
            f"- Probabilit√© de r√©-identification : **{1/k*100:.1f}%** (1/{k})\n"
            f"- Incorrectness : **{k-1}** fausses suppositions possibles\n"
            f"- Min Entropy : **{np.log2(k):.2f} bits** de privacy\n"
            f"- Niveau de privacy : {privacy_level}\n\n"
            f"üí° **Interpr√©tation** : {privacy_comment}. "
            f"Le graphe satisfait la **{k}-anonymit√©** pour ce n≈ìud."
        )

    return results


def simulate_subgraph_attack(G_orig, G_anon, target_node=0):
    """
    Simule une ATTAQUE PAR SOUS-GRAPHE (Subgraph/Neighborhood Attack) sur le graphe.

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    PRINCIPE DE L'ATTAQUE :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    L'adversaire conna√Æt la STRUCTURE LOCALE autour du n≈ìud cible, notamment :
    - Le nombre de connexions (degr√©)
    - Les TRIANGLES form√©s avec ses voisins (amis communs)
    - Le coefficient de clustering (densit√© du voisinage)

    Cette attaque est BEAUCOUP PLUS PUISSANTE que l'attaque par degr√© seul,
    car elle exploite des MOTIFS STRUCTURELS (patterns) qui sont souvent uniques.

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    EXEMPLE CONCRET (Karate Club) :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    - Mr. Hi (n≈ìud 0) a 16 amis ET forme 45 triangles avec eux
    - Pattern : [degr√©=16, triangles=45]
    - L'attaquant cherche ce pattern dans le graphe anonymis√©
    - Ce pattern est souvent UNIQUE ‚Üí R√©-identification r√©ussie

    Comparaison avec Degree Attack :
    - Degree Attack : Cherche seulement "degr√© = 16"
      ‚Üí Peut trouver plusieurs candidats (k-anonymity)
    - Subgraph Attack : Cherche "degr√© = 16 ET 45 triangles"
      ‚Üí Pattern beaucoup plus distinctif ‚Üí Moins de candidats

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    PROTECTION CONTRE CETTE ATTAQUE :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    üü¢ G√âN√âRALISATION (Super-n≈ìuds) : ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
       ‚Üí D√©truit compl√®tement les motifs locaux en regroupant les n≈ìuds
       ‚Üí L'attaque devient IMPOSSIBLE

    üü¢ DIFFERENTIAL PRIVACY : ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ
       ‚Üí Ajoute/supprime des triangles de mani√®re al√©atoire
       ‚Üí Brouille les patterns structurels

    üü† RANDOMISATION : ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ
       ‚Üí Peut pr√©server certains triangles
       ‚Üí Protection limit√©e

    üî¥ k-DEGREE ANONYMITY : ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ
       ‚Üí Ne prot√®ge que le degr√©, pas les triangles
       ‚Üí VULN√âRABLE √† cette attaque

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    M√âTRIQUES DE PRIVACY CALCUL√âES :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    1. STRUCTURAL QUERY H2 : (degr√©, nombre_triangles)
       ‚Üí D√©crit la structure locale du n≈ìud

    2. INCORRECTNESS : k - 1 (nombre de fausses suppositions)

    3. MIN ENTROPY : log‚ÇÇ(k) bits d'incertitude

    4. PROBABILIT√â DE R√â-IDENTIFICATION : 1/k

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    PARAM√àTRES :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    G_orig : networkx.Graph
        Graphe original (avant anonymisation)
    G_anon : networkx.Graph ou autre
        Graphe apr√®s anonymisation
    target_node : int
        N≈ìud que l'attaquant cherche √† r√©-identifier

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    RETOURNE :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    dict contenant:
        - attack_type : 'Subgraph Attack'
        - target_node : N≈ìud cibl√©
        - target_degree : Degr√© du n≈ìud cible
        - target_triangles : Nombre de triangles form√©s par le n≈ìud
        - clustering_coefficient : Coefficient de clustering
        - candidates : Liste des n≈ìuds candidats
        - success : True si r√©-identification unique
        - re_identification_probability : 1/k
        - incorrectness : k-1
        - min_entropy_bits : log‚ÇÇ(k)
        - explanation : Explication d√©taill√©e
    """

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # INITIALISATION DES R√âSULTATS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    results = {
        'attack_type': 'Subgraph Attack',
        'target_node': target_node,
        'success': False,
        'candidates': [],
        'explanation': ''
    }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # V√âRIFICATION : L'attaque n√©cessite des graphes avec structure locale
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if not isinstance(G_anon, nx.Graph):
        results['explanation'] = (
            "‚ö†Ô∏è **Attaque impossible sur ce type de graphe (super-nodes)**\n\n"
            "La G√âN√âRALISATION d√©truit les motifs locaux (triangles, voisinages).\n"
            "C'est justement la FORCE de cette m√©thode contre les attaques structurelles !\n\n"
            "üü¢ Protection : **EXCELLENTE** (‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ)"
        )
        results['incorrectness'] = float('inf')
        results['min_entropy_bits'] = float('inf')
        results['re_identification_probability'] = 0.0
        return results

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 1 : ANALYSER LA STRUCTURE LOCALE DU N≈íUD CIBLE (Graphe Original)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Compter les TRIANGLES dont le n≈ìud fait partie
    # Un triangle = 3 n≈ìuds tous connect√©s entre eux (A-B, B-C, A-C)
    target_triangles = []
    for u, v in G_orig.edges(target_node):
        # Si u et v sont aussi connect√©s ‚Üí triangle [target, u, v]
        if G_orig.has_edge(u, v):
            target_triangles.append(sorted([target_node, u, v]))

    # Si le n≈ìud n'a aucun triangle, l'attaque structurelle est limit√©e
    if not target_triangles:
        results['explanation'] = (
            f"‚ö†Ô∏è Le n≈ìud {target_node} ne fait partie d'AUCUN triangle.\n\n"
            f"Cette attaque n√©cessite des motifs structurels (triangles).\n"
            f"Utilisez plut√¥t une **Degree Attack** pour ce n≈ìud."
        )
        return results

    # Caract√©ristiques structurelles du n≈ìud cible
    target_degree = G_orig.degree(target_node)
    target_triangle_count = len(target_triangles)

    # Coefficient de clustering : proportion de voisins connect√©s entre eux
    try:
        target_clustering = nx.clustering(G_orig, target_node)
    except:
        target_clustering = 0.0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 2 : RECHERCHER LE PATTERN STRUCTUREL DANS LE GRAPHE ANONYMIS√â
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # L'attaquant cherche les n≈ìuds ayant le M√äME PATTERN : (degr√©, triangles)

    candidates = []
    for n in G_anon.nodes():
        # Filtrer d'abord par degr√© (crit√®re rapide)
        if G_anon.degree(n) == target_degree:
            # Compter les triangles pour ce n≈ìud candidat
            node_triangles = 0
            for u, v in G_anon.edges(n):
                if G_anon.has_edge(u, v):
                    node_triangles += 1

            # Si le nombre de triangles correspond aussi ‚Üí Candidat potentiel !
            if node_triangles == target_triangle_count:
                candidates.append(n)

    k = len(candidates)  # Taille de l'ensemble d'anonymat

    results['candidates'] = candidates
    results['target_degree'] = target_degree
    results['target_triangles'] = target_triangle_count
    results['clustering_coefficient'] = target_clustering

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # √âTAPE 3 : √âVALUATION DU SUCC√àS DE L'ATTAQUE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    if k == 1:
        # CAS 1 : PATTERN UNIQUE ‚Üí R√â-IDENTIFICATION R√âUSSIE
        results['success'] = True
        results['re_identified_node'] = candidates[0]
        results['re_identification_probability'] = 1.0
        results['incorrectness'] = 0
        results['min_entropy_bits'] = 0.0
        results['explanation'] = (
            f"‚úÖ **R√â-IDENTIFICATION R√âUSSIE !**\n\n"
            f"Le pattern structurel du n≈ìud {target_node} est UNIQUE :\n"
            f"- Degr√© : **{target_degree}** connexions\n"
            f"- Triangles : **{target_triangle_count}**\n"
            f"- Clustering : **{target_clustering:.3f}**\n\n"
            f"Un seul n≈ìud dans le graphe anonymis√© poss√®de ce pattern.\n\n"
            f"üìä **M√©triques de Privacy** :\n"
            f"- Probabilit√© de r√©-identification : **100%**\n"
            f"- Incorrectness : **0** (certitude absolue)\n"
            f"- Min Entropy : **0 bits**\n\n"
            f"üî¥ **DANGER** : L'attaque structurelle est PLUS PUISSANTE que l'attaque par degr√©.\n"
            f"üí° **Protection recommand√©e** : G√©n√©ralisation ou Differential Privacy"
        )

    elif k == 0:
        # CAS 2 : AUCUN CANDIDAT ‚Üí STRUCTURE MODIFI√âE
        results['success'] = False
        results['re_identification_probability'] = 0.0
        results['incorrectness'] = float('inf')
        results['min_entropy_bits'] = float('inf')
        results['explanation'] = (
            f"‚ùå **ATTAQUE √âCHOU√âE - Structure modifi√©e**\n\n"
            f"Aucun n≈ìud ne correspond au pattern recherch√© :\n"
            f"- Degr√© : {target_degree}\n"
            f"- Triangles : {target_triangle_count}\n\n"
            f"L'anonymisation a modifi√© la structure locale du graphe.\n\n"
            f"üìä **M√©triques de Privacy** :\n"
            f"- Probabilit√© de r√©-identification : **0%**\n"
            f"- Incorrectness : **‚àû**\n"
            f"- Min Entropy : **‚àû bits**\n\n"
            f"üü¢ **S√âCURIT√â** : Excellente protection contre cette attaque structurelle !"
        )

    else:
        # CAS 3 : PLUSIEURS CANDIDATS ‚Üí AMBIGU√èT√â
        results['success'] = False
        results['re_identification_probability'] = 1.0 / k
        results['incorrectness'] = k - 1
        results['min_entropy_bits'] = np.log2(k)

        # √âvaluation qualitative
        if k >= 8:
            privacy_level = "üü¢ FORTE"
            protection_comment = "Excellente protection structurelle"
        elif k >= 4:
            privacy_level = "üü° MOYENNE"
            protection_comment = "Protection acceptable"
        else:
            privacy_level = "üü† FAIBLE"
            protection_comment = "Protection limit√©e - Pattern encore distinctif"

        results['explanation'] = (
            f"‚ö†Ô∏è **R√â-IDENTIFICATION AMBIGU√ã**\n\n"
            f"{k} n≈ìuds partagent le pattern structurel :\n"
            f"- Degr√© : **{target_degree}**\n"
            f"- Triangles : **{target_triangle_count}**\n"
            f"- Clustering : **{target_clustering:.3f}**\n\n"
            f"L'attaquant doit deviner parmi {k} candidats.\n\n"
            f"üìä **M√©triques de Privacy** :\n"
            f"- Probabilit√© de r√©-identification : **{1/k*100:.1f}%** (1/{k})\n"
            f"- Incorrectness : **{k-1}** fausses suppositions\n"
            f"- Min Entropy : **{np.log2(k):.2f} bits**\n"
            f"- Niveau de privacy : {privacy_level}\n\n"
            f"üí° **Interpr√©tation** : {protection_comment}.\n\n"
            f"‚ö†Ô∏è **Note** : Cette attaque est plus discriminante qu'une simple Degree Attack.\n"
            f"Pour une meilleure protection, utilisez la G√©n√©ralisation ou Differential Privacy."
        )

    return results


def calculate_supergraph_metrics(G_orig, G_super):
    """
    Calcule les m√©triques d'utilit√© pour un SUPER-GRAPHE (G√©n√©ralisation).

    Le super-graphe a une structure diff√©rente :
    - Chaque N≈íUD = un CLUSTER (super-n≈ìud)
    - Attributs des n≈ìuds : 'cluster_size', 'internal_edges'
    - Ar√™tes INTRA-cluster : self-loops avec poids = nb d'ar√™tes internes
    - Ar√™tes INTER-cluster : ar√™tes normales avec poids = nb d'ar√™tes entre clusters

    On calcule les m√©triques directement √† partir de ces informations.
    """
    metrics = {
        'type': 'super-graph',
        'comparable': True,
        'is_super_graph': True
    }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES DE BASE (Structure du Super-Graphe)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    num_clusters = G_super.number_of_nodes()
    metrics['num_clusters'] = num_clusters

    # Extraire les tailles de clusters
    cluster_sizes = []
    total_internal_edges = 0

    for node in G_super.nodes():
        node_data = G_super.nodes[node]
        size = node_data.get('cluster_size', 0)
        internal = node_data.get('internal_edges', 0)

        cluster_sizes.append(size)
        total_internal_edges += internal

    metrics['num_nodes'] = sum(cluster_sizes)  # Total de n≈ìuds originaux
    metrics['min_cluster_size'] = min(cluster_sizes) if cluster_sizes else 0
    metrics['max_cluster_size'] = max(cluster_sizes) if cluster_sizes else 0
    metrics['avg_cluster_size'] = np.mean(cluster_sizes) if cluster_sizes else 0
    metrics['cluster_size_variance'] = np.var(cluster_sizes) if cluster_sizes else 0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES D'AR√äTES (Intra vs Inter)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Ar√™tes intra-cluster (depuis les self-loops ou attributs de n≈ìuds)
    metrics['intra_cluster_edges'] = total_internal_edges

    # Ar√™tes inter-cluster (depuis les ar√™tes entre clusters)
    inter_cluster_edges = 0
    for u, v in G_super.edges():
        if u != v:  # Exclure les self-loops
            weight = G_super[u][v].get('weight', 1)
            inter_cluster_edges += weight

    metrics['inter_cluster_edges'] = inter_cluster_edges
    metrics['num_edges'] = total_internal_edges + inter_cluster_edges

    # Ratio intra/total
    total_edges = metrics['num_edges']
    if total_edges > 0:
        metrics['intra_ratio'] = total_internal_edges / total_edges
        metrics['inter_ratio'] = inter_cluster_edges / total_edges
    else:
        metrics['intra_ratio'] = 0
        metrics['inter_ratio'] = 0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PERTE D'INFORMATION (Information Loss)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Comparer avec le graphe original
    orig_edges = G_orig.number_of_edges()
    orig_nodes = G_orig.number_of_nodes()

    # Perte de granularit√© : passage de n n≈ìuds √† k clusters
    metrics['node_compression_ratio'] = num_clusters / orig_nodes if orig_nodes > 0 else 0
    metrics['information_loss'] = 1 - metrics['node_compression_ratio']

    # Conservation des ar√™tes
    if orig_edges > 0:
        metrics['edge_preservation_ratio'] = total_edges / orig_edges
    else:
        metrics['edge_preservation_ratio'] = 0

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES STRUCTURELLES (sur le super-graphe lui-m√™me)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Densit√© du super-graphe (sans compter les self-loops)
    super_graph_no_loops = G_super.copy()
    super_graph_no_loops.remove_edges_from(nx.selfloop_edges(super_graph_no_loops))
    metrics['super_graph_density'] = nx.density(super_graph_no_loops)

    # Degr√© moyen des clusters (nb de clusters voisins)
    super_degrees = [d for n, d in super_graph_no_loops.degree()]
    metrics['avg_cluster_degree'] = np.mean(super_degrees) if super_degrees else 0
    metrics['max_cluster_degree'] = max(super_degrees) if super_degrees else 0

    # Connectivit√© du super-graphe
    metrics['super_graph_connected'] = nx.is_connected(super_graph_no_loops)

    if metrics['super_graph_connected']:
        try:
            metrics['super_graph_diameter'] = nx.diameter(super_graph_no_loops)
        except:
            metrics['super_graph_diameter'] = None
    else:
        metrics['super_graph_diameter'] = None

    return metrics


def calculate_utility_metrics(G_orig, G_anon):
    """
    Calcule les m√©triques d'utilit√© selon la th√®se (Section 3.5.2).

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    M√âTRIQUES D'UTILIT√â (selon la th√®se, lignes 2503-2636) :
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    3 GROUPES DE STATISTIQUES :

    1. DEGREE-BASED (statistiques bas√©es sur les degr√©s) :
       - Nombre d'ar√™tes (S_NE)
       - Degr√© moyen (S_AD)
       - Degr√© maximal (S_MD)
       - Variance des degr√©s (S_DV)
       - Exposant power-law (S_PL)

    2. SHORTEST PATH-BASED (statistiques bas√©es sur les chemins) :
       - Distance moyenne (S_APD)
       - Diam√®tre effectif - 90e percentile (S_EDiam)
       - Longueur de connectivit√© - moyenne harmonique (S_CL)
       - Diam√®tre (S_Diam)

    3. CLUSTERING :
       - Coefficient de clustering (S_CC) = 3 √ó triangles / triples connect√©s

    CAS SP√âCIAUX :
    - Graphes PROBABILISTES : Calculer sur √âCHANTILLONS (sample graphs)
    - G√âN√âRALISATION (super-graphe) : M√©triques adapt√©es au format cluster
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """
    metrics = {}

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CAS 1 : SUPER-GRAPHE (G√©n√©ralisation)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if not isinstance(G_anon, nx.Graph):
        # Pour la g√©n√©ralisation, on ne peut pas comparer
        return {'type': 'super-graph', 'comparable': False}

    # V√©rifier si c'est un super-graphe (a des attributs cluster)
    is_super_graph = False
    if G_anon.number_of_nodes() > 0:
        first_node = list(G_anon.nodes())[0]
        node_data = G_anon.nodes[first_node]
        is_super_graph = 'cluster_size' in node_data

    if is_super_graph:
        # M√âTRIQUES SP√âCIFIQUES POUR LE SUPER-GRAPHE
        return calculate_supergraph_metrics(G_orig, G_anon)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CAS 2 : GRAPHE PROBABILISTE ‚Üí √âchantillonner d'abord
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    is_probabilistic = False
    if G_anon.number_of_edges() > 0:
        first_edge = list(G_anon.edges())[0]
        is_probabilistic = 'probability' in G_anon[first_edge[0]][first_edge[1]]

    if is_probabilistic:
        # G√©n√©rer un √©chantillon d√©terministe depuis le graphe probabiliste
        G_sample = sample_from_probabilistic_graph(G_anon)
        metrics['is_sample'] = True
        metrics['probabilistic_edges'] = G_anon.number_of_edges()
    else:
        G_sample = G_anon
        metrics['is_sample'] = False

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GROUPE 1 : DEGREE-BASED STATISTICS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # S_NE : Nombre d'ar√™tes
    metrics['num_edges'] = G_sample.number_of_edges()
    metrics['num_nodes'] = G_sample.number_of_nodes()

    # Calculer les degr√©s
    degrees = [d for n, d in G_sample.degree()]

    # S_AD : Degr√© moyen
    metrics['avg_degree'] = np.mean(degrees) if degrees else 0

    # S_MD : Degr√© maximal
    metrics['max_degree'] = max(degrees) if degrees else 0

    # S_DV : Variance des degr√©s
    metrics['degree_variance'] = np.var(degrees) if degrees else 0

    # S_PL : Exposant power-law
    # On estime Œ≥ (gamma) de la distribution P(k) ‚àù k^(-Œ≥)
    try:
        from scipy.stats import linregress
        # Compter la distribution des degr√©s
        degree_counts = Counter(degrees)
        degrees_unique = sorted([d for d in degree_counts.keys() if d > 0])
        counts = [degree_counts[d] for d in degrees_unique]

        if len(degrees_unique) >= 3:
            # R√©gression log-log : log(P(k)) = -Œ≥ √ó log(k) + C
            log_degrees = np.log(degrees_unique)
            log_counts = np.log(counts)
            slope, intercept, r_value, p_value, std_err = linregress(log_degrees, log_counts)
            metrics['power_law_exponent'] = -slope  # Œ≥ = -slope
            metrics['power_law_r_squared'] = r_value ** 2
        else:
            metrics['power_law_exponent'] = None
            metrics['power_law_r_squared'] = None
    except:
        metrics['power_law_exponent'] = None
        metrics['power_law_r_squared'] = None

    # Densit√© (m√©trique additionnelle)
    metrics['density'] = nx.density(G_sample)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GROUPE 2 : SHORTEST PATH-BASED STATISTICS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    try:
        if nx.is_connected(G_sample):
            # S_Diam : Diam√®tre
            metrics['diameter'] = nx.diameter(G_sample)

            # S_APD : Distance moyenne
            metrics['avg_shortest_path'] = nx.average_shortest_path_length(G_sample)

            # S_EDiam : Diam√®tre effectif (90e percentile)
            # Calculer toutes les distances
            all_paths = dict(nx.all_pairs_shortest_path_length(G_sample))
            all_distances = []
            for source in all_paths:
                for target, dist in all_paths[source].items():
                    if source != target:
                        all_distances.append(dist)

            if all_distances:
                metrics['effective_diameter'] = np.percentile(all_distances, 90)

                # S_CL : Longueur de connectivit√© (moyenne harmonique)
                # CL = n(n-1) / Œ£(1/d(u,v))
                harmonic_sum = sum([1.0/d for d in all_distances if d > 0])
                n = G_sample.number_of_nodes()
                metrics['connectivity_length'] = n * (n-1) / harmonic_sum if harmonic_sum > 0 else None
            else:
                metrics['effective_diameter'] = None
                metrics['connectivity_length'] = None
        else:
            # Prendre la plus grande composante connexe
            largest_cc = max(nx.connected_components(G_sample), key=len)
            subgraph = G_sample.subgraph(largest_cc)

            metrics['diameter'] = nx.diameter(subgraph)
            metrics['avg_shortest_path'] = nx.average_shortest_path_length(subgraph)

            # Pour effective diameter et connectivity length sur la LCC
            all_paths = dict(nx.all_pairs_shortest_path_length(subgraph))
            all_distances = []
            for source in all_paths:
                for target, dist in all_paths[source].items():
                    if source != target:
                        all_distances.append(dist)

            if all_distances:
                metrics['effective_diameter'] = np.percentile(all_distances, 90)
                harmonic_sum = sum([1.0/d for d in all_distances if d > 0])
                n_lcc = subgraph.number_of_nodes()
                metrics['connectivity_length'] = n_lcc * (n_lcc-1) / harmonic_sum if harmonic_sum > 0 else None
            else:
                metrics['effective_diameter'] = None
                metrics['connectivity_length'] = None
    except:
        metrics['diameter'] = None
        metrics['avg_shortest_path'] = None
        metrics['effective_diameter'] = None
        metrics['connectivity_length'] = None

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # GROUPE 3 : CLUSTERING COEFFICIENT
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # S_CC : Clustering coefficient = 3 √ó triangles / triples connect√©s
    try:
        # Compter les triangles
        triangles = sum(nx.triangles(G_sample).values()) / 3  # Divis√© par 3 car chaque triangle compt√© 3 fois

        # Compter les triples connect√©s (chemins de longueur 2)
        connected_triples = 0
        for node in G_sample.nodes():
            degree = G_sample.degree(node)
            # Chaque n≈ìud de degr√© k cr√©e k(k-1)/2 triples
            connected_triples += degree * (degree - 1) / 2

        if connected_triples > 0:
            metrics['clustering_coefficient'] = (3 * triangles) / connected_triples
        else:
            metrics['clustering_coefficient'] = 0

        # Clustering moyen (m√©trique alternative)
        metrics['avg_clustering'] = nx.average_clustering(G_sample)
    except:
        metrics['clustering_coefficient'] = None
        metrics['avg_clustering'] = None

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTRIQUES ADDITIONNELLES : Pr√©servation de la structure
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # Corr√©lation des s√©quences de degr√©s (Spearman)
    orig_degrees = sorted([d for n, d in G_orig.degree()])
    sample_degrees = sorted([d for n, d in G_sample.degree()])

    if len(orig_degrees) == len(sample_degrees):
        from scipy.stats import spearmanr
        try:
            corr, _ = spearmanr(orig_degrees, sample_degrees)
            metrics['degree_correlation'] = corr
        except:
            metrics['degree_correlation'] = None
    else:
        metrics['degree_correlation'] = None

    # Erreur relative moyenne (comme dans la th√®se)
    # rel.err = |S(G0) - S(G)| / S(G0)
    metrics['comparable'] = True

    return metrics


def calculate_privacy_metrics_separated(G_orig, G_anon, method_key, method_params):
    """Calcule les m√©triques de privacy s√©par√©es"""
    metrics = {}

    if method_key == "k-degree anonymity":
        degrees = dict(G_anon.degree()) if isinstance(G_anon, nx.Graph) else {}
        degree_counts = Counter(degrees.values()) if degrees else Counter()
        k_value = method_params.get('k', 2)
        min_count = min(degree_counts.values()) if degree_counts else 0

        metrics['k_value'] = k_value
        metrics['min_anonymity_set'] = min_count
        metrics['satisfies_k_anonymity'] = min_count >= k_value
        metrics['re_identification_probability'] = 1/min_count if min_count > 0 else 1.0

    elif method_key in ["EdgeFlip", "Laplace"]:
        epsilon = method_params.get('epsilon', 1.0)
        metrics['epsilon_budget'] = epsilon
        metrics['privacy_loss_bound'] = np.exp(epsilon)
        metrics['privacy_level'] = "Forte (Œµ<1)" if epsilon < 1.0 else ("Moyenne (1‚â§Œµ<2)" if epsilon < 2.0 else "Faible (Œµ‚â•2)")

        if method_key == "EdgeFlip":
            s = 1 - np.exp(-epsilon)
            metrics['flip_probability'] = s
            metrics['expected_noise_edges'] = int(G_orig.number_of_edges() * s / 2)

    elif method_key == "Probabilistic":
        k = method_params.get('k', 5)
        eps = method_params.get('epsilon', 0.3)
        metrics['k_candidates'] = k
        metrics['epsilon_tolerance'] = eps
        metrics['min_entropy'] = np.log(k) - eps
        metrics['confusion_factor'] = k

    elif method_key == "MaxVar":
        num_pot = method_params.get('num_potential_edges', 50)
        metrics['num_potential_edges'] = num_pot

        # Analyser les probabilit√©s pour v√©rifier la dispersion
        if G_anon.number_of_edges() > 0:
            # S√©parer ar√™tes existantes vs potentielles
            existing_probs = []
            potential_probs = []
            all_probs = []

            for u, v in G_anon.edges():
                prob = G_anon[u][v].get('probability', 1.0)
                is_original = G_anon[u][v].get('is_original', False)
                all_probs.append(prob)

                if is_original:
                    existing_probs.append(prob)
                else:
                    potential_probs.append(prob)

            # M√©triques globales
            metrics['avg_probability'] = np.mean(all_probs)
            metrics['std_probability'] = np.std(all_probs)
            metrics['min_probability'] = np.min(all_probs)
            metrics['max_probability'] = np.max(all_probs)

            # M√©triques pour ar√™tes existantes
            if existing_probs:
                metrics['existing_avg_prob'] = np.mean(existing_probs)
                metrics['existing_std_prob'] = np.std(existing_probs)

            # M√©triques pour ar√™tes potentielles
            if potential_probs:
                metrics['potential_avg_prob'] = np.mean(potential_probs)
                metrics['potential_std_prob'] = np.std(potential_probs)

            # Calculer la variance totale (objectif maximis√©)
            total_variance = sum(p * (1 - p) for p in all_probs)
            metrics['total_variance'] = total_variance
            metrics['avg_edge_variance'] = total_variance / len(all_probs) if all_probs else 0

            # Tester la r√©sistance au seuillage
            threshold = 0.5
            reconstructed = sum(1 for p in all_probs if p > threshold)
            original_edges_count = len(existing_probs)
            if original_edges_count > 0:
                reconstruction_rate = sum(1 for p in existing_probs if p > threshold) / original_edges_count
                metrics['threshold_resistance'] = 1 - reconstruction_rate  # Plus proche de 1 = meilleur
                metrics['reconstruction_rate'] = reconstruction_rate

    elif method_key == "Generalization":
        if hasattr(G_anon, 'graph') and 'cluster_to_nodes' in G_anon.graph:
            cluster_sizes = [len(nodes) for nodes in G_anon.graph['cluster_to_nodes'].values()]
            metrics['min_cluster_size'] = min(cluster_sizes) if cluster_sizes else 0
            metrics['avg_cluster_size'] = np.mean(cluster_sizes) if cluster_sizes else 0
            metrics['max_privacy'] = 1/min(cluster_sizes) if cluster_sizes else 1.0

    return metrics


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DICTIONNAIRE DES D√âFINITIONS ET M√âTHODES DE CALCUL (pour tooltips)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

METRIC_DEFINITIONS = {
    # M√©triques de base
    'num_nodes': {
        'name': 'Nombre de N≈ìuds',
        'definition': "Nombre total de n≈ìuds dans le graphe",
        'formula': "n = |V|",
        'interpretation': "Plus √©lev√© = graphe plus grand"
    },
    'num_edges': {
        'name': 'Nombre d\'Ar√™tes',
        'definition': "Nombre total d'ar√™tes dans le graphe",
        'formula': "m = |E|",
        'interpretation': "Plus √©lev√© = graphe plus connect√©"
    },
    'density': {
        'name': 'Densit√©',
        'definition': "Proportion d'ar√™tes existantes par rapport au maximum possible",
        'formula': "D = 2m / (n(n-1))",
        'interpretation': "0 = vide, 1 = complet, ~0.1 = √©pars, ~0.5 = dense"
    },

    # Groupe 1: Degree-based
    'avg_degree': {
        'name': 'Degr√© Moyen (S_AD)',
        'definition': "Nombre moyen de voisins par n≈ìud",
        'formula': r"d_{avg} = \frac{1}{n} \sum_{v \in V} \deg(v)",
        'interpretation': "Mesure la connectivit√© moyenne du graphe"
    },
    'max_degree': {
        'name': 'Degr√© Maximal (S_MD)',
        'definition': "Plus grand nombre de voisins d'un n≈ìud",
        'formula': r"d_{max} = \max_{v \in V} \deg(v)",
        'interpretation': "Identifie les hubs (n≈ìuds tr√®s connect√©s)"
    },
    'degree_variance': {
        'name': 'Variance des Degr√©s (S_DV)',
        'definition': "Dispersion des degr√©s autour de la moyenne",
        'formula': r"\sigma^2 = \frac{1}{n} \sum_{v \in V} (\deg(v) - d_{avg})^2",
        'interpretation': "√âlev√©e = h√©t√©rog√©n√©it√© (hubs + n≈ìuds peu connect√©s)"
    },
    'power_law_exponent': {
        'name': 'Exposant Power-Law (S_PL)',
        'definition': "Caract√©rise la distribution des degr√©s pour les r√©seaux scale-free",
        'formula': r"P(k) \propto k^{-\gamma} \text{ o√π } \gamma \text{ est estim√© par r√©gression log-log}",
        'interpretation': r"\gamma \in [2,3] \text{ typique pour r√©seaux sociaux (loi de puissance)}"
    },

    # Groupe 2: Shortest path-based
    'diameter': {
        'name': 'Diam√®tre (S_Diam)',
        'definition': "Plus grande distance entre deux n≈ìuds connect√©s",
        'formula': r"D = \max_{u,v \in V} d(u,v)",
        'interpretation': "Borne sup√©rieure sur toutes les distances"
    },
    'avg_shortest_path': {
        'name': 'Distance Moyenne (S_APD)',
        'definition': "Longueur moyenne des plus courts chemins entre tous les couples",
        'formula': r"L = \frac{2}{n(n-1)} \sum_{u,v \in V} d(u,v)",
        'interpretation': "Mesure la compacit√© du r√©seau (propri√©t√© small-world)"
    },
    'effective_diameter': {
        'name': 'Diam√®tre Effectif (S_EDiam)',
        'definition': "90e percentile des distances (plus robuste que le diam√®tre)",
        'formula': r"D_{eff} = \text{Percentile}_{90}\{d(u,v)\}",
        'interpretation': r"90\% \text{ des n≈ìuds sont √† distance } \leq D_{eff}"
    },
    'connectivity_length': {
        'name': 'Longueur de Connectivit√© (S_CL)',
        'definition': "Moyenne harmonique des distances (privil√©gie les courtes distances)",
        'formula': r"CL = \frac{n(n-1)}{\sum_{u,v} \frac{1}{d(u,v)}}",
        'interpretation': "Plus faible = meilleure connectivit√© locale"
    },

    # Groupe 3: Clustering
    'clustering_coefficient': {
        'name': 'Coefficient de Clustering (S_CC)',
        'definition': "Mesure la tendance √† former des triangles (cliques locales)",
        'formula': "CC = (3 √ó nb_triangles) / nb_triples_connect√©s",
        'interpretation': "√âlev√© = forte transitivit√© (ami de mes amis = mon ami)"
    },
    'avg_clustering': {
        'name': 'Clustering Moyen',
        'definition': "Moyenne des coefficients de clustering locaux",
        'formula': r"C_{avg} = \frac{1}{n} \sum_{v \in V} C(v) \text{ o√π } C(v) = \frac{\text{triangles}(v)}{\text{triples}(v)}",
        'interpretation': "Mesure alternative du clustering (locale ‚Üí globale)"
    },

    # M√©triques de pr√©servation
    'degree_correlation': {
        'name': 'Corr√©lation des Degr√©s',
        'definition': "Corr√©lation de Spearman entre s√©quences de degr√©s (original vs anonymis√©)",
        'formula': "œÅ = Spearman(deg_orig, deg_anon)",
        'interpretation': ">0.9 = excellente pr√©servation, >0.7 = bonne, <0.7 = faible"
    },

    # M√©triques pour super-graphe (g√©n√©ralisation)
    'num_clusters': {
        'name': 'Nombre de Clusters',
        'definition': "Nombre de super-n≈ìuds dans le graphe de g√©n√©ralisation",
        'formula': "k = nombre de clusters",
        'interpretation': "Plus faible = plus de privacy, moins d'utilit√©"
    },
    'min_cluster_size': {
        'name': 'Taille Min. Cluster',
        'definition': "Plus petit nombre de n≈ìuds dans un cluster",
        'formula': r"\min_i |C_i|",
        'interpretation': r"\text{Doit √™tre } \geq k \text{ pour garantir k-anonymity}"
    },
    'avg_cluster_size': {
        'name': 'Taille Moy. Cluster',
        'definition': "Nombre moyen de n≈ìuds par cluster",
        'formula': r"\text{avg}\{|C_i|\} = \frac{n}{k}",
        'interpretation': "Plus √©lev√© = clusters plus gros = plus de privacy"
    },
    'intra_cluster_edges': {
        'name': 'Ar√™tes Intra-Cluster',
        'definition': "Nombre d'ar√™tes √† l'int√©rieur des clusters",
        'formula': "Somme des ar√™tes internes de chaque cluster",
        'interpretation': "Repr√©sentent la structure locale pr√©serv√©e"
    },
    'inter_cluster_edges': {
        'name': 'Ar√™tes Inter-Cluster',
        'definition': "Nombre d'ar√™tes entre diff√©rents clusters",
        'formula': "Ar√™tes reliant des n≈ìuds de clusters diff√©rents",
        'interpretation': "Repr√©sentent les connexions globales"
    },
    'intra_ratio': {
        'name': 'Ratio Intra/Total',
        'definition': "Proportion d'ar√™tes intra-cluster par rapport au total",
        'formula': "ratio = intra_edges / (intra_edges + inter_edges)",
        'interpretation': "√âlev√© = structure locale bien pr√©serv√©e"
    },
    'information_loss': {
        'name': 'Perte d\'Information',
        'definition': "Proportion de granularit√© perdue lors du clustering",
        'formula': "loss = 1 - (k_clusters / n_nodes)",
        'interpretation': "0 = aucune perte, 1 = perte totale (1 seul cluster)"
    },
    'edge_preservation_ratio': {
        'name': 'Taux de Pr√©servation des Ar√™tes',
        'definition': "Proportion d'ar√™tes pr√©serv√©es apr√®s anonymisation",
        'formula': "ratio = edges_anon / edges_orig",
        'interpretation': "1 = toutes pr√©serv√©es, <1 = pertes, >1 = ar√™tes ajout√©es"
    },
    'super_graph_density': {
        'name': 'Densit√© du Super-Graphe',
        'definition': "Densit√© du graphe des clusters (sans self-loops)",
        'formula': "D_super = 2m_inter / (k(k-1))",
        'interpretation': "Mesure la connectivit√© entre clusters"
    },
}


def get_metric_tooltip(metric_key):
    """
    G√©n√®re un tooltip format√© pour une m√©trique donn√©e.

    Args:
        metric_key: Cl√© de la m√©trique dans METRIC_DEFINITIONS

    Returns:
        String format√© pour le param√®tre 'help' de st.metric()
    """
    if metric_key not in METRIC_DEFINITIONS:
        return None

    info = METRIC_DEFINITIONS[metric_key]

    tooltip = (
        f"üìñ **D√©finition**: {info['definition']}\n\n"
        f"üìê **Formule**: {info['formula']}\n\n"
        f"üí° **Interpr√©tation**: {info['interpretation']}"
    )

    return tooltip


def main():
    """Application principale Streamlit"""

    # En-t√™te
    st.markdown('<p class="main-header">üîí Anonymisation de Graphes Sociaux</p>',
                unsafe_allow_html=True)
    st.markdown("---")

    st.markdown("""
    ### Application Interactive bas√©e sur la th√®se de NGUYEN Huu-Hiep (2016)

    Cette application d√©montre les **5 types de m√©thodes d'anonymisation** de graphes sociaux
    avec explications math√©matiques d√©taill√©es et m√©triques d'anonymisation.
    """)

    # Sidebar - S√©lection de la m√©thode
    st.sidebar.title("‚öôÔ∏è Configuration")

    st.sidebar.markdown("### üìä Graphe de Test")
    graph_choice = st.sidebar.selectbox(
        "Choisir un graphe",
        ["Karate Club (34 n≈ìuds)", "Graphe Al√©atoire Petit (20 n≈ìuds)",
         "Graphe Al√©atoire Moyen (50 n≈ìuds)"]
    )

    # Charger le graphe
    if "Karate" in graph_choice:
        G = nx.karate_club_graph()
        st.sidebar.success(f"‚úì Graphe Karate Club charg√©: {G.number_of_nodes()} n≈ìuds, {G.number_of_edges()} ar√™tes")

        # Mini pr√©sentation du Karate Club
        with st.sidebar.expander("‚ÑπÔ∏è √Ä propos du Karate Club", expanded=False):
            st.markdown("""
            **Graphe de Zachary** (1977)

            R√©seau social d'un club de karat√© universitaire :
            - **34 membres** (n≈ìuds)
            - **78 relations** sociales (ar√™tes)
            - **2 communaut√©s** form√©es apr√®s une scission r√©elle du club

            Graphe de r√©f√©rence classique en analyse de r√©seaux sociaux.
            """)
    elif "Petit" in graph_choice:
        G = nx.erdos_renyi_graph(20, 0.15, seed=42)
        st.sidebar.success(f"‚úì Graphe al√©atoire charg√©: {G.number_of_nodes()} n≈ìuds, {G.number_of_edges()} ar√™tes")
    else:
        G = nx.erdos_renyi_graph(50, 0.1, seed=42)
        st.sidebar.success(f"‚úì Graphe al√©atoire charg√©: {G.number_of_nodes()} n≈ìuds, {G.number_of_edges()} ar√™tes")

    st.sidebar.markdown("---")
    st.sidebar.markdown("### üî¨ M√©thode d'Anonymisation")

    method_key = st.sidebar.selectbox(
        "Choisir une m√©thode",
        list(METHODS.keys()),
        format_func=lambda x: METHODS[x]["name"]
    )

    method = METHODS[method_key]

    st.sidebar.markdown(f"**Cat√©gorie** : {method['category']}")
    st.sidebar.markdown(f"**Description** : {method['description_short']}")

    # R√©f√©rences √† la th√®se pour cette m√©thode
    method_internal_key = {
        "k-degree anonymity": "KDegreeAnonymity",
        "Generalization": "Generalization",
        "Probabilistic": "ProbabilisticObfuscation",
        "MaxVar": "MaxVar",
        "EdgeFlip": "EdgeFlip",
        "Laplace": "Laplace"
    }.get(method_key, None)

    if method_internal_key:
        refs = get_method_references(method_internal_key)
        if refs:
            with st.sidebar.expander("üìñ R√©f√©rences Th√®se", expanded=False):
                st.markdown("**Sources dans la th√®se:**")
                for ref_key in refs[:3]:  # Afficher max 3 r√©f√©rences principales
                    if ref_key in THESIS_REFERENCES:
                        ref = THESIS_REFERENCES[ref_key]
                        st.markdown(f"‚Ä¢ **p.{ref['page']}** - {ref['section']}")
                st.caption("üí° Cliquez sur 'Voir la Th√®se' en bas pour plus de d√©tails")

    # Section de param√®tres modulables
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ‚öôÔ∏è Budget de Privacy (Modulable)")

    # Param√®tres dynamiques selon la m√©thode
    dynamic_params = {}

    if method_key in ["Random Add/Del", "Random Switch"]:
        k_value = st.sidebar.slider(
            "k = Nombre de modifications",
            min_value=5,
            max_value=50,
            value=method['params']['k'],
            step=5,
            help="Nombre d'ar√™tes √† modifier (ajout/suppression ou √©change)"
        )
        dynamic_params['k'] = k_value

    elif method_key == "k-degree anonymity":
        k_value = st.sidebar.slider(
            "k = Taille minimale des groupes",
            min_value=2,
            max_value=10,
            value=method['params']['k'],
            step=1,
            help="Nombre minimum de n≈ìuds ayant le m√™me degr√©"
        )
        dynamic_params['k'] = k_value

    elif method_key == "Generalization":
        k_value = st.sidebar.slider(
            "k = Taille minimale des clusters",
            min_value=2,
            max_value=10,
            value=method['params']['k'],
            step=1,
            help="Nombre minimum de n≈ìuds dans chaque cluster"
        )
        dynamic_params['k'] = k_value

    elif method_key == "Probabilistic":
        k_value = st.sidebar.slider(
            "k = Nombre de graphes candidats",
            min_value=3,
            max_value=15,
            value=method['params']['k'],
            step=1,
            help="Nombre minimum de graphes plausibles"
        )
        epsilon_value = st.sidebar.slider(
            "Œµ = Taux de transfert de probabilit√©",
            min_value=0.1,
            max_value=10.0,
            value=method['params']['epsilon'],
            step=0.1,
            help="‚ö†Ô∏è ATTENTION : Plus Œµ est GRAND, plus on transf√®re de probabilit√© des ar√™tes existantes vers les potentielles ‚Üí PLUS DE PRIVACY ! Avec Œµ grand (ex: 5.0), p_exist diminue et p_potential augmente, rendant le graphe plus anonymis√©."
        )
        dynamic_params['k'] = k_value
        dynamic_params['epsilon'] = epsilon_value

    elif method_key == "MaxVar":
        num_pot_edges = st.sidebar.slider(
            "Nombre d'ar√™tes potentielles",
            min_value=20,
            max_value=100,
            value=method['params']['num_potential_edges'],
            step=10,
            help="Nombre d'ar√™tes potentielles (nearby, distance=2) √† ajouter avant l'optimisation. Plus ce nombre est √©lev√©, plus la variance peut √™tre maximis√©e, mais le calcul est plus long."
        )
        dynamic_params['num_potential_edges'] = num_pot_edges

    elif method_key in ["EdgeFlip", "Laplace"]:
        epsilon_value = st.sidebar.slider(
            "Œµ = Budget de Privacy",
            min_value=0.1,
            max_value=3.0,
            value=method['params']['epsilon'],
            step=0.1,
            help="""üìñ Budget de Privacy Diff√©rentielle (Œµ-DP)

FORMULE CORRECTE : s = 2/(e^Œµ + 1), flip_probability = s/2

Trade-off Privacy-Utilit√© :
‚Ä¢ Œµ PETIT = FORTE privacy (beaucoup de modifications)
‚Ä¢ Œµ GRAND = FAIBLE privacy (peu de modifications)

Exemples concrets :
‚Ä¢ Œµ = 0.1 (petit): flip_prob = 47.5% ‚Üí graphe tr√®s diff√©rent ‚Üí FORTE privacy ‚úì
‚Ä¢ Œµ = 1.0 (moyen): flip_prob = 26.9% ‚Üí changements mod√©r√©s ‚Üí privacy moyenne
‚Ä¢ Œµ = 3.0 (grand): flip_prob = 4.7% ‚Üí graphe proche ‚Üí FAIBLE privacy

En DP, epsilon mesure la "perte de privacy" : plus c'est petit, mieux c'est !"""
        )
        dynamic_params['epsilon'] = epsilon_value

        # Afficher l'impact du budget AVEC LA FORMULE CORRECTE
        privacy_loss = np.exp(epsilon_value)
        s = 2 / (np.exp(epsilon_value) + 1)  # FORMULE CORRECTE
        flip_prob = s / 2

        if epsilon_value < 1.0:
            st.sidebar.success(f"‚úÖ Privacy Forte (Œµ={epsilon_value:.1f})")
            st.sidebar.caption(f"Flip: {flip_prob*100:.1f}% | Graphe tr√®s modifi√©")
        elif epsilon_value < 2.0:
            st.sidebar.warning(f"‚ö†Ô∏è Privacy Moyenne (Œµ={epsilon_value:.1f})")
            st.sidebar.caption(f"Flip: {flip_prob*100:.1f}% | Modifications mod√©r√©es")
        else:
            st.sidebar.error(f"‚ùå Privacy Faible (Œµ={epsilon_value:.1f})")
            st.sidebar.caption(f"Flip: {flip_prob*100:.1f}% | Graphe proche de l'original")

    # Bouton pour anonymiser
    st.sidebar.markdown("---")
    if st.sidebar.button("üöÄ Anonymiser le Graphe", type="primary"):
        st.session_state.anonymized = True
        st.session_state.method_key = method_key
        st.session_state.method_params = dynamic_params  # Sauvegarder les param√®tres utilis√©s
        st.session_state.show_sample = False  # R√©initialiser l'affichage d'√©chantillon

        # Anonymiser
        anonymizer = GraphAnonymizer(G)

        with st.spinner('Anonymisation en cours...'):
            node_to_cluster = None
            if method_key == "Random Add/Del":
                G_anon = anonymizer.random_add_del(**dynamic_params)
            elif method_key == "Random Switch":
                G_anon = anonymizer.random_switch(**dynamic_params)
            elif method_key == "k-degree anonymity":
                G_anon = anonymizer.k_degree_anonymity(**dynamic_params)
            elif method_key == "Generalization":
                G_anon, node_to_cluster = anonymizer.generalization(**dynamic_params)
                st.session_state.node_to_cluster = node_to_cluster
            elif method_key == "Probabilistic":
                G_anon = anonymizer.probabilistic_obfuscation(**dynamic_params)
            elif method_key == "MaxVar":
                G_anon = anonymizer.maxvar_obfuscation(**dynamic_params)
            elif method_key == "EdgeFlip":
                G_anon = anonymizer.differential_privacy_edgeflip(**dynamic_params)
            elif method_key == "Laplace":
                G_anon = anonymizer.differential_privacy_laplace(**dynamic_params)

            st.session_state.G_anon = G_anon
            st.session_state.G_orig = G
            if node_to_cluster is None:
                st.session_state.node_to_cluster = None

    # Bouton pour tirer un √©chantillon (seulement pour graphes probabilistes)
    if 'anonymized' in st.session_state and st.session_state.anonymized:
        if st.session_state.method_key in ["Probabilistic", "MaxVar"]:
            st.sidebar.markdown("---")
            st.sidebar.markdown("### üé≤ √âchantillonnage")
            st.sidebar.caption("Les m√©thodes probabilistes publient un graphe incertain. Tirez un √©chantillon d√©terministe!")

            if st.sidebar.button("üé≤ Tirer un √âchantillon", type="secondary"):
                with st.spinner('Tirage d\'√©chantillon en cours...'):
                    G_sample = sample_from_probabilistic_graph(st.session_state.G_anon)
                    st.session_state.G_sample = G_sample
                    st.session_state.show_sample = True

            if st.session_state.get('show_sample', False):
                st.sidebar.success("‚úÖ √âchantillon tir√©!")
                st.sidebar.caption(f"N≈ìuds: {st.session_state.G_sample.number_of_nodes()}, Ar√™tes: {st.session_state.G_sample.number_of_edges()}")

                if st.sidebar.button("üîÑ Afficher graphe incertain", type="secondary"):
                    st.session_state.show_sample = False

    # Section Th√®se PDF
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìö Th√®se de R√©f√©rence")

    with st.sidebar.expander("üìñ Voir la Th√®se", expanded=False):
        st.markdown("""
        **"Anonymisation de Graphes Sociaux"**
        *NGUYEN Huu-Hiep (2016)*

        Universit√© de Lorraine
        """)

        # Afficher le PDF
        try:
            import base64
            with open("assets/thesis.pdf", "rb") as f:
                base64_pdf = base64.b64encode(f.read()).decode('utf-8')

            # Bouton pour t√©l√©charger le PDF
            st.download_button(
                label="üì• T√©l√©charger la th√®se (PDF)",
                data=open("assets/thesis.pdf", "rb").read(),
                file_name="NGUYEN_Anonymisation_Graphes_Sociaux_2016.pdf",
                mime="application/pdf"
            )

            # Afficher un iframe avec le PDF
            pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="400" type="application/pdf"></iframe>'
            st.markdown(pdf_display, unsafe_allow_html=True)

        except FileNotFoundError:
            st.warning("Fichier PDF non trouv√©. Placez thesis.pdf dans le dossier assets/")

    # Affichage des r√©sultats
    if 'anonymized' in st.session_state and st.session_state.anonymized:
        G_orig = st.session_state.G_orig
        G_anon = st.session_state.G_anon

        # Utiliser l'√©chantillon si disponible pour l'affichage
        G_display = st.session_state.get('G_sample', G_anon) if st.session_state.get('show_sample', False) else G_anon

        current_method = METHODS[st.session_state.method_key]

        # Onglets - VERSION AM√âLIOR√âE avec 8 onglets
        tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
            "üìä R√©sultats",
            "üìñ D√©finitions",
            "üìà M√©triques Utilit√©",
            "üîí M√©triques Privacy",
            "üéØ Simulations d'Attaques",
            "üõ°Ô∏è Attaques & Garanties",
            "üìö Dict. Attaques",
            "üîç Dict. Propri√©t√©s"
        ])

        with tab1:
            st.markdown("## üìä R√©sultats de l'Anonymisation")

            # Indicateur si on affiche un √©chantillon
            if st.session_state.get('show_sample', False):
                st.info("üé≤ **Affichage d'un graphe √©chantillon** tir√© depuis le graphe incertain. Les probabilit√©s ont √©t√© converties en ar√™tes d√©terministes.")

            col1, col2 = st.columns(2)

            with col1:
                st.metric("N≈ìuds Originaux", G_orig.number_of_nodes())
                st.metric("Ar√™tes Originales", G_orig.number_of_edges())

            with col2:
                if isinstance(G_display, nx.Graph):
                    label = "√âchantillon" if st.session_state.get('show_sample', False) else "Anonymis√©s"
                    st.metric(f"N≈ìuds {label}", G_display.number_of_nodes())
                    st.metric(f"Ar√™tes {label}", G_display.number_of_edges(),
                             delta=f"{G_display.number_of_edges() - G_orig.number_of_edges():+d}")
                else:
                    st.info("Format de graphe non standard (super-nodes)")

            st.markdown("---")
            st.markdown("### Comparaison Visuelle")

            node_to_cluster = st.session_state.get('node_to_cluster', None)
            # Utiliser G_display pour la visualisation
            fig = plot_graph_comparison(G_orig, G_display, current_method['name'], node_to_cluster)
            st.pyplot(fig)

            # Afficher les statistiques sp√©cifiques aux super-nodes
            if st.session_state.method_key == "Generalization" and hasattr(G_anon, 'graph'):
                st.markdown("---")
                st.markdown("### üìä Statistiques des Super-Nodes")

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Ar√™tes Intra-Cluster", G_anon.graph.get('intra_edges', 'N/A'),
                             help="Ar√™tes √† l'int√©rieur des clusters (vert)")
                with col2:
                    st.metric("Ar√™tes Inter-Cluster", G_anon.graph.get('inter_edges', 'N/A'),
                             help="Ar√™tes entre diff√©rents clusters (rouge)")
                with col3:
                    total = G_anon.graph.get('intra_edges', 0) + G_anon.graph.get('inter_edges', 0)
                    ratio = G_anon.graph.get('intra_edges', 0) / total * 100 if total > 0 else 0
                    st.metric("Ratio Intra/Total", f"{ratio:.1f}%")

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # SECTION SP√âCIALE : TIRAGE D'√âCHANTILLONS (Graphes Probabilistes)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if st.session_state.method_key == "Probabilistic" and isinstance(G_anon, nx.Graph):
                # V√©rifier si c'est un graphe probabiliste
                if G_anon.number_of_edges() > 0:
                    first_edge = list(G_anon.edges())[0]
                    if 'probability' in G_anon[first_edge[0]][first_edge[1]]:
                        st.markdown("---")
                        st.markdown("### üé≤ Tirage d'√âchantillons depuis le Graphe Probabiliste")

                        st.info("""
                        **üí° Principe du Tirage (Sampling)** :

                        Dans un graphe probabiliste (k,Œµ)-obfuscation, chaque ar√™te a une **probabilit√© d'existence**.
                        Au lieu de publier le graphe probabiliste directement, on peut publier des **graphes √©chantillons**
                        tir√©s au sort selon ces probabilit√©s.

                        **üîç Visualisation** :
                        - **Ar√™tes pleines (‚îÄ‚îÄ‚îÄ)** : Ar√™tes du graphe **original**
                        - **Ar√™tes pointill√©s (¬∑¬∑¬∑)** : Ar√™tes **potentielles** ajout√©es pour l'anonymisation
                        - **Couleur & √âpaisseur** : Indiquent la probabilit√© d'existence

                        - **Ar√™tes √† haute probabilit√©** (‚âà 95%) : Apparaissent presque toujours
                        - **Ar√™tes √† faible probabilit√©** (‚âà 10%) : Apparaissent rarement

                        Cliquez sur le bouton ci-dessous pour g√©n√©rer 3 √©chantillons diff√©rents !
                        """)

                        # Bouton pour g√©n√©rer des √©chantillons
                        if st.button("üé≤ G√©n√©rer 3 √âchantillons Al√©atoires", key="sample_btn"):
                            st.markdown("#### √âchantillons G√©n√©r√©s :")

                            cols = st.columns(3)
                            for i, col in enumerate(cols):
                                with col:
                                    # G√©n√©rer un √©chantillon
                                    sampled_graph = sample_from_probabilistic_graph(G_anon)

                                    # Cr√©er une figure pour cet √©chantillon
                                    fig_sample, ax_sample = plt.subplots(1, 1, figsize=(6, 6))

                                    pos = nx.spring_layout(G_orig, seed=42, k=0.5, iterations=50)

                                    # Dessiner les n≈ìuds
                                    nx.draw_networkx_nodes(sampled_graph, pos, ax=ax_sample,
                                                          node_color='lightyellow',
                                                          node_size=400, alpha=0.9,
                                                          edgecolors='orange', linewidths=2)

                                    # Dessiner les ar√™tes
                                    nx.draw_networkx_edges(sampled_graph, pos, ax=ax_sample,
                                                          edge_color='gray', width=1.5, alpha=0.6)

                                    # Labels
                                    nx.draw_networkx_labels(sampled_graph, pos, ax=ax_sample,
                                                           font_size=7, font_weight='bold')

                                    ax_sample.set_title(f'√âchantillon #{i+1}\n{sampled_graph.number_of_edges()} ar√™tes',
                                                       fontsize=12, fontweight='bold')
                                    ax_sample.axis('off')

                                    plt.tight_layout()
                                    st.pyplot(fig_sample)
                                    plt.close(fig_sample)

                                    # Afficher les stats
                                    st.caption(f"**{sampled_graph.number_of_nodes()}** n≈ìuds | **{sampled_graph.number_of_edges()}** ar√™tes")

                        st.markdown("""
                        **üîç Observation** : Chaque √©chantillon est diff√©rent ! C'est cette variabilit√© qui cr√©e
                        de l'incertitude pour l'attaquant. Il ne peut pas savoir quel √©chantillon correspond au graphe original.
                        """)

            st.markdown("---")
            st.markdown("### Distribution des Degr√©s")

            fig_dist = plot_degree_distribution(G_orig, G_anon, current_method['name'])
            st.pyplot(fig_dist)

            # Explication de la m√©thode actuelle (d√©plac√©e depuis tab2)
            st.markdown("---")
            st.markdown(f"### üî¨ Explication : {current_method['name']}")

            with st.expander("üìö D√©tails de la m√©thode", expanded=False):
                st.markdown(current_method['description'])
                st.markdown("**Formule** :")
                st.latex(current_method['formula'])

                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**üîí Niveau de Privacy**")
                    st.info(current_method['privacy_level'])
                with col2:
                    st.markdown("**üìä Pr√©servation de l'Utilit√©**")
                    st.info(current_method['utility_preservation'])

        with tab2:
            st.markdown("## üìñ D√©finitions des Concepts d'Anonymisation")

            st.markdown("""
            Cette section pr√©sente les d√©finitions formelles et intuitions pour chaque type d'anonymisation.
            Choisissez un concept ci-dessous pour voir sa d√©finition compl√®te.
            """)

            st.markdown("---")

            # S√©lecteur de concept
            concept_keys = list(ANONYMIZATION_DEFINITIONS.keys())
            concept_names = [ANONYMIZATION_DEFINITIONS[k]['name'] for k in concept_keys]

            selected_concept_name = st.selectbox(
                "Choisir un concept √† explorer",
                concept_names
            )

            # Trouver la cl√© correspondante
            selected_concept_key = concept_keys[concept_names.index(selected_concept_name)]
            concept = ANONYMIZATION_DEFINITIONS[selected_concept_key]

            st.markdown(f"### {concept['name']}")

            with st.expander("üìù D√©finition Formelle", expanded=True):
                st.markdown(concept['definition'])
                st.markdown("**Formule math√©matique** :")
                st.latex(concept['math_formula'])

            with st.expander("üí° Intuition (Explication en langage naturel)", expanded=True):
                st.markdown(concept['intuition'])

            with st.expander("üîí Garantie de Privacy"):
                st.info(f"**Garantie** : {concept['privacy_guarantee']}")

            with st.expander("‚öôÔ∏è Signification des Param√®tres"):
                st.markdown(concept['parameter_meaning'])

        with tab3:
            st.markdown("## üìà M√©triques d'Utilit√© du Graphe")

            st.markdown("""
            Ces m√©triques mesurent la **pr√©servation de l'utilit√©** du graphe apr√®s anonymisation.
            Plus ces m√©triques sont proches du graphe original, mieux l'utilit√© est pr√©serv√©e.

            üí° **Astuce** : Passez votre souris sur le ‚ÑπÔ∏è √† c√¥t√© de chaque m√©trique pour voir sa d√©finition et m√©thode de calcul.
            """)

            utility_metrics = calculate_utility_metrics(G_orig, G_anon)

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # CAS 1 : SUPER-GRAPHE (G√©n√©ralisation)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if utility_metrics.get('is_super_graph', False):
                st.info("üîç **Type de graphe** : Super-Graphe (G√©n√©ralisation) - M√©triques adapt√©es au format cluster")

                st.markdown("### üèòÔ∏è M√©triques de Clustering")

                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("Nombre de Clusters",
                             utility_metrics.get('num_clusters', 'N/A'),
                             help=get_metric_tooltip('num_clusters'))
                with col2:
                    st.metric("Taille Min. Cluster",
                             utility_metrics.get('min_cluster_size', 'N/A'),
                             help=get_metric_tooltip('min_cluster_size'))
                with col3:
                    st.metric("Taille Moy. Cluster",
                             f"{utility_metrics.get('avg_cluster_size', 0):.1f}",
                             help=get_metric_tooltip('avg_cluster_size'))
                with col4:
                    st.metric("Taille Max. Cluster",
                             utility_metrics.get('max_cluster_size', 'N/A'))

                st.markdown("---")
                st.markdown("### üîó M√©triques d'Ar√™tes")

                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric("Ar√™tes Intra-Cluster",
                             utility_metrics.get('intra_cluster_edges', 'N/A'),
                             help=get_metric_tooltip('intra_cluster_edges'))
                with col2:
                    st.metric("Ar√™tes Inter-Cluster",
                             utility_metrics.get('inter_cluster_edges', 'N/A'),
                             help=get_metric_tooltip('inter_cluster_edges'))
                with col3:
                    intra_ratio = utility_metrics.get('intra_ratio', 0)
                    st.metric("Ratio Intra/Total",
                             f"{intra_ratio*100:.1f}%",
                             help=get_metric_tooltip('intra_ratio'))

                st.markdown("---")
                st.markdown("### üìä Perte d'Information")

                col1, col2, col3 = st.columns(3)

                with col1:
                    info_loss = utility_metrics.get('information_loss', 0)
                    st.metric("Perte d'Information",
                             f"{info_loss*100:.1f}%",
                             help=get_metric_tooltip('information_loss'))
                with col2:
                    edge_pres = utility_metrics.get('edge_preservation_ratio', 0)
                    st.metric("Pr√©servation des Ar√™tes",
                             f"{edge_pres*100:.1f}%",
                             help=get_metric_tooltip('edge_preservation_ratio'))
                with col3:
                    super_density = utility_metrics.get('super_graph_density', 0)
                    st.metric("Densit√© Super-Graphe",
                             f"{super_density:.3f}",
                             help=get_metric_tooltip('super_graph_density'))

                # Afficher un r√©sum√© comparatif
                st.markdown("---")
                st.markdown("### üìâ Comparaison Original ‚Üî Anonymis√©")

                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("**Graphe Original**")
                    st.metric("N≈ìuds", G_orig.number_of_nodes())
                    st.metric("Ar√™tes", G_orig.number_of_edges())

                with col2:
                    st.markdown("**Super-Graphe**")
                    st.metric("Clusters (super-n≈ìuds)", utility_metrics.get('num_clusters', 'N/A'))
                    st.metric("Ar√™tes Totales", utility_metrics.get('num_edges', 'N/A'))

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # CAS 2 : GRAPHE CLASSIQUE ou PROBABILISTE
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            elif utility_metrics.get('comparable', True):
                if utility_metrics.get('is_sample', False):
                    st.info("üé≤ **Type de graphe** : √âchantillon tir√© depuis un graphe probabiliste")

                st.markdown("### üìä M√©triques de Base")

                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("N≈ìuds",
                             utility_metrics.get('num_nodes', 'N/A'),
                             help=get_metric_tooltip('num_nodes'))
                with col2:
                    st.metric("Ar√™tes",
                             utility_metrics.get('num_edges', 'N/A'),
                             help=get_metric_tooltip('num_edges'))
                with col3:
                    orig_density = nx.density(G_orig)
                    anon_density = utility_metrics.get('density', 0)
                    delta_density = anon_density - orig_density
                    st.metric("Densit√©",
                             f"{anon_density:.3f}",
                             delta=f"{delta_density:+.3f}",
                             help=get_metric_tooltip('density'))
                with col4:
                    if utility_metrics.get('avg_clustering') is not None:
                        orig_clust = nx.average_clustering(G_orig)
                        anon_clust = utility_metrics['avg_clustering']
                        delta_clust = anon_clust - orig_clust
                        st.metric("Clustering Moyen",
                                 f"{anon_clust:.3f}",
                                 delta=f"{delta_clust:+.3f}",
                                 help=get_metric_tooltip('avg_clustering'))

                st.markdown("---")
                st.markdown("### üåê M√©triques Globales")

                col1, col2, col3 = st.columns(3)

                with col1:
                    if utility_metrics.get('diameter') is not None:
                        try:
                            if nx.is_connected(G_orig):
                                orig_diam = nx.diameter(G_orig)
                            else:
                                largest_cc = max(nx.connected_components(G_orig), key=len)
                                orig_diam = nx.diameter(G_orig.subgraph(largest_cc))
                            delta_diam = utility_metrics['diameter'] - orig_diam
                            st.metric("Diam√®tre",
                                     utility_metrics['diameter'],
                                     delta=f"{delta_diam:+d}",
                                     help=get_metric_tooltip('diameter'))
                        except:
                            st.metric("Diam√®tre",
                                     utility_metrics['diameter'],
                                     help=get_metric_tooltip('diameter'))

                with col2:
                    if utility_metrics.get('avg_shortest_path') is not None:
                        try:
                            if nx.is_connected(G_orig):
                                orig_asp = nx.average_shortest_path_length(G_orig)
                            else:
                                largest_cc = max(nx.connected_components(G_orig), key=len)
                                orig_asp = nx.average_shortest_path_length(G_orig.subgraph(largest_cc))
                            delta_asp = utility_metrics['avg_shortest_path'] - orig_asp
                            st.metric("Chemin Moyen",
                                     f"{utility_metrics['avg_shortest_path']:.2f}",
                                     delta=f"{delta_asp:+.2f}",
                                     help=get_metric_tooltip('avg_shortest_path'))
                        except:
                            st.metric("Chemin Moyen",
                                     f"{utility_metrics['avg_shortest_path']:.2f}",
                                     help=get_metric_tooltip('avg_shortest_path'))

                with col3:
                    if utility_metrics.get('degree_correlation') is not None:
                        st.metric("Corr√©lation des Degr√©s",
                                 f"{utility_metrics['degree_correlation']:.3f}",
                                 help=get_metric_tooltip('degree_correlation'))

                st.markdown("---")
                st.markdown("### üìâ Trade-off Utilit√© vs Modifications")

                metrics = calculate_anonymization_metrics(G_orig, G_anon)

                if metrics:
                    col1, col2 = st.columns(2)

                    with col1:
                        st.markdown("**Modifications des Ar√™tes**")
                        added = metrics.get('edges_added', 0)
                        removed = metrics.get('edges_removed', 0)
                        preserved = metrics.get('edges_preserved', 0)

                        df_edges = pd.DataFrame({
                            'Type': ['Pr√©serv√©es', 'Ajout√©es', 'Supprim√©es'],
                            'Nombre': [preserved, added, removed]
                        })
                        st.bar_chart(df_edges.set_index('Type'))

                    with col2:
                        st.markdown("**Taux de Modification**")
                        rate = metrics.get('modification_rate', 0)
                        st.progress(min(rate, 1.0))
                        st.metric("Taux de modification", f"{rate*100:.1f}%")

                        if rate < 0.1:
                            st.success("‚úÖ Utilit√© tr√®s bien pr√©serv√©e")
                        elif rate < 0.3:
                            st.info("‚ÑπÔ∏è Utilit√© correctement pr√©serv√©e")
                        else:
                            st.warning("‚ö†Ô∏è Modifications importantes")

            else:
                st.warning("‚ö†Ô∏è Type de graphe non reconnu - impossible de calculer les m√©triques")

        with tab4:
            st.markdown("## üîí M√©triques de Privacy")

            st.markdown("""
            Ces m√©triques quantifient la **protection de la vie priv√©e** offerte par l'anonymisation.
            Plus ces valeurs sont √©lev√©es, meilleure est la protection.
            """)

            method_params = st.session_state.get('method_params', {})
            privacy_metrics = calculate_privacy_metrics_separated(G_orig, G_anon, st.session_state.method_key, method_params)

            if privacy_metrics:
                st.markdown("### üõ°Ô∏è Garanties de Privacy")

                if 'k_value' in privacy_metrics:
                    # k-anonymity
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("k requis", privacy_metrics['k_value'])
                    with col2:
                        st.metric("Ensemble d'anonymat min.", privacy_metrics['min_anonymity_set'])
                    with col3:
                        satisfies = privacy_metrics['satisfies_k_anonymity']
                        if satisfies:
                            st.success(f"‚úÖ {privacy_metrics['k_value']}-anonymit√© satisfaite")
                        else:
                            st.error(f"‚ùå {privacy_metrics['k_value']}-anonymit√© NON satisfaite")

                    st.markdown("---")
                    prob = privacy_metrics['re_identification_probability']
                    st.markdown(f"**Probabilit√© de r√©-identification** : {prob:.3f} ({prob*100:.1f}%)")

                    st.progress(1 - prob)

                    if prob < 0.2:
                        st.success("‚úÖ Risque de r√©-identification faible")
                    elif prob < 0.5:
                        st.warning("‚ö†Ô∏è Risque de r√©-identification mod√©r√©")
                    else:
                        st.error("‚ùå Risque de r√©-identification √©lev√©")

                elif 'epsilon_budget' in privacy_metrics:
                    # Differential Privacy
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        eps = privacy_metrics['epsilon_budget']
                        st.metric("Œµ (epsilon) Budget", f"{eps:.2f}")

                    with col2:
                        loss = privacy_metrics['privacy_loss_bound']
                        st.metric("Borne de perte de privacy", f"e^{eps:.2f} = {loss:.2f}x")

                    with col3:
                        level = privacy_metrics['privacy_level']
                        if "Forte" in level:
                            st.success(f"‚úÖ {level}")
                        elif "Moyenne" in level:
                            st.warning(f"‚ö†Ô∏è {level}")
                        else:
                            st.error(f"‚ùå {level}")

                    st.markdown("---")

                    if 'flip_probability' in privacy_metrics:
                        st.markdown("### üé≤ EdgeFlip - Param√®tres de Randomisation")
                        col1, col2 = st.columns(2)

                        with col1:
                            flip_prob = privacy_metrics['flip_probability']
                            st.metric("Probabilit√© de flip", f"{flip_prob:.3f}")

                        with col2:
                            expected_noise = privacy_metrics['expected_noise_edges']
                            st.metric("Ar√™tes bruit√©es (attendu)", expected_noise)

                elif 'k_candidates' in privacy_metrics:
                    # Probabilistic
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("k graphes candidats", privacy_metrics['k_candidates'])

                    with col2:
                        st.metric("Œµ tol√©rance", f"{privacy_metrics['epsilon_tolerance']:.2f}")

                    with col3:
                        entropy = privacy_metrics['min_entropy']
                        st.metric("Entropie minimale", f"{entropy:.2f}")

                    st.markdown("---")
                    confusion = privacy_metrics['confusion_factor']
                    st.info(f"**Facteur de confusion** : {confusion} graphes plausibles")

                elif 'num_potential_edges' in privacy_metrics:
                    # MaxVar
                    st.markdown("### üîí MaxVar - M√©triques de Variance et Dispersion")

                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("Ar√™tes potentielles ajout√©es", privacy_metrics['num_potential_edges'])

                    with col2:
                        if 'total_variance' in privacy_metrics:
                            var = privacy_metrics['total_variance']
                            st.metric("Variance totale", f"{var:.2f}",
                                     help="Plus √©lev√©e = meilleure dispersion des probabilit√©s")

                    with col3:
                        if 'avg_edge_variance' in privacy_metrics:
                            avg_var = privacy_metrics['avg_edge_variance']
                            st.metric("Variance moyenne/ar√™te", f"{avg_var:.3f}")

                    st.markdown("---")
                    st.markdown("### üìä Analyse des Probabilit√©s")

                    col1, col2 = st.columns(2)

                    with col1:
                        st.markdown("**Ar√™tes existantes (originales)**")
                        if 'existing_avg_prob' in privacy_metrics:
                            st.metric("Probabilit√© moyenne", f"{privacy_metrics['existing_avg_prob']:.3f}")
                            if 'existing_std_prob' in privacy_metrics:
                                st.metric("√âcart-type", f"{privacy_metrics['existing_std_prob']:.3f}",
                                         help="Plus √©lev√© = probabilit√©s plus dispers√©es")

                    with col2:
                        st.markdown("**Ar√™tes potentielles (ajout√©es)**")
                        if 'potential_avg_prob' in privacy_metrics:
                            st.metric("Probabilit√© moyenne", f"{privacy_metrics['potential_avg_prob']:.3f}")
                            if 'potential_std_prob' in privacy_metrics:
                                st.metric("√âcart-type", f"{privacy_metrics['potential_std_prob']:.3f}")

                    st.markdown("---")
                    st.markdown("### üõ°Ô∏è R√©sistance au Seuillage")

                    if 'threshold_resistance' in privacy_metrics:
                        resistance = privacy_metrics['threshold_resistance']
                        reconstruction = privacy_metrics.get('reconstruction_rate', 0)

                        col1, col2 = st.columns(2)

                        with col1:
                            st.metric("Taux de r√©sistance", f"{resistance*100:.1f}%",
                                     help="% d'ar√™tes originales non r√©cup√©rables par seuillage √† 0.5")

                        with col2:
                            st.metric("Taux de reconstruction", f"{reconstruction*100:.1f}%",
                                     help="% d'ar√™tes originales r√©cup√©rables par seuillage √† 0.5")

                        st.progress(resistance)

                        if resistance > 0.2:
                            st.success(f"‚úÖ Bonne r√©sistance au seuillage ({resistance*100:.1f}%)")
                        elif resistance > 0.1:
                            st.warning(f"‚ö†Ô∏è R√©sistance mod√©r√©e ({resistance*100:.1f}%)")
                        else:
                            st.error(f"‚ùå Faible r√©sistance - vuln√©rable au seuillage ({resistance*100:.1f}%)")

                        st.caption("üí° Un attaquant qui applique un seuil √† 0.5 ne r√©cup√®re que "
                                  f"{reconstruction*100:.1f}% des ar√™tes originales (contre 100% pour (k,Œµ)-obf)")

                elif 'min_cluster_size' in privacy_metrics:
                    # Generalization
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("Taille min. cluster", int(privacy_metrics['min_cluster_size']))

                    with col2:
                        st.metric("Taille moy. cluster", f"{privacy_metrics['avg_cluster_size']:.1f}")

                    with col3:
                        max_priv = privacy_metrics['max_privacy']
                        st.metric("Prob. max r√©-identification", f"{max_priv:.3f}")

                st.markdown("---")

                # Garanties globales
                guarantees = calculate_privacy_guarantees(G_orig, G_anon, st.session_state.method_key, method_params)

                if guarantees:
                    st.markdown("### üìã Garanties D√©taill√©es")

                    with st.expander("Voir toutes les garanties"):
                        for key, value in guarantees.items():
                            st.text(f"{key}: {value}")

            else:
                st.info("Aucune m√©trique de privacy sp√©cifique pour cette m√©thode")

        with tab5:
            st.markdown("## üéØ Simulations d'Attaques R√©elles")

            st.markdown("""
            Cette section simule des attaques de **r√©-identification** sur le graphe anonymis√©.
            Ces simulations montrent concr√®tement si un adversaire peut retrouver des n≈ìuds sp√©cifiques.
            """)

            st.markdown("---")

            # S√©lection du n≈ìud cible
            st.markdown("### üéØ Configuration de l'Attaque")

            col1, col2 = st.columns(2)

            with col1:
                target_node = st.number_input(
                    "N≈ìud cible √† retrouver",
                    min_value=0,
                    max_value=G_orig.number_of_nodes()-1,
                    value=0,
                    help="Le n≈ìud que l'adversaire essaie de r√©-identifier"
                )

            with col2:
                attack_type = st.selectbox(
                    "Type d'attaque",
                    ["Degree Attack", "Subgraph Attack (Triangles)"]
                )

            st.markdown("---")

            if st.button("üöÄ Lancer l'Attaque"):
                st.markdown("### üìä R√©sultats de l'Attaque")

                with st.spinner("Simulation en cours..."):
                    if attack_type == "Degree Attack":
                        results = simulate_degree_attack(G_orig, G_anon, target_node)
                    else:
                        results = simulate_subgraph_attack(G_orig, G_anon, target_node)

                if results['success']:
                    st.error("### ‚ö†Ô∏è Attaque R√©ussie !")
                    st.markdown(results['explanation'])

                    st.markdown(f"**N≈ìud r√©-identifi√©** : {results.get('re_identified_node', 'N/A')}")

                else:
                    st.success("### ‚úÖ Attaque √âchou√©e / Partiellement R√©ussie")
                    st.markdown(results['explanation'])

                st.markdown("---")
                st.markdown("### üìà D√©tails Techniques")

                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("**N≈ìud cible** :")
                    st.info(f"N≈ìud {target_node}")

                    if 'target_degree' in results:
                        st.markdown("**Degr√© du n≈ìud** :")
                        st.info(f"Degr√© = {results['target_degree']}")

                    if 'target_triangles' in results:
                        st.markdown("**Triangles** :")
                        st.info(f"{results['target_triangles']} triangles")

                with col2:
                    st.markdown("**Candidats trouv√©s** :")
                    if results['candidates']:
                        st.info(f"{len(results['candidates'])} n≈ìuds : {results['candidates'][:10]}")
                    else:
                        st.info("Aucun candidat")

                    if len(results['candidates']) > 1:
                        prob_success = 1 / len(results['candidates'])
                        st.markdown("**Probabilit√© de succ√®s** :")
                        st.warning(f"{prob_success*100:.1f}%")

            st.markdown("---")

            # Section √©ducative
            with st.expander("üìö En savoir plus sur ces attaques"):
                st.markdown("""
                ### Degree Attack (Attaque par Degr√©)

                L'adversaire conna√Æt le degr√© (nombre de connexions) du n≈ìud cible et cherche
                dans le graphe anonymis√© tous les n≈ìuds ayant ce degr√©.

                **Protection** :
                - k-degree anonymity garantit au moins k n≈ìuds par degr√©
                - Randomisation modifie les degr√©s
                - Differential Privacy ajoute du bruit

                ### Subgraph Attack (Attaque par Sous-graphe)

                L'adversaire conna√Æt la structure locale autour du n≈ìud (ex: triangles, motifs).
                Cette attaque est plus puissante car elle exploite plus d'information.

                **Protection** :
                - Generalization d√©truit les motifs locaux
                - Differential Privacy ajoute/supprime des triangles fictifs
                - Randomisation casse certains motifs
                """)

        with tab6:
            st.markdown(f"## üõ°Ô∏è Attaques et Garanties : {current_method['name']}")

            method_details = ATTACKS_AND_GUARANTEES.get(st.session_state.method_key, {})

            if method_details:
                # Attaques prot√©g√©es
                st.markdown("### ‚úÖ Attaques contre lesquelles la m√©thode prot√®ge")
                attacks_protected = method_details.get('attacks_protected', [])
                for attack in attacks_protected:
                    with st.expander(f"üõ°Ô∏è {attack['name']}", expanded=False):
                        st.markdown(attack['description'])

                # Attaques vuln√©rables
                st.markdown("---")
                st.markdown("### ‚ö†Ô∏è Vuln√©rabilit√©s et Limitations")
                attacks_vulnerable = method_details.get('attacks_vulnerable', [])
                for attack in attacks_vulnerable:
                    with st.expander(f"üö® {attack['name']}", expanded=False):
                        st.markdown(attack['description'])

                # Avantages
                st.markdown("---")
                st.markdown("### ‚úÖ Avantages de la M√©thode")
                advantages = method_details.get('advantages', [])
                for adv in advantages:
                    st.markdown(adv)

                # Inconv√©nients
                st.markdown("---")
                st.markdown("### ‚ùå Inconv√©nients et Limitations")
                disadvantages = method_details.get('disadvantages', [])
                for dis in disadvantages:
                    st.markdown(dis)

                # Exemple Karate
                st.markdown("---")
                st.markdown("### ü•ã Exemple Concret : Graphe Karate Club")
                karate_example = method_details.get('karate_example', '')
                if karate_example:
                    st.markdown(karate_example)
                else:
                    st.info("Exemple √† venir pour cette m√©thode.")
            else:
                st.warning("Informations d√©taill√©es non disponibles pour cette m√©thode.")

        with tab7:
            st.markdown("## üìö Dictionnaire des Attaques de R√©-Identification")

            st.markdown("""
            Ce dictionnaire pr√©sente **toutes les attaques connues** contre les graphes anonymis√©s,
            avec des exemples concrets et des explications d√©taill√©es.
            """)

            st.markdown("---")

            # Liste des attaques
            attack_names = [ATTACKS_DICTIONARY[k]['name'] for k in ATTACKS_DICTIONARY.keys()]

            selected_attack_name = st.selectbox(
                "Choisir une attaque √† explorer",
                attack_names
            )

            # Trouver l'attaque correspondante
            selected_attack_key = list(ATTACKS_DICTIONARY.keys())[attack_names.index(selected_attack_name)]
            attack = ATTACKS_DICTIONARY[selected_attack_key]

            st.markdown(f"### {attack['name']}")

            col1, col2 = st.columns([2, 1])

            with col1:
                with st.expander("üìù Description de l'Attaque", expanded=True):
                    st.markdown(attack['description'])

                with st.expander("üí° Exemple Concret"):
                    st.markdown(attack['example'])

            with col2:
                st.markdown("**‚ö†Ô∏è S√©v√©rit√©**")
                severity = attack['severity']
                if "Tr√®s √©lev√©e" in severity or "√âlev√©e" in severity:
                    st.error(severity)
                elif "Moyenne" in severity:
                    st.warning(severity)
                else:
                    st.info(severity)

                st.markdown("**üõ°Ô∏è Protection**")
                st.success(attack['protection'])

            st.markdown("---")

            # Exemples concrets sur Karate Club
            st.markdown("### ü•ã Exemples Concrets sur Karate Club")

            example_keys = list(CONCRETE_ATTACK_EXAMPLES.keys())

            for example_key in example_keys:
                example = CONCRETE_ATTACK_EXAMPLES[example_key]

                with st.expander(f"üìñ {example['title']}"):
                    st.markdown(f"**Sc√©nario** : {example['scenario']}")

                    st.markdown("**√âtapes de l'attaque** :")
                    for step in example['steps']:
                        st.markdown(f"- {step}")

                    st.markdown("---")
                    st.markdown("**Taux de Succ√®s** :")

                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("Sans protection", example.get('success_rate_no_protection', 'N/A'))

                    with col2:
                        if 'success_rate_k_anonymity' in example:
                            st.metric("Avec k-anonymity", example['success_rate_k_anonymity'])
                        elif 'success_rate_randomization' in example:
                            st.metric("Avec randomization", example['success_rate_randomization'])

                    with col3:
                        if 'success_rate_differential_privacy' in example:
                            st.metric("Avec Diff. Privacy", example['success_rate_differential_privacy'])
                        elif 'success_rate_generalization' in example:
                            st.metric("Avec Generalization", example['success_rate_generalization'])

                    if 'code_simulation' in example:
                        with st.expander("üíª Code de Simulation"):
                            st.code(example['code_simulation'], language='python')

        with tab8:
            st.markdown("## üîç Dictionnaire des Propri√©t√©s de Graphes")

            st.markdown("""
            Ce dictionnaire explique **toutes les propri√©t√©s de graphes** utilis√©es en anonymisation,
            leur importance pour l'utilit√©, et leur risque pour la privacy.
            """)

            st.markdown("---")

            # Liste des propri√©t√©s
            property_names = [GRAPH_PROPERTIES[k]['name'] for k in GRAPH_PROPERTIES.keys()]

            selected_property_name = st.selectbox(
                "Choisir une propri√©t√© √† explorer",
                property_names
            )

            # Trouver la propri√©t√© correspondante
            selected_property_key = list(GRAPH_PROPERTIES.keys())[property_names.index(selected_property_name)]
            prop = GRAPH_PROPERTIES[selected_property_key]

            st.markdown(f"### {prop['name']}")

            col1, col2 = st.columns(2)

            with col1:
                with st.expander("üìù D√©finition", expanded=True):
                    st.markdown(prop['definition'])

                with st.expander("üî¢ Formule"):
                    st.latex(prop['formula'])

                with st.expander("üí° Exemple"):
                    st.info(prop['example'])

            with col2:
                st.markdown("**üìä Importance pour l'Utilit√©**")
                importance = prop['utility_importance']
                if "Critique" in importance or "√âlev√©e" in importance:
                    st.success(importance)
                else:
                    st.info(importance)

                st.markdown("**‚ö†Ô∏è Risque pour la Privacy**")
                risk = prop['privacy_risk']
                if "√âlev√©" in risk:
                    st.error(risk)
                elif "Moyen" in risk:
                    st.warning(risk)
                else:
                    st.success(risk)

            st.markdown("---")

            # Calcul des propri√©t√©s sur le graphe actuel
            if isinstance(G_anon, nx.Graph):
                st.markdown("### üìä Valeurs pour le Graphe Actuel")

                try:
                    if selected_property_key == 'degree':
                        degrees = dict(G_anon.degree())
                        st.metric("Degr√© moyen", f"{np.mean(list(degrees.values())):.2f}")
                        st.metric("Degr√© max", max(degrees.values()))

                    elif selected_property_key == 'clustering_coefficient':
                        clustering = nx.average_clustering(G_anon)
                        st.metric("Coefficient de clustering moyen", f"{clustering:.3f}")

                    elif selected_property_key == 'density':
                        density = nx.density(G_anon)
                        st.metric("Densit√©", f"{density:.3f}")

                    elif selected_property_key == 'diameter':
                        if nx.is_connected(G_anon):
                            diameter = nx.diameter(G_anon)
                            st.metric("Diam√®tre", diameter)
                        else:
                            st.info("Graphe non connexe, diam√®tre non d√©fini")

                    elif selected_property_key == 'average_path_length':
                        if nx.is_connected(G_anon):
                            apl = nx.average_shortest_path_length(G_anon)
                            st.metric("Longueur moyenne des chemins", f"{apl:.2f}")
                        else:
                            st.info("Graphe non connexe, calcul√© sur la plus grande composante")

                except Exception as e:
                    st.warning(f"Calcul non disponible pour ce graphe")

    else:
        st.info("üëà S√©lectionnez une m√©thode et cliquez sur 'Anonymiser le Graphe' pour commencer")

        # Afficher un aper√ßu du graphe original
        st.markdown("### üìä Aper√ßu du Graphe Original")

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("N≈ìuds", G.number_of_nodes())
        with col2:
            st.metric("Ar√™tes", G.number_of_edges())
        with col3:
            st.metric("Degr√© Moyen", f"{sum(d for n, d in G.degree()) / G.number_of_nodes():.2f}")

        fig, ax = plt.subplots(figsize=(10, 8))
        pos = nx.spring_layout(G, seed=42, k=0.5, iterations=50)
        nx.draw_networkx_nodes(G, pos, ax=ax, node_color='lightblue', node_size=500, alpha=0.9)
        nx.draw_networkx_edges(G, pos, ax=ax, edge_color='gray', width=1.5, alpha=0.6)
        nx.draw_networkx_labels(G, pos, ax=ax, font_size=8, font_weight='bold')
        ax.set_title('Graphe Original', fontsize=16, fontweight='bold')
        ax.axis('off')
        st.pyplot(fig)


if __name__ == "__main__":
    main()
