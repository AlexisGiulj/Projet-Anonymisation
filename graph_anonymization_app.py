"""
Application Interactive d'Anonymisation de Graphes Sociaux
BasÃ©e sur la thÃ¨se "Anonymisation de Graphes Sociaux" par NGUYEN Huu-Hiep

Application Streamlit avec sÃ©lection de mÃ©thodes et explications dÃ©taillÃ©es
"""

import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
import random
from copy import deepcopy
import io
import pandas as pd
from scipy.optimize import minimize
from scipy.sparse import csr_matrix
from method_details import ATTACKS_AND_GUARANTEES
from definitions_and_attacks import (
    ANONYMIZATION_DEFINITIONS,
    ATTACKS_DICTIONARY,
    GRAPH_PROPERTIES,
    CONCRETE_ATTACK_EXAMPLES
)

# Configuration de la page
st.set_page_config(
    page_title="Anonymisation de Graphes Sociaux",
    page_icon="ğŸ”’",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Style CSS personnalisÃ©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 20px;
    }
    .method-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .metric-box {
        background-color: #e8f4f8;
        padding: 15px;
        border-radius: 5px;
        border-left: 5px solid #1f77b4;
    }
    .math-formula {
        background-color: #fff3cd;
        padding: 10px;
        border-radius: 5px;
        font-family: 'Courier New', monospace;
    }
</style>
""", unsafe_allow_html=True)


class GraphAnonymizer:
    """Classe pour anonymiser des graphes sociaux - VERSION Ã‰QUILIBRÃ‰E"""

    def __init__(self, graph):
        self.original_graph = graph.copy()
        self.n = graph.number_of_nodes()
        self.m = graph.number_of_edges()

    def random_add_del(self, k=20):
        """Random Add/Del optimisÃ© pour effet visible"""
        G = self.original_graph.copy()
        added = 0
        attempts = 0
        max_attempts = k * 100

        while added < k and attempts < max_attempts:
            u, v = random.sample(list(G.nodes()), 2)
            if not G.has_edge(u, v):
                G.add_edge(u, v)
                added += 1
            attempts += 1

        edges = list(self.original_graph.edges())
        if len(edges) >= k:
            edges_to_remove = random.sample(edges, min(k, len(edges)))
            for u, v in edges_to_remove:
                if G.has_edge(u, v):
                    G.remove_edge(u, v)

        return G

    def random_switch(self, k=25):
        """Random Switch optimisÃ©"""
        G = self.original_graph.copy()
        successful_switches = 0

        for _ in range(k * 3):  # Plus de tentatives
            if successful_switches >= k:
                break

            edges = list(G.edges())
            if len(edges) < 2:
                break

            (u, w), (v, x) = random.sample(edges, 2)

            if u != v and u != x and w != v and w != x:
                if not G.has_edge(u, v) and not G.has_edge(w, x):
                    G.remove_edge(u, w)
                    G.remove_edge(v, x)
                    G.add_edge(u, v)
                    G.add_edge(w, x)
                    successful_switches += 1

        return G

    def k_degree_anonymity(self, k=2):
        """
        k-degree anonymity : Garantit qu'au moins k nÅ“uds ont le mÃªme degrÃ©.

        ALGORITHME EN LANGAGE NATUREL :
        1. Compter combien de nÅ“uds ont chaque degrÃ©
        2. Pour chaque degrÃ© avec MOINS de k nÅ“uds :
           - FUSIONNER ce groupe avec un groupe voisin (degrÃ© proche)
           - Ajuster les degrÃ©s en ajoutant/supprimant des arÃªtes
        3. RÃ©pÃ©ter jusqu'Ã  ce que TOUS les groupes aient au moins k nÅ“uds

        STRATÃ‰GIE :
        - Groupes trop petits â†’ Fusionner avec degrÃ© voisin
        - Ajouter des arÃªtes pour augmenter les degrÃ©s vers le degrÃ© cible
        - Garantir : chaque degrÃ© apparaÃ®t au moins k fois
        """
        G = self.original_graph.copy()

        # ItÃ©rer jusqu'Ã  satisfaire la contrainte
        max_iterations = 100
        for iteration in range(max_iterations):
            # Calculer la distribution actuelle des degrÃ©s
            degrees = dict(G.degree())
            degree_counts = Counter(degrees.values())

            # Trouver les degrÃ©s qui violent la contrainte k
            violating_degrees = [d for d, count in degree_counts.items() if count < k]

            if not violating_degrees:
                # Contrainte satisfaite !
                break

            # StratÃ©gie : Fusionner les petits groupes avec leurs voisins
            for degree in sorted(violating_degrees):
                nodes_with_degree = [n for n, d in degrees.items() if d == degree]

                if len(nodes_with_degree) == 0:
                    continue

                # Trouver le degrÃ© cible (degrÃ© voisin le plus frÃ©quent)
                all_degrees = sorted(degree_counts.keys())

                # Chercher le degrÃ© voisin (supÃ©rieur ou infÃ©rieur)
                target_degree = None
                if degree < max(all_degrees):
                    # Augmenter vers le degrÃ© supÃ©rieur
                    target_degree = min([d for d in all_degrees if d > degree])
                elif degree > min(all_degrees):
                    # Diminuer vers le degrÃ© infÃ©rieur (en supprimant des arÃªtes)
                    target_degree = max([d for d in all_degrees if d < degree])
                else:
                    # Dernier recours : dupliquer le degrÃ© en ajoutant des arÃªtes
                    target_degree = degree + 1

                if target_degree is None:
                    continue

                # Ajuster les degrÃ©s des nÅ“uds pour atteindre target_degree
                for node in nodes_with_degree[:]:  # Copie pour Ã©viter modification pendant itÃ©ration
                    current_degree = G.degree(node)

                    if current_degree < target_degree:
                        # AUGMENTER le degrÃ© en ajoutant des arÃªtes
                        edges_to_add = target_degree - current_degree

                        for _ in range(edges_to_add):
                            # Trouver un nÅ“ud non connectÃ©
                            candidates = [n for n in G.nodes()
                                        if n != node and not G.has_edge(node, n)]

                            if candidates:
                                # PrÃ©fÃ©rer les nÅ“uds qui ont aussi besoin d'augmenter leur degrÃ©
                                target_node = random.choice(candidates)
                                G.add_edge(node, target_node)
                            else:
                                break  # Pas de candidat disponible

                    elif current_degree > target_degree:
                        # DIMINUER le degrÃ© en supprimant des arÃªtes
                        edges_to_remove = current_degree - target_degree

                        neighbors = list(G.neighbors(node))
                        edges_to_delete = random.sample(neighbors, min(edges_to_remove, len(neighbors)))

                        for neighbor in edges_to_delete:
                            G.remove_edge(node, neighbor)

                # Recalculer pour la prochaine itÃ©ration
                degrees = dict(G.degree())
                degree_counts = Counter(degrees.values())

        return G

    def generalization(self, k=4):
        """
        GÃ©nÃ©ralisation par clustering avec taille minimale k.

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        PRINCIPE DE LA GÃ‰NÃ‰RALISATION (k-anonymity structurelle) :
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        - Regrouper les nÅ“uds en CLUSTERS (super-nÅ“uds) de taille â‰¥ k
        - Chaque cluster reprÃ©sente au moins k nÅ“uds indistinguables
        - Les arÃªtes deviennent des super-arÃªtes entre clusters

        GARANTIE DE PRIVACY :
        - Un attaquant ne peut identifier un nÅ“ud spÃ©cifique dans un cluster
        - ProbabilitÃ© de rÃ©-identification â‰¤ 1/k pour chaque nÅ“ud

        PARAMÃˆTRE k :
        - Plus k est GRAND â†’ Clusters plus gros â†’ PLUS de privacy â†’ MOINS d'utilitÃ©
        - Plus k est PETIT â†’ Clusters plus petits â†’ MOINS de privacy â†’ PLUS d'utilitÃ©
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        G = self.original_graph.copy()
        n = G.number_of_nodes()

        # Nombre de clusters basÃ© sur k
        # On veut environ n/k clusters de taille k
        num_clusters = max(2, n // k)  # Au moins 2 clusters

        # Utiliser un algorithme de clustering spectral pour crÃ©er num_clusters
        try:
            # Essayer d'abord avec la mÃ©thode des communautÃ©s
            from networkx.algorithms import community

            # Utiliser label propagation qui converge rapidement
            communities_generator = community.label_propagation_communities(G)
            communities = list(communities_generator)

            # Si on a trop de clusters, fusionner les plus petits
            while len(communities) > num_clusters:
                # Trouver les 2 plus petits clusters
                communities = sorted(communities, key=len)
                smallest = communities[0]
                second_smallest = communities[1]
                # Fusionner
                merged = smallest.union(second_smallest)
                communities = [merged] + communities[2:]

            # Si on a trop peu de clusters, diviser les plus gros
            while len(communities) < num_clusters and any(len(c) >= 2*k for c in communities):
                # Trouver le plus gros cluster
                communities = sorted(communities, key=len, reverse=True)
                largest = communities[0]
                if len(largest) >= 2*k:
                    # Diviser en deux
                    largest_list = list(largest)
                    mid = len(largest_list) // 2
                    part1 = set(largest_list[:mid])
                    part2 = set(largest_list[mid:])
                    communities = [part1, part2] + communities[1:]
                else:
                    break

        except Exception as e:
            # Fallback : clustering simple par degrÃ©
            # Regrouper les nÅ“uds par degrÃ© similaire
            nodes_by_degree = {}
            for node in G.nodes():
                degree = G.degree(node)
                if degree not in nodes_by_degree:
                    nodes_by_degree[degree] = []
                nodes_by_degree[degree].append(node)

            # CrÃ©er des clusters de taille k
            communities = []
            current_cluster = set()
            for degree in sorted(nodes_by_degree.keys()):
                for node in nodes_by_degree[degree]:
                    current_cluster.add(node)
                    if len(current_cluster) >= k:
                        communities.append(current_cluster)
                        current_cluster = set()

            # Ajouter le dernier cluster s'il existe
            if current_cluster:
                if communities:
                    # Fusionner avec le dernier cluster
                    communities[-1] = communities[-1].union(current_cluster)
                else:
                    communities.append(current_cluster)

        # Assurer que tous les clusters ont au moins k nÅ“uds
        final_communities = []
        buffer = set()

        for community in sorted(communities, key=len, reverse=True):
            community = set(community).union(buffer)
            buffer = set()

            if len(community) >= k:
                final_communities.append(community)
            else:
                # Trop petit, mettre en buffer pour fusionner avec le prochain
                buffer = community

        # Si reste un buffer, fusionner avec le dernier cluster
        if buffer and final_communities:
            final_communities[-1] = final_communities[-1].union(buffer)
        elif buffer:
            final_communities.append(buffer)

        communities = final_communities

        # CrÃ©er le super-graphe
        super_graph = nx.Graph()
        node_to_cluster = {}
        cluster_to_nodes = {}
        cluster_id = 0

        for community in communities:
            community = set(community)

            for node in community:
                node_to_cluster[node] = cluster_id

            super_graph.add_node(cluster_id, cluster_size=len(community), size=len(community),
                               nodes=list(community), internal_edges=0)
            cluster_to_nodes[cluster_id] = list(community)
            cluster_id += 1

        # Compter les arÃªtes intra et inter-clusters
        intra_edges = 0
        inter_edges = 0

        # Ajouter les super-arÃªtes
        for u, v in G.edges():
            cluster_u = node_to_cluster.get(u)
            cluster_v = node_to_cluster.get(v)

            if cluster_u is not None and cluster_v is not None:
                if cluster_u != cluster_v:
                    inter_edges += 1
                    if super_graph.has_edge(cluster_u, cluster_v):
                        super_graph[cluster_u][cluster_v]['weight'] += 1
                    else:
                        super_graph.add_edge(cluster_u, cluster_v, weight=1)
                else:
                    # Self-loops pour les arÃªtes internes
                    intra_edges += 1
                    # IncrÃ©menter le compteur d'arÃªtes internes du nÅ“ud
                    super_graph.nodes[cluster_u]['internal_edges'] += 1
                    if super_graph.has_edge(cluster_u, cluster_u):
                        super_graph[cluster_u][cluster_u]['weight'] += 1
                    else:
                        super_graph.add_edge(cluster_u, cluster_u, weight=1)

        # Stocker les statistiques
        super_graph.graph['intra_edges'] = intra_edges
        super_graph.graph['inter_edges'] = inter_edges
        super_graph.graph['cluster_to_nodes'] = cluster_to_nodes
        super_graph.graph['node_to_cluster'] = node_to_cluster

        return super_graph, node_to_cluster

    def probabilistic_obfuscation(self, k=5, epsilon=0.3):
        """
        (k,Îµ)-obfuscation basÃ© sur la thÃ¨se de Nguyen Huu-Hiep

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        FORMULE CONFORME Ã€ LA THÃˆSE :
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        - ArÃªtes existantes : p = 1 - Îµ/k
        - ArÃªtes potentielles : p = Îµ/(2k)

        VULNÃ‰RABILITÃ‰ : Quand Îµ est PETIT (ex: 0.3, privacy thÃ©orique forte),
        les probabilitÃ©s se CONCENTRENT :
        - p_existantes â‰ˆ 1.0 (ex: 0.94 pour k=5, Îµ=0.3)
        - p_potentielles â‰ˆ 0.0 (ex: 0.03 pour k=5, Îµ=0.3)
        â†’ Un attaquant applique un seuil Ã  0.5 et rÃ©cupÃ¨re 100% du graphe original!

        UTILITÃ‰ PÃ‰DAGOGIQUE : Cette implÃ©mentation correcte montre que suivre
        la formule mathÃ©matique de la thÃ¨se ne garantit PAS la sÃ©curitÃ© pratique.
        C'est pourquoi MaxVar a Ã©tÃ© dÃ©veloppÃ©.
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        G = self.original_graph.copy()
        prob_graph = nx.Graph()
        prob_graph.add_nodes_from(G.nodes())

        # FORMULE CONFORME Ã€ LA THÃˆSE
        prob_existing = 1.0 - (epsilon / k)
        prob_potential = epsilon / (2 * k)

        # ArÃªtes EXISTANTES : probabilitÃ© Ã©levÃ©e (proche de 1.0 quand Îµ petit)
        for u, v in G.edges():
            prob_graph.add_edge(u, v, probability=prob_existing, is_original=True)

        # Ajouter des arÃªtes POTENTIELLES
        # Pour chaque nÅ“ud, ajouter k arÃªtes potentielles alÃ©atoires
        non_edges = [(u, v) for u in G.nodes() for v in G.nodes()
                     if u < v and not G.has_edge(u, v)]

        # Nombre d'arÃªtes potentielles : environ k arÃªtes par nÅ“ud
        # (comme spÃ©cifiÃ© dans la dÃ©finition de N_k(v))
        num_to_add = min(len(non_edges), k * G.number_of_nodes() // 2)
        edges_to_add = random.sample(non_edges, num_to_add)

        # ArÃªtes POTENTIELLES : probabilitÃ© faible (proche de 0.0 quand Îµ petit)
        for u, v in edges_to_add:
            prob_graph.add_edge(u, v, probability=prob_potential, is_original=False)

        return prob_graph

    def differential_privacy_edgeflip(self, epsilon=0.8):
        """
        EdgeFlip avec epsilon-differential privacy.

        FORMULE CORRECTE : s = 2 / (e^epsilon + 1)

        Trade-off :
        - epsilon PETIT => s GRAND => beaucoup de flips => FORTE privacy
        - epsilon GRAND => s PETIT => peu de flips => FAIBLE privacy
        """
        G = nx.Graph()
        G.add_nodes_from(self.original_graph.nodes())

        # FORMULE CORRECTE (dÃ©rivÃ©e du ratio de DP)
        s = 2 / (np.exp(epsilon) + 1)

        for u in self.original_graph.nodes():
            for v in self.original_graph.nodes():
                if u < v:
                    exists = self.original_graph.has_edge(u, v)

                    if random.random() < s/2:
                        if not exists:
                            G.add_edge(u, v)
                    else:
                        if exists:
                            G.add_edge(u, v)

        return G

    def differential_privacy_laplace(self, epsilon=1.2):
        """MÃ©canisme de Laplace optimisÃ©"""
        G = self.original_graph.copy()
        sensitivity = 1
        scale = sensitivity / epsilon

        new_graph = nx.Graph()
        new_graph.add_nodes_from(G.nodes())

        for u in G.nodes():
            for v in G.nodes():
                if u < v:
                    true_value = 1 if G.has_edge(u, v) else 0
                    noise = np.random.laplace(0, scale)
                    noisy_value = true_value + noise

                    if noisy_value > 0.5:
                        new_graph.add_edge(u, v)

        return new_graph

    def maxvar_obfuscation(self, num_potential_edges=50):
        """
        MaxVar: Variance Maximizing Scheme

        AmÃ©lioration de (k,Îµ)-obfuscation qui rÃ©sout le problÃ¨me de reconstruction
        par seuillage en maximisant la variance totale des degrÃ©s.

        ALGORITHME (Nguyen Huu-Hiep, 2016):
        1. Ajouter des arÃªtes potentielles "nearby" (distance 2, friend-of-friend)
        2. RÃ©soudre un programme quadratique pour assigner les probabilitÃ©s:
           - Minimiser: Î£ p_iÂ² (Ã©quivalent Ã  maximiser la variance)
           - Contrainte: Î£ p_uv = degree(u) pour chaque nÅ“ud u
        3. Les probabilitÃ©s rÃ©sultantes sont DISPERSÃ‰ES (pas concentrÃ©es Ã  0/1)

        AVANTAGE vs (k,Îµ)-obf:
        - Pas de reconstruction par seuillage!
        - ProbabilitÃ©s varient significativement autour de 0.5
        - PrÃ©servation exacte des degrÃ©s attendus
        """
        G0 = self.original_graph.copy()
        n = G0.number_of_nodes()

        # Phase 1: Ajouter des arÃªtes potentielles "nearby" (distance = 2)
        potential_edges = []
        for u in G0.nodes():
            # Trouver les voisins Ã  distance 2 (friend-of-friend)
            neighbors_dist_2 = set()
            for neighbor in G0.neighbors(u):
                for neighbor2 in G0.neighbors(neighbor):
                    if neighbor2 != u and not G0.has_edge(u, neighbor2):
                        neighbors_dist_2.add(neighbor2)

            # Ajouter des arÃªtes potentielles vers ces voisins
            for v in neighbors_dist_2:
                if u < v:  # Ã‰viter les doublons
                    potential_edges.append((u, v))

        # Limiter le nombre d'arÃªtes potentielles
        if len(potential_edges) > num_potential_edges:
            potential_edges = random.sample(potential_edges, num_potential_edges)

        # CrÃ©er le graphe Ã©tendu avec arÃªtes existantes + potentielles
        all_edges = list(G0.edges()) + potential_edges
        edge_to_idx = {edge: idx for idx, edge in enumerate(all_edges)}
        m = len(all_edges)

        # Phase 2: Formulation du programme quadratique
        # Objectif: Minimiser Î£ p_iÂ²
        # Contrainte: Î£ p_uv = degree(u) pour chaque nÅ“ud u

        # Construire la matrice de contraintes d'Ã©galitÃ© (A_eq)
        # Chaque ligne correspond Ã  un nÅ“ud, chaque colonne Ã  une arÃªte
        A_eq = np.zeros((n, m))
        b_eq = np.zeros(n)

        node_to_idx = {node: idx for idx, node in enumerate(G0.nodes())}

        for node in G0.nodes():
            node_idx = node_to_idx[node]
            b_eq[node_idx] = G0.degree(node)  # DegrÃ© attendu = degrÃ© original

            # Pour chaque arÃªte touchant ce nÅ“ud
            for u, v in all_edges:
                if u == node or v == node:
                    edge_idx = edge_to_idx[(u, v)]
                    A_eq[node_idx, edge_idx] = 1.0

        # Fonction objectif: f(p) = Î£ p_iÂ²
        def objective(p):
            return np.sum(p ** 2)

        # Gradient: f'(p) = 2p
        def gradient(p):
            return 2 * p

        # Contraintes d'Ã©galitÃ©
        constraints = {'type': 'eq', 'fun': lambda p: A_eq @ p - b_eq}

        # Bornes: 0 â‰¤ p_i â‰¤ 1
        bounds = [(0.0, 1.0) for _ in range(m)]

        # Point initial: probabilitÃ© uniforme qui satisfait les contraintes
        # ArÃªtes existantes: prob = 1.0, arÃªtes potentielles: prob faible
        p0 = np.zeros(m)
        for idx, (u, v) in enumerate(all_edges):
            if G0.has_edge(u, v):
                p0[idx] = 0.8  # ArÃªte existante: prob Ã©levÃ©e mais pas 1.0
            else:
                p0[idx] = 0.2  # ArÃªte potentielle: prob faible mais pas 0.0

        # RÃ©soudre le programme quadratique
        result = minimize(
            objective,
            p0,
            method='SLSQP',
            jac=gradient,
            bounds=bounds,
            constraints=constraints,
            options={'maxiter': 1000, 'ftol': 1e-6}
        )

        if not result.success:
            # Si l'optimisation Ã©choue, utiliser p0
            probabilities = p0
        else:
            probabilities = result.x

        # Phase 3: CrÃ©er le graphe incertain avec les probabilitÃ©s optimisÃ©es
        prob_graph = nx.Graph()
        prob_graph.add_nodes_from(G0.nodes())

        for idx, (u, v) in enumerate(all_edges):
            prob = probabilities[idx]
            is_original = G0.has_edge(u, v)

            # Ajouter toutes les arÃªtes avec leur probabilitÃ©
            prob_graph.add_edge(u, v, probability=prob, is_original=is_original)

        return prob_graph


# DÃ©finitions des mÃ©thodes avec explications
METHODS = {
    "Random Add/Del": {
        "name": "Randomisation - Random Add/Del",
        "category": "1. Anonymisation par Randomisation",
        "params": {"k": 20},
        "description_short": "Ajoute k fausses arÃªtes puis supprime k vraies arÃªtes alÃ©atoirement",
        "description": """
### Principe en Langage Naturel

La mÃ©thode **Random Add/Del** est l'une des plus simples. Elle fonctionne en deux Ã©tapes :
1. **Ajout** : On ajoute k arÃªtes alÃ©atoires entre des nÅ“uds non connectÃ©s
2. **Suppression** : On supprime k arÃªtes existantes choisies au hasard

Cette approche crÃ©e de l'incertitude en modifiant la structure du graphe de maniÃ¨re alÃ©atoire.
Un attaquant qui connaÃ®trait le degrÃ© d'un nÅ“ud ne pourra plus le retrouver avec certitude
car les degrÃ©s ont changÃ©.

### Formalisation MathÃ©matique

Soit G = (V, E) le graphe original.

**Algorithme** :
```
1. Pour i = 1 Ã  k :
   - Choisir (u, v) âˆˆ V Ã— V tel que (u,v) âˆ‰ E
   - E â† E âˆª {(u,v)}

2. Pour i = 1 Ã  k :
   - Choisir (u, v) âˆˆ E uniformÃ©ment
   - E â† E \\ {(u,v)}

3. Retourner G' = (V, E)
```

**PropriÃ©tÃ©s** :
- Nombre d'arÃªtes prÃ©servÃ© : |E'| = |E|
- Distribution des degrÃ©s modifiÃ©e
- Pas de garantie formelle de privacy

**ComplexitÃ©** : O(k)
        """,
        "formula": r"P(edge_{added}) = \frac{k}{|V|(|V|-1)/2 - |E|}, \quad P(edge_{removed}) = \frac{k}{|E|}",
        "privacy_level": "Faible (pas de garantie formelle)",
        "utility_preservation": "Moyenne Ã  Bonne"
    },

    "Random Switch": {
        "name": "Randomisation - Random Switch",
        "category": "1. Anonymisation par Randomisation",
        "params": {"k": 25},
        "description_short": "Ã‰change k paires d'arÃªtes en prÃ©servant les degrÃ©s",
        "description": """
### Principe en Langage Naturel

**Random Switch** amÃ©liore Random Add/Del en prÃ©servant une propriÃ©tÃ© importante : **les degrÃ©s des nÅ“uds**.

Au lieu d'ajouter/supprimer des arÃªtes indÃ©pendamment, on **Ã©change** des arÃªtes :
- On choisit deux arÃªtes (u,w) et (v,x)
- On les remplace par (u,v) et (w,x)
- Si ces nouvelles arÃªtes n'existent pas dÃ©jÃ 

Ainsi, chaque nÅ“ud conserve exactement le mÃªme nombre de connexions, mais ces connexions
pointent vers d'autres nÅ“uds. C'est comme si on "rÃ©arrangeait" les liens sociaux sans
changer le nombre d'amis de chacun.

### Formalisation MathÃ©matique

**Algorithme** :
```
Pour i = 1 Ã  k :
  1. Choisir (u,w), (v,x) âˆˆ E uniformÃ©ment
  2. Si u â‰  v â‰  w â‰  x et (u,v) âˆ‰ E et (w,x) âˆ‰ E :
     - E â† E \\ {(u,w), (v,x)}
     - E â† E âˆª {(u,v), (w,x)}

Retourner G' = (V, E)
```

**Invariants prÃ©servÃ©s** :
- SÃ©quence de degrÃ©s : deg_G'(v) = deg_G(v) âˆ€v âˆˆ V
- Nombre d'arÃªtes : |E'| = |E|

**PropriÃ©tÃ© clÃ©** : Les chemins et la structure globale sont modifiÃ©s tout en
prÃ©servant les propriÃ©tÃ©s locales (degrÃ©s).

**ComplexitÃ©** : O(k)
        """,
        "formula": r"deg_{G'}(v) = deg_G(v) \quad \forall v \in V",
        "privacy_level": "Faible Ã  Moyenne",
        "utility_preservation": "TrÃ¨s Bonne (degrÃ©s prÃ©servÃ©s)"
    },

    "k-degree anonymity": {
        "name": "K-Anonymisation - k-degree anonymity",
        "category": "2. K-Anonymisation",
        "params": {"k": 2},
        "description_short": "Garantit que chaque degrÃ© apparaÃ®t au moins k fois",
        "description": """
### Principe en Langage Naturel

La **k-degree anonymity** fournit une garantie formelle : chaque nÅ“ud doit Ãªtre
**indistinguable** d'au moins k-1 autres nÅ“uds en termes de degrÃ©.

**Intuition** : Si un attaquant connaÃ®t le degrÃ© d'un nÅ“ud cible (ex: 5 amis),
il doit y avoir au moins k nÅ“uds avec ce mÃªme degrÃ©. L'attaquant ne peut donc
identifier le nÅ“ud cible qu'avec une probabilitÃ© $\\leq 1/k$.

**Exemple** : Avec k=3, si Alice a 7 amis, on s'assure qu'au moins 2 autres
personnes ont aussi 7 amis. L'attaquant ne peut pas dire laquelle est Alice.

L'algorithme ajoute des arÃªtes de maniÃ¨re **dÃ©terministe** pour atteindre cette propriÃ©tÃ©.

### Formalisation MathÃ©matique

**DÃ©finition formelle** :

Un graphe G = (V, E) satisfait la k-degree anonymity si :

$$\\forall d \\in \\{\\deg(v) : v \\in V\\}, |\\{v \\in V : \\deg(v) = d\\}| \\geq k$$

C'est-Ã -dire : pour tout degrÃ© d qui apparaÃ®t dans le graphe,
il doit y avoir au moins k nÅ“uds avec ce degrÃ©.

**Algorithme** :
```
EntrÃ©e : G = (V, E), k
Sortie : G' = (V, E') satisfaisant k-degree anonymity

1. Calculer la sÃ©quence de degrÃ©s D = [deg(v) : v âˆˆ V]
2. Pour chaque degrÃ© d apparaissant moins de k fois :
   - Identifier les nÅ“uds V_d = {v : deg(v) = d}
   - Ajouter des arÃªtes pour augmenter/uniformiser les degrÃ©s
3. Retourner G'
```

### Heuristique ImplÃ©mentÃ©e

L'implÃ©mentation utilise une **stratÃ©gie greedy itÃ©rative** qui fusionne les groupes de degrÃ©s trop petits :

**Ã‰tape 1 - DÃ©tection des violations** :
```
RÃ‰PÃ‰TER jusqu'Ã  convergence (max 100 itÃ©rations):
  1. Calculer la distribution des degrÃ©s
  2. Identifier les degrÃ©s "violants" : count(d) < k
  3. SI aucun violant â†’ STOP (contrainte satisfaite)
  4. SINON â†’ Passer Ã  l'Ã©tape 2
```

**Ã‰tape 2 - Fusion vers degrÃ© cible** :

Pour chaque degrÃ© violant d avec count(d) < k :
```
SI d < max(all_degrees):
  â†’ AUGMENTER vers le degrÃ© supÃ©rieur le plus proche
  â†’ target_degree = min([degrÃ©s > d])

SINON SI d > min(all_degrees):
  â†’ DIMINUER vers le degrÃ© infÃ©rieur le plus proche
  â†’ target_degree = max([degrÃ©s < d])

SINON (dernier recours):
  â†’ CrÃ©er un nouveau groupe Ã  d+1
```

**Ã‰tape 3 - Ajustement des degrÃ©s** :
```
Pour AUGMENTER un degrÃ© (current < target):
  1. Chercher candidats = [nÅ“uds NON connectÃ©s]
  2. SÃ©lection alÃ©atoire parmi candidats
  3. Ajouter arÃªte (node, candidat)

Pour DIMINUER un degrÃ© (current > target):
  1. Lister les voisins du nÅ“ud
  2. Ã‰chantillonner alÃ©atoirement les arÃªtes Ã  supprimer
  3. Supprimer les arÃªtes sÃ©lectionnÃ©es
```

**Exemple d'exÃ©cution** (Karate Club, k=2) :
- Distribution originale : {1: 1, 2: 11, 3: 6, 9: 1, 10: 1, 12: 1, 16: 1, 17: 1}
  - DegrÃ©s violants : 1, 9, 10, 12, 16, 17 (< 2 occurrences)
- AprÃ¨s anonymisation : {1: 6, 2: 7, 3: 8, 4: 2, 5: 2, 8: 9}
  - âœ… Tous les degrÃ©s $\\geq$ 2 occurrences
  - Modification : -14.1% d'arÃªtes

**PropriÃ©tÃ©s** :
- âœ… **Garantit** la contrainte k-anonymity
- âœ… **Minimise** les modifications (fusion vers voisin proche)
- âš ï¸ **Non-optimal** (peut modifier plus que le minimum thÃ©orique)
- âš ï¸ **RandomisÃ©** (rÃ©sultats non dÃ©terministes)

**Garantie de privacy** :

$$P(\\text{identitÃ© de } v | \\deg(v) = d) \\leq \\frac{1}{k}$$

**NP-complÃ©tude** : Trouver le nombre minimum d'arÃªtes Ã  ajouter est NP-difficile.

**ComplexitÃ©** : O(nÂ²) en pratique (itÃ©rations Ã— ajustements)
        """,
        "formula": r"|\{v \in V : deg(v) = d\}| \geq k \quad \forall d",
        "privacy_level": "Moyenne Ã  Forte (garantie k-anonymity)",
        "utility_preservation": "Bonne"
    },

    "Generalization": {
        "name": "GÃ©nÃ©ralisation - Super-nodes",
        "category": "3. Anonymisation par GÃ©nÃ©ralisation",
        "params": {"k": 4},
        "description_short": "Regroupe les nÅ“uds en super-nÅ“uds de taille $\\geq k$",
        "description": """
### Principe en Langage Naturel

La **gÃ©nÃ©ralisation** adopte une approche radicalement diffÃ©rente : au lieu de modifier
les arÃªtes, on **regroupe** les nÅ“uds similaires en "super-nÅ“uds".

**Analogie** : C'est comme publier des statistiques par dÃ©partement plutÃ´t que par personne.
- Au lieu de "Alice (Paris) connectÃ©e Ã  Bob (Lyon)"
- On dit "RÃ©gion Ãle-de-France (10000 personnes) connectÃ©e Ã  RÃ©gion Auvergne-RhÃ´ne-Alpes (5000 personnes)"

**Avantages** :
- Protection maximale de l'identitÃ© individuelle
- RÃ©duction de la taille du graphe publiÃ©
- Chaque individu est "cachÃ©" dans un groupe de k personnes minimum

**InconvÃ©nient** : Perte importante d'information structurelle fine.

### Formalisation MathÃ©matique

**ModÃ¨le de graphe gÃ©nÃ©ralisÃ©** :

Soit G = (V, E) le graphe original. On crÃ©e une partition P = {Câ‚, Câ‚‚, ..., Câ‚˜}
de V telle que $|C_i| \\geq k \\; \\forall i$.

Le **super-graphe** G* = (V*, E*) est dÃ©fini par :
- V* = {Câ‚, Câ‚‚, ..., Câ‚˜} (les clusters)
- $E^* = \\{(C_i, C_j) : \\exists(u,v) \\in E \\text{ avec } u \\in C_i, v \\in C_j\\}$

Chaque super-arÃªte (Cáµ¢, Câ±¼) a un **poids** :

$$w(C_i, C_j) = |\\{(u,v) \\in E : u \\in C_i, v \\in C_j\\}|$$

**ProbabilitÃ© d'arÃªte dans le cluster** :

$$P(\\text{edge} | C_i, C_j) = \\frac{w(C_i, C_j)}{|C_i| \\times |C_j|}$$

**Garantie de privacy** : Un individu est cachÃ© parmi au moins k-1 autres
dans son cluster.

**ProblÃ¨me d'optimisation** : Trouver la partition P qui minimise la perte
d'information tout en respectant $|C_i| \\geq k$ est NP-difficile.

**ComplexitÃ©** : O(nÂ²) Ã  O(nÂ³) selon l'algorithme de clustering
        """,
        "formula": r"G^* = (V^*, E^*) \text{ oÃ¹ } V^* = \{C_i : |C_i| \geq k\}",
        "privacy_level": "Forte (k-anonymity structurelle)",
        "utility_preservation": "Faible Ã  Moyenne"
    },

    "Probabilistic": {
        "name": "Probabiliste - (k,Îµ)-obfuscation",
        "category": "4. Approches Probabilistes",
        "params": {"k": 5, "epsilon": 0.3},
        "description_short": "CrÃ©e un graphe incertain avec probabilitÃ©s sur les arÃªtes",
        "description": """
### Principe en Langage Naturel

Les approches **probabilistes** crÃ©ent un "graphe incertain" oÃ¹ chaque arÃªte existe
avec une certaine **probabilitÃ©**.

**IdÃ©e clÃ©** : Au lieu de publier un graphe dÃ©terministe (arÃªte = oui/non), on publie
des probabilitÃ©s. Par exemple :
- ArÃªte (Alice, Bob) : 95% de probabilitÃ© d'exister
- ArÃªte (Alice, Charlie) : 20% de probabilitÃ© d'exister

Un attaquant ne peut plus Ãªtre certain de rien : mÃªme les vraies arÃªtes ont une incertitude.

**ModÃ¨le (k,Îµ)-obfuscation** :
- **k** : niveau d'anonymisation souhaitÃ© (plus k est grand, plus de protection)
- **Îµ** : paramÃ¨tre de tolÃ©rance (plus Îµ est petit, plus de protection)

### Formalisation MathÃ©matique

**Graphe incertain** :

Un graphe incertain est un triplet GÌƒ = (V, E, p) oÃ¹ :
- V : ensemble de nÅ“uds
- E : ensemble d'arÃªtes (rÃ©elles + potentielles)
- p : E â†’ [0,1] fonction de probabilitÃ©

**DÃ©finition (k,Îµ)-obfuscation** :

Pour tout nÅ“ud v âˆˆ V, l'entropie de Shannon de la distribution
de probabilitÃ© sur les k voisins candidats doit Ãªtre $\\geq \\log(k) - \\varepsilon$ :

$$H(N_k(v)) = -\\sum_i p_i \\log(p_i) \\geq \\log(k) - \\varepsilon$$

oÃ¹ $N_k(v)$ sont les k nÅ“uds les plus susceptibles d'Ãªtre voisins de v.

**Assignation des probabilitÃ©s** :

Pour les arÃªtes existantes :

$$p((u,v)) = 1 - \\frac{\\varepsilon}{k}$$

Pour les arÃªtes potentielles (ajoutÃ©es pour l'obfuscation) :

$$p((u,v)) = \\frac{\\varepsilon}{2k}$$

**Graphe d'exemple (sample graph)** :

Ã€ partir de GÌƒ, on peut gÃ©nÃ©rer des graphes compatibles en Ã©chantillonnant :

$$G_{sample} = (V, E_{sample}) \\text{ oÃ¹ } e \\in E_{sample} \\text{ ssi } X_e \\leq p(e), X_e \\sim U[0,1]$$

**PropriÃ©tÃ©** : L'espÃ©rance des degrÃ©s est prÃ©servÃ©e.

**ComplexitÃ©** : O(|E| + kÂ·n)

### âš ï¸ **LIMITATION CRITIQUE : Reconstruction par Seuillage**

L'implÃ©mentation actuelle de (k,Îµ)-obfuscation a une **faille majeure** :

**ProblÃ¨me** :
- ArÃªtes existantes : probabilitÃ© $\\approx$ 1.0
- ArÃªtes potentielles : probabilitÃ© $\\approx$ 0.0
- **Un attaquant peut appliquer un seuil Ã  0.5 et rÃ©cupÃ©rer EXACTEMENT le graphe original!**

**Pourquoi ?** Comme mentionnÃ© dans la thÃ¨se (Section 3.3.3) :
> "With small values of Îµ, re highly concentrates around zero, so existing sampled
> edges have probabilities nearly 1 and non-existing sampled edges are assigned
> probabilities almost 0. **Simple rounding techniques can easily reveal the true graph.**"

**Solution** : Utiliser **MaxVar** (voir ci-dessous) qui maximise la variance des
probabilitÃ©s pour Ã©viter cette concentration autour de 0/1.

**UtilitÃ© pÃ©dagogique** : Cette mÃ©thode est conservÃ©e dans l'application pour
montrer l'importance de la **conception d'algorithmes** en privacy. Une formulation
mathÃ©matique correcte ne garantit pas une implÃ©mentation sÃ©curisÃ©e!
        """,
        "formula": r"H(N_k(v)) = -\sum_i p_i \log(p_i) \geq \log(k) - \varepsilon",
        "privacy_level": "âš ï¸ FAIBLE (vulnÃ©rable au seuillage) - Voir MaxVar",
        "utility_preservation": "Bonne (espÃ©rance prÃ©servÃ©e)"
    },

    "MaxVar": {
        "name": "Probabiliste - MaxVar (Variance Maximizing)",
        "category": "4. Approches Probabilistes",
        "params": {"num_potential_edges": 50},
        "description_short": "Graphe incertain avec probabilitÃ©s dispersÃ©es (rÃ©siste au seuillage)",
        "description": """
### Principe en Langage Naturel

**MaxVar** est une amÃ©lioration de (k,Îµ)-obfuscation qui rÃ©sout le **problÃ¨me de reconstruction par seuillage**.

**IdÃ©e clÃ©** : Au lieu de minimiser Îµ (ce qui concentre les probabilitÃ©s autour de 0/1),
on **maximise la variance totale des degrÃ©s** tout en prÃ©servant les degrÃ©s attendus.

**RÃ©sultat** : Les probabilitÃ©s sont **dispersÃ©es** autour de 0.5, rendant impossible
la reconstruction du graphe original par simple seuillage!

**Analogie** : Imaginons que vous voulez cacher quelle porte est la vraie parmi 10 portes :
- **(k,Îµ)-obf** : Porte vraie = 99% de chance, portes fausses = 1% â†’ **Trop Ã©vident!**
- **MaxVar** : Toutes les portes ont des probabilitÃ©s variÃ©es entre 30% et 70% â†’ **Confusion maximale!**

### Formalisation MathÃ©matique

**Programme Quadratique** :

L'algorithme rÃ©sout un programme d'optimisation quadratique :

$$\\min \\sum_{i \\in E} p_i^2$$

Contraintes:

$$0 \\leq p_i \\leq 1 \\quad \\forall i \\in E$$

$$\\sum_{v \\in N(u)} p_{uv} = \\deg(u) \\quad \\forall u \\in V$$

oÃ¹ $E$ contient Ã  la fois les arÃªtes existantes ET les arÃªtes potentielles.

**Pourquoi minimiser $\\sum p_i^2$?**

La variance de la distance d'Ã©dition (ThÃ©orÃ¨me 3.3, thÃ¨se) est :

$$\\text{Var}[D(\\tilde{G}, G)] = \\sum_i p_i(1 - p_i) = |E_{\\text{original}}| - \\sum_i p_i^2$$

Donc **minimiser $\\sum p_i^2$** Ã©quivaut Ã  **maximiser la variance**, ce qui maximise
l'incertitude sur le graphe!

**Algorithme (3 phases)** :

**Phase 1 - Proposition d'arÃªtes "nearby"** :
```
Pour chaque nÅ“ud u :
  1. Trouver les voisins Ã  distance 2 (friend-of-friend)
  2. Ajouter des arÃªtes potentielles vers ces voisins
  3. Limiter le nombre total d'arÃªtes potentielles
```

**Observation clÃ©** : Proposer des arÃªtes "nearby" (distance 2) minimise la distorsion
structurelle tout en maximisant la confusion. C'est plus plausible qu'ajouter des arÃªtes
alÃ©atoires entre nÅ“uds distants!

**Phase 2 - Optimisation quadratique** :
```
1. Construire la matrice A_eq : ligne u, colonne (u,v) â†’ 1
2. Vecteur b_eq : degree(u) pour chaque nÅ“ud u
3. RÃ©soudre: min Î£ pÂ² sous contrainte A_eq @ p = b_eq
4. Utiliser SLSQP (Sequential Least Squares Programming)
```

**Phase 3 - Publication** :
```
1. CrÃ©er le graphe incertain GÌƒ = (V, E, p)
2. Publier plusieurs graphes Ã©chantillons G_sample
3. Chaque arÃªte e âˆˆ E_sample si X_e â‰¤ p(e), X_e ~ U[0,1]
```

**PropriÃ©tÃ©s mathÃ©matiques** :

1. **Conservation des degrÃ©s attendus** :
   $$\\mathbb{E}[\\deg(u) \\text{ dans } \\tilde{G}] = \\deg(u) \\text{ dans } G_0 \\quad \\forall u$$

2. **Maximisation de la variance** :
   $$\\text{Var}[D(\\tilde{G}, G)] \\text{ est maximale sous contraintes}$$

3. **RÃ©sistance au seuillage** :
   Les probabilitÃ©s NE sont PAS concentrÃ©es Ã  0/1, donc $\\text{threshold}(\\tilde{G}, 0.5) \\neq G_0$

**Exemple numÃ©rique** (Karate Club) :

ArÃªte existante (0,1) :
- (k,Îµ)-obf : p = 0.95 (proche de 1.0) â†’ **facilement identifiable**
- MaxVar : p = 0.63 (dispersÃ©) â†’ **ambiguÃ«**

ArÃªte potentielle (5,12) :
- (k,Îµ)-obf : p = 0.05 (proche de 0.0) â†’ **facilement identifiable**
- MaxVar : p = 0.42 (dispersÃ©) â†’ **ambiguÃ«**

**ComplexitÃ©** :
- Phase 1 : $O(\\sum \\deg(u)^2) \\approx O(n \\cdot d_{avg}^2)$
- Phase 2 : $O(m^2)$ pour l'optimisation quadratique
- Phase 3 : $O(m)$ pour l'Ã©chantillonnage

Total : **O(mÂ²)** (peut Ãªtre rÃ©duit avec partitionnement du graphe)

### Comparaison (k,Îµ)-obf vs MaxVar

| CritÃ¨re | (k,Îµ)-obf | MaxVar |
|---------|-----------|--------|
| **ProbabilitÃ©s** | ConcentrÃ©es (0/1) | DispersÃ©es (0.3-0.7) |
| **RÃ©sistance seuillage** | âŒ VulnÃ©rable | âœ… RÃ©sistant |
| **PrÃ©servation degrÃ©s** | Approximative | âœ… Exacte |
| **ArÃªtes proposÃ©es** | AlÃ©atoires | âœ… Nearby (distance 2) |
| **Variance** | Minimale | âœ… Maximale |
| **ComplexitÃ©** | O(|E| + kn) | O(mÂ²) |

**Trade-off** : MaxVar est plus coÃ»teux en calcul mais offre de meilleures garanties
de privacy et d'utilitÃ©.
        """,
        "formula": r"\min \sum_{i} p_i^2 \text{ s.t. } \sum_{v \in N(u)} p_{uv} = \deg(u)",
        "privacy_level": "Forte (rÃ©siste au seuillage)",
        "utility_preservation": "Excellente (degrÃ©s exacts + arÃªtes nearby)"
    },

    "EdgeFlip": {
        "name": "Privacy DiffÃ©rentielle - EdgeFlip",
        "category": "5. Privacy DiffÃ©rentielle",
        "params": {"epsilon": 0.8},
        "description_short": "Applique le Randomized Response Technique avec Îµ-DP",
        "description": """
### Principe en Langage Naturel

**EdgeFlip** applique le cÃ©lÃ¨bre **Randomized Response Technique** (RRT) des statistiques
Ã  la publication de graphes.

**Intuition du RRT** (exemple classique) :
Pour une question sensible ("Avez-vous trichÃ© Ã  l'examen ?") :
- Lancez une piÃ¨ce en secret
- Si Face : rÃ©pondez la vÃ©ritÃ©
- Si Pile : rÃ©pondez au hasard (oui/non Ã  pile ou face)

RÃ©sultat : Votre rÃ©ponse a du "dÃ©ni plausible" mais les statistiques globales
restent calculables.

**Application Ã  EdgeFlip** :
Pour chaque paire de nÅ“uds (u,v) :
- Avec probabilitÃ© s/2 : **inverser** l'arÃªte (0â†’1 ou 1â†’0)
- Avec probabilitÃ© 1-s/2 : garder l'Ã©tat rÃ©el

oÃ¹ s est dÃ©terminÃ© par le paramÃ¨tre de privacy Îµ.

**Garantie Îµ-differential privacy** : La prÃ©sence/absence d'une arÃªte
est protÃ©gÃ©e avec garantie mathÃ©matique Îµ-DP.

### Formalisation MathÃ©matique

**DÃ©finition Îµ-Differential Privacy** :

Un algorithme $\\mathcal{A}$ satisfait Îµ-DP si pour tous graphes voisins $G, G'$
(diffÃ©rant par une arÃªte) et pour tout output $O$ :

$$P[\\mathcal{A}(G) = O] \\leq e^\\varepsilon \\cdot P[\\mathcal{A}(G') = O]$$

Plus $\\varepsilon$ est petit, plus forte est la garantie de privacy.

**Algorithme EdgeFlip (en langage naturel)** :

Pour chaque paire de nÅ“uds possible (u, v) dans le graphe :
1. **Lancer une piÃ¨ce biaisÃ©e** avec probabilitÃ© s/2
2. **Si pile** (probabilitÃ© s/2) : INVERSER l'Ã©tat de l'arÃªte
   - Si l'arÃªte existe â†’ la supprimer
   - Si l'arÃªte n'existe pas â†’ l'ajouter
3. **Si face** (probabilitÃ© 1-s/2) : GARDER l'Ã©tat de l'arÃªte
   - Si l'arÃªte existe â†’ la garder
   - Si l'arÃªte n'existe pas â†’ ne rien faire

Le paramÃ¨tre s dÃ©pend du budget privacy Îµ selon :

$$s = \\frac{2}{e^\\varepsilon + 1}$$

**Trade-off** :
- Îµ petit (0.1) â†’ s = 0.95 â†’ flip 47.5% des arÃªtes â†’ **forte privacy**
- Îµ grand (3.0) â†’ s = 0.09 â†’ flip 4.7% des arÃªtes â†’ **faible privacy**

**Algorithme EdgeFlip (pseudo-code formel)** :

```
EntrÃ©e : G = (V, E), Îµ
ParamÃ¨tre : s = 2 / (e^Îµ + 1)    â† FORMULE CORRECTE

Pour chaque paire (u, v) avec u < v :
  exists = (u,v) âˆˆ E

  Avec probabilitÃ© s/2 :
    output = NOT exists   // Inverser
  Sinon :
    output = exists       // Garder

  Si output = TRUE :
    Ajouter (u,v) Ã  E_output

Retourner G_output = (V, E_output)
```

**Preuve de Îµ-DP** :

Pour une arÃªte (u,v) :

P[output=1 | exists=1] = 1 - s/2
P[output=1 | exists=0] = s/2

Ratio : (1 - s/2) / (s/2) = (e^Îµ + 1 - 1) / 1 = e^Îµ âœ“

Donc EdgeFlip satisfait Îµ-edge-DP.

**EspÃ©rance du nombre d'arÃªtes** :

$$\\mathbb{E}[|E_{output}|] = |E| \\cdot (1 - s/2) + (n(n-1)/2 - |E|) \\cdot s/2$$
$$\\approx n(n-1)/4 \\text{  (pour } s \\approx 1\\text{, trÃ¨s bruitÃ©)}$$

**ComplexitÃ©** : O(nÂ²)

**InconvÃ©nient** : ComplexitÃ© quadratique limite le passage Ã  l'Ã©chelle.
        """,
        "formula": r"P[\mathcal{A}(G) = O] \leq e^\varepsilon \cdot P[\mathcal{A}(G') = O]",
        "privacy_level": "TrÃ¨s Forte (Îµ-differential privacy)",
        "utility_preservation": "Variable (dÃ©pend de Îµ)"
    },

    "Laplace": {
        "name": "Privacy DiffÃ©rentielle - MÃ©canisme de Laplace",
        "category": "5. Privacy DiffÃ©rentielle",
        "params": {"epsilon": 1.2},
        "description_short": "Ajoute du bruit Laplacien pour dÃ©cider de l'inclusion des arÃªtes",
        "description": """
### Principe en Langage Naturel

Le **MÃ©canisme de Laplace** est la technique fondamentale de la privacy diffÃ©rentielle.

**Principe gÃ©nÃ©ral** : Pour publier une statistique f(donnÃ©es) de maniÃ¨re privÃ©e,
on ajoute du **bruit alÃ©atoire** calibrÃ© Ã  la **sensibilitÃ©** de f.

**Pour les graphes** :
- On considÃ¨re chaque arÃªte potentielle (u,v)
- Valeur rÃ©elle : 1 si l'arÃªte existe, 0 sinon
- On ajoute du bruit Laplacien ~ Lap(Î”f/Îµ)
- On dÃ©cide d'inclure l'arÃªte si valeur_bruitÃ©e > seuil

**Intuition du bruit** : Le bruit "masque" la contribution d'une arÃªte individuelle,
rendant impossible de dÃ©terminer si une arÃªte spÃ©cifique Ã©tait prÃ©sente ou non.

### Formalisation MathÃ©matique

**MÃ©canisme de Laplace gÃ©nÃ©ral** :

Pour une fonction f : D â†’ â„^d, le mÃ©canisme de Laplace est :

M(D) = f(D) + (Yâ‚, ..., Y_d)

oÃ¹ Y_i ~ Lap(Î”f/Îµ) sont indÃ©pendants et Î”f est la sensibilitÃ© globale.

**SensibilitÃ© globale** :

$$\\Delta f = \\max_{G,G' \\text{ voisins}} ||f(G) - f(G')||_1$$

Pour les graphes (edge-DP), deux graphes sont voisins s'ils diffÃ¨rent par une arÃªte.
Donc : $\\Delta f = 1$ pour une requÃªte de type "cette arÃªte existe-t-elle ?"

**Distribution de Laplace** :

$\\text{Lap}(b)$ a la densitÃ© :
$$p(x|b) = \\frac{1}{2b} \\cdot \\exp\\left(-\\frac{|x|}{b}\\right)$$

- Moyenne : 0
- Variance : $2b^2$
- Plus b est grand, plus le bruit est important

**Application aux graphes** :

```
EntrÃ©e : G = (V, E), Îµ
Scale : b = 1/Îµ

Pour chaque paire (u, v) avec u < v :
  true_value = 1 si (u,v) âˆˆ E, 0 sinon
  noise = Laplace(0, b)
  noisy_value = true_value + noise

  Si noisy_value > 0.5 :
    Ajouter (u,v) Ã  E_output

Retourner G_output = (V, E_output)
```

**ThÃ©orÃ¨me** : Ce mÃ©canisme satisfait Îµ-differential privacy.

**Preuve (sketch)** :
Pour G et G' diffÃ©rant par une arÃªte (uâ‚€, vâ‚€) :

$$\\frac{P[M(G) = O]}{P[M(G') = O]} = \\exp(-\\varepsilon \\cdot |f(G)-f(G')|) \\leq e^\\varepsilon$$

car $|f(G) - f(G')| \\leq \\Delta f = 1$.

**Trade-off Îµ** :
- Îµ petit (ex: 0.1) : forte privacy, beaucoup de bruit, faible utilitÃ©
- Îµ grand (ex: 10) : faible privacy, peu de bruit, forte utilitÃ©
- Valeurs typiques : Îµ âˆˆ [0.1, 10]

**ComplexitÃ©** : O(nÂ²)
        """,
        "formula": r"M(D) = f(D) + \text{Lap}(\Delta f / \varepsilon)",
        "privacy_level": "TrÃ¨s Forte (Îµ-differential privacy)",
        "utility_preservation": "Variable (dÃ©pend de Îµ)"
    }
}


def calculate_anonymization_metrics(G_orig, G_anon):
    """Calcule des mÃ©triques d'anonymisation dÃ©taillÃ©es"""
    metrics = {}

    # Changements dans les arÃªtes
    if isinstance(G_anon, nx.Graph):
        orig_edges = set(G_orig.edges())
        anon_edges = set(G_anon.edges())

        added = len(anon_edges - orig_edges)
        removed = len(orig_edges - anon_edges)
        preserved = len(orig_edges & anon_edges)

        metrics['edges_added'] = added
        metrics['edges_removed'] = removed
        metrics['edges_preserved'] = preserved
        metrics['modification_rate'] = (added + removed) / (2 * len(orig_edges)) if len(orig_edges) > 0 else 0

        # Changements dans les degrÃ©s
        deg_orig = dict(G_orig.degree())
        deg_anon = dict(G_anon.degree())

        if set(deg_orig.keys()) == set(deg_anon.keys()):
            deg_changes = sum(abs(deg_orig[v] - deg_anon[v]) for v in deg_orig.keys())
            metrics['total_degree_change'] = deg_changes
            metrics['avg_degree_change'] = deg_changes / len(deg_orig)

            # Incorrectness (combien de nÅ“uds ont changÃ© de degrÃ©)
            metrics['nodes_with_degree_change'] = sum(1 for v in deg_orig.keys() if deg_orig[v] != deg_anon[v])
            metrics['degree_preservation_rate'] = 1 - (metrics['nodes_with_degree_change'] / len(deg_orig))

        # MÃ©triques structurelles
        try:
            metrics['clustering_change'] = abs(
                nx.average_clustering(G_orig) - nx.average_clustering(G_anon)
            )
        except:
            metrics['clustering_change'] = None

        metrics['density_change'] = abs(nx.density(G_orig) - nx.density(G_anon))

    return metrics


def calculate_privacy_guarantees(G_orig, G_anon, method_key, method_params):
    """Calcule les garanties de privacy spÃ©cifiques Ã  chaque mÃ©thode"""
    guarantees = {}

    if method_key == "k-degree anonymity":
        # VÃ©rifier la k-anonymitÃ© des degrÃ©s
        degrees = dict(G_anon.degree())
        degree_counts = Counter(degrees.values())

        k_value = method_params.get('k', 2)
        min_count = min(degree_counts.values()) if degree_counts else 0
        is_k_anonymous = min_count >= k_value

        guarantees['k_anonymity_satisfied'] = is_k_anonymous
        guarantees['min_degree_count'] = min_count
        guarantees['k_required'] = k_value
        guarantees['re_identification_risk'] = f"â‰¤ 1/{min_count}" if min_count > 0 else "N/A"
        guarantees['unique_degrees'] = len(degree_counts)

    elif method_key == "Generalization":
        # MÃ©triques pour super-nodes
        if hasattr(G_anon, 'graph'):
            cluster_sizes = [G_anon.nodes[n].get('size', 0) for n in G_anon.nodes()]
            min_cluster_size = min(cluster_sizes) if cluster_sizes else 0
            max_cluster_size = max(cluster_sizes) if cluster_sizes else 0
            avg_cluster_size = np.mean(cluster_sizes) if cluster_sizes else 0

            intra_edges = G_anon.graph.get('intra_edges', 0)
            inter_edges = G_anon.graph.get('inter_edges', 0)
            total_edges = intra_edges + inter_edges

            guarantees['num_clusters'] = G_anon.number_of_nodes()
            guarantees['min_cluster_size'] = min_cluster_size
            guarantees['max_cluster_size'] = max_cluster_size
            guarantees['avg_cluster_size'] = f"{avg_cluster_size:.1f}"
            guarantees['intra_cluster_edges'] = intra_edges
            guarantees['inter_cluster_edges'] = inter_edges
            guarantees['intra_ratio'] = f"{intra_edges/total_edges*100:.1f}%" if total_edges > 0 else "N/A"
            guarantees['inter_ratio'] = f"{inter_edges/total_edges*100:.1f}%" if total_edges > 0 else "N/A"
            guarantees['re_identification_risk'] = f"â‰¤ 1/{min_cluster_size}" if min_cluster_size > 0 else "N/A"
            guarantees['information_loss'] = f"{(1 - G_anon.number_of_nodes()/G_orig.number_of_nodes())*100:.1f}%"

    elif method_key in ["EdgeFlip", "Laplace"]:
        # Privacy diffÃ©rentielle
        epsilon = method_params.get('epsilon', 1.0)
        guarantees['epsilon'] = epsilon
        guarantees['privacy_budget'] = epsilon
        guarantees['privacy_level'] = "Forte" if epsilon < 1.0 else ("Moyenne" if epsilon < 2.0 else "Faible")
        guarantees['max_privacy_loss'] = f"e^{epsilon:.2f} â‰ˆ {np.exp(epsilon):.2f}"

        # Calculer le taux de faux positifs/nÃ©gatifs attendu
        if method_key == "EdgeFlip":
            s = 1 - np.exp(-epsilon)
            false_positive_rate = s/2
            false_negative_rate = s/2
            guarantees['expected_false_positive_rate'] = f"{false_positive_rate*100:.1f}%"
            guarantees['expected_false_negative_rate'] = f"{false_negative_rate*100:.1f}%"

    elif method_key == "Probabilistic":
        # (k,Îµ)-obfuscation
        k = method_params.get('k', 5)
        eps = method_params.get('epsilon', 0.3)

        guarantees['k_neighborhood'] = k
        guarantees['epsilon_tolerance'] = eps
        guarantees['min_entropy'] = f"log({k}) - {eps:.2f} â‰ˆ {np.log(k) - eps:.2f}"
        guarantees['uncertainty_level'] = "Ã‰levÃ©e" if eps < 0.5 else "Moyenne"
        guarantees['vulnerability'] = "âš ï¸ Reconstruction par seuillage possible!"

    elif method_key == "MaxVar":
        # MaxVar obfuscation
        num_pot = method_params.get('num_potential_edges', 50)

        guarantees['potential_edges'] = num_pot
        guarantees['optimization'] = "Programme quadratique (SLSQP)"
        guarantees['degree_preservation'] = "Exacte (E[deg(u)] = deg(u))"
        guarantees['variance_maximization'] = "âœ“ ProbabilitÃ©s dispersÃ©es"
        guarantees['threshold_resistance'] = "âœ“ RÃ©siste au seuillage Ã  0.5"

    elif method_key == "Random Switch":
        # PrÃ©servation de la sÃ©quence de degrÃ©s
        deg_orig = sorted([d for n, d in G_orig.degree()])
        deg_anon = sorted([d for n, d in G_anon.degree()])

        degree_sequence_preserved = deg_orig == deg_anon
        guarantees['degree_sequence_preserved'] = degree_sequence_preserved
        guarantees['structural_property'] = "SÃ©quence de degrÃ©s prÃ©servÃ©e" if degree_sequence_preserved else "ModifiÃ©e"

    elif method_key == "Random Add/Del":
        # Quantifier l'incertitude introduite
        k = method_params.get('k', 20)
        total_possible_edges = G_orig.number_of_nodes() * (G_orig.number_of_nodes() - 1) // 2

        guarantees['edges_modified'] = 2 * k  # k ajoutÃ©es + k supprimÃ©es (thÃ©orique)
        guarantees['modification_budget'] = k
        guarantees['structural_uncertainty'] = "ModÃ©rÃ©e"

    return guarantees


def sample_from_probabilistic_graph(prob_graph):
    """
    Tire un Ã©chantillon de graphe dÃ©terministe depuis un graphe probabiliste.

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRINCIPE DU TIRAGE (SAMPLING) :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Dans un graphe probabiliste (k,Îµ)-obfuscation, chaque arÃªte a une PROBABILITÃ‰
    d'existence. Pour crÃ©er un graphe dÃ©terministe, on effectue un TIRAGE au sort
    pour chaque arÃªte :

    - Si prob(arÃªte) = 0.95 â†’ 95% de chance d'apparaÃ®tre dans l'Ã©chantillon
    - Si prob(arÃªte) = 0.10 â†’ 10% de chance d'apparaÃ®tre dans l'Ã©chantillon

    Ce mÃ©canisme permet de :
    1. Publier plusieurs graphes Ã©chantillons diffÃ©rents
    2. CrÃ©er de la confusion pour l'attaquant (plusieurs graphes plausibles)
    3. Garantir que l'attaquant ne peut pas identifier le graphe original avec certitude

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ParamÃ¨tres:
        prob_graph : networkx.Graph avec attributs 'probability' sur les arÃªtes

    Retourne:
        networkx.Graph : Graphe dÃ©terministe Ã©chantillonnÃ©
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    import random

    # CrÃ©er un nouveau graphe avec les mÃªmes nÅ“uds
    sampled_graph = nx.Graph()
    sampled_graph.add_nodes_from(prob_graph.nodes())

    # Pour chaque arÃªte du graphe probabiliste
    for u, v in prob_graph.edges():
        # RÃ©cupÃ©rer la probabilitÃ©
        prob = prob_graph[u][v].get('probability', 0.5)

        # Tirer au sort : l'arÃªte apparaÃ®t si random < prob
        if random.random() < prob:
            sampled_graph.add_edge(u, v)

    return sampled_graph


def plot_probabilistic_graph(prob_graph, G_orig, method_name, ax):
    """
    Visualise un graphe probabiliste avec des arÃªtes de diffÃ©rentes intensitÃ©s.

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRINCIPE DE LA VISUALISATION :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    Dans un graphe probabiliste (k,Îµ)-obfuscation :
    - Les arÃªtes EXISTANTES ont une probabilitÃ© Ã‰LEVÃ‰E (â‰ˆ 1 - Îµ/k) â†’ FONCÃ‰ES
    - Les arÃªtes POTENTIELLES ont une probabilitÃ© FAIBLE (â‰ˆ Îµ/2k) â†’ CLAIRES

    Cette visualisation utilise :
    1. INTENSITÃ‰ DE COULEUR : Plus la probabilitÃ© est Ã©levÃ©e, plus l'arÃªte est foncÃ©e
    2. Ã‰PAISSEUR : Les arÃªtes Ã  haute probabilitÃ© sont plus Ã©paisses
    3. LÃ‰GENDE : Code couleur pour interprÃ©ter les probabilitÃ©s

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    import matplotlib.cm as cm
    from matplotlib.colors import LinearSegmentedColormap

    # Position pour visualisation
    pos = nx.spring_layout(G_orig, seed=42, k=0.5, iterations=50)

    # Dessiner les nÅ“uds
    nx.draw_networkx_nodes(prob_graph, pos, ax=ax,
                          node_color='lightcyan',
                          node_size=500, alpha=0.9,
                          edgecolors='darkblue', linewidths=2)

    # Collecter les arÃªtes par probabilitÃ©
    edges_with_prob = []
    for u, v in prob_graph.edges():
        prob = prob_graph[u][v].get('probability', 0.5)
        is_orig = prob_graph[u][v].get('is_original', False)
        edges_with_prob.append(((u, v), prob, is_orig))

    if not edges_with_prob:
        nx.draw_networkx_labels(prob_graph, pos, ax=ax, font_size=8, font_weight='bold')
        return

    # Trier par probabilitÃ© pour dessiner les faibles d'abord
    edges_with_prob.sort(key=lambda x: x[1])

    # CrÃ©er un colormap du clair (prob faible) au foncÃ© (prob Ã©levÃ©e)
    cmap = cm.get_cmap('RdYlGn')  # Rouge (faible) -> Jaune (moyen) -> Vert (Ã©levÃ©)

    # Dessiner chaque arÃªte avec sa couleur et Ã©paisseur
    for (u, v), prob, is_orig in edges_with_prob:
        # Couleur basÃ©e sur la probabilitÃ©
        color = cmap(prob)

        # Ã‰paisseur basÃ©e sur la probabilitÃ©
        width = 0.5 + 3.5 * prob  # De 0.5 (prob=0) Ã  4.0 (prob=1)

        # Style : solide pour arÃªtes originales, pointillÃ© pour potentielles
        style = 'solid' if is_orig else 'dotted'

        # Transparence basÃ©e sur la probabilitÃ©
        alpha = 0.3 + 0.6 * prob  # De 0.3 Ã  0.9

        nx.draw_networkx_edges(prob_graph, pos, [(u, v)], ax=ax,
                              edge_color=[color], width=width,
                              style=style, alpha=alpha)

    # Labels des nÅ“uds
    nx.draw_networkx_labels(prob_graph, pos, ax=ax, font_size=8, font_weight='bold')

    # CrÃ©er une lÃ©gende explicative
    from matplotlib.patches import Rectangle
    from matplotlib.lines import Line2D

    legend_elements = [
        Line2D([0], [0], color=cmap(0.95), linewidth=4, label='Prob. trÃ¨s Ã©levÃ©e (â‰ˆ 95%)'),
        Line2D([0], [0], color=cmap(0.70), linewidth=3, label='Prob. Ã©levÃ©e (â‰ˆ 70%)'),
        Line2D([0], [0], color=cmap(0.50), linewidth=2, label='Prob. moyenne (â‰ˆ 50%)'),
        Line2D([0], [0], color=cmap(0.30), linewidth=1.5, label='Prob. faible (â‰ˆ 30%)'),
        Line2D([0], [0], color=cmap(0.10), linewidth=1, linestyle='dotted', label='Prob. trÃ¨s faible (â‰ˆ 10%)'),
    ]

    ax.legend(handles=legend_elements, loc='upper right', fontsize=9, framealpha=0.9)
    ax.set_title(f'{method_name}\nGraphe Probabiliste ({prob_graph.number_of_nodes()} nÅ“uds, {prob_graph.number_of_edges()} arÃªtes)',
                fontsize=14, fontweight='bold')


def plot_graph_comparison(G_orig, G_anon, method_name, node_to_cluster=None):
    """
    CrÃ©e une comparaison visuelle des graphes.

    GÃ¨re plusieurs types de graphes :
    - Graphes classiques (Random Add/Del, Random Switch, k-anonymity)
    - Graphes probabilistes (k,Îµ)-obfuscation avec arÃªtes pondÃ©rÃ©es
    - Super-graphes (GÃ©nÃ©ralisation avec clusters)
    - Graphes diffÃ©rentiellement privÃ©s (EdgeFlip, Laplace)
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))

    # Position commune pour comparaison
    pos = nx.spring_layout(G_orig, seed=42, k=0.5, iterations=50)

    # Graphe original
    if node_to_cluster is not None:
        # Si c'est une gÃ©nÃ©ralisation, colorier par cluster
        clusters = {}
        for node, cluster in node_to_cluster.items():
            if cluster not in clusters:
                clusters[cluster] = []
            clusters[cluster].append(node)

        # GÃ©nÃ©rer des couleurs pour chaque cluster
        import matplotlib.cm as cm
        colors = cm.tab20(np.linspace(0, 1, len(clusters)))

        # Dessiner les nÅ“uds par cluster
        for idx, (cluster_id, nodes) in enumerate(clusters.items()):
            node_color = colors[idx % len(colors)]
            nx.draw_networkx_nodes(G_orig, pos, nodelist=nodes, ax=ax1,
                                  node_color=[node_color], node_size=500, alpha=0.7)

            # Encercler le cluster
            node_positions = np.array([pos[n] for n in nodes])
            if len(node_positions) > 0:
                center = node_positions.mean(axis=0)
                max_dist = np.max(np.linalg.norm(node_positions - center, axis=1))
                circle = plt.Circle(center, max_dist + 0.15, color=node_color,
                                  fill=False, linewidth=2.5, linestyle='--', alpha=0.6)
                ax1.add_patch(circle)

        # Dessiner les arÃªtes par type
        intra_edges = []
        inter_edges = []
        for u, v in G_orig.edges():
            if node_to_cluster[u] == node_to_cluster[v]:
                intra_edges.append((u, v))
            else:
                inter_edges.append((u, v))

        if intra_edges:
            nx.draw_networkx_edges(G_orig, pos, intra_edges, ax=ax1,
                                  edge_color='green', width=2, alpha=0.4,
                                  label='Intra-cluster')
        if inter_edges:
            nx.draw_networkx_edges(G_orig, pos, inter_edges, ax=ax1,
                                  edge_color='red', width=1.5, alpha=0.5,
                                  label='Inter-cluster', style='dashed')

        ax1.legend(loc='upper right')
        ax1.set_title(f'Graphe Original avec Clusters\n{G_orig.number_of_nodes()} nÅ“uds, {len(clusters)} clusters',
                      fontsize=14, fontweight='bold')
    else:
        # Affichage normal
        nx.draw_networkx_nodes(G_orig, pos, ax=ax1, node_color='lightblue',
                              node_size=500, alpha=0.9)
        nx.draw_networkx_edges(G_orig, pos, ax=ax1, edge_color='gray',
                              width=1.5, alpha=0.6)
        ax1.set_title(f'Graphe Original\n{G_orig.number_of_nodes()} nÅ“uds, {G_orig.number_of_edges()} arÃªtes',
                      fontsize=14, fontweight='bold')

    nx.draw_networkx_labels(G_orig, pos, ax=ax1, font_size=8, font_weight='bold')
    ax1.axis('off')

    # Graphe anonymisÃ©
    if isinstance(G_anon, nx.Graph) and G_anon.number_of_nodes() > 0:
        # Adapter la position si diffÃ©rent nombre de nÅ“uds
        if set(G_anon.nodes()) != set(G_orig.nodes()):
            pos_anon = nx.spring_layout(G_anon, seed=42, k=0.5, iterations=50)

            # Si c'est un super-graphe, ajuster la visualisation
            if node_to_cluster is not None and hasattr(G_anon, 'graph'):
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # VISUALISATION AMÃ‰LIORÃ‰E DU SUPER-GRAPHE
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                # Tailles proportionnelles au nombre de nÅ“uds dans chaque cluster
                node_sizes = [G_anon.nodes[n].get('size', 1) * 300 for n in G_anon.nodes()]

                # Couleurs variÃ©es pour distinguer les clusters
                import matplotlib.cm as cm
                num_clusters = G_anon.number_of_nodes()
                colors_palette = cm.Set3(np.linspace(0, 1, num_clusters))

                # Dessiner les super-nÅ“uds
                for idx, node in enumerate(G_anon.nodes()):
                    nx.draw_networkx_nodes(G_anon, pos_anon, nodelist=[node], ax=ax2,
                                          node_color=[colors_palette[idx]],
                                          node_size=node_sizes[idx],
                                          alpha=0.85, edgecolors='darkblue',
                                          linewidths=2.5)

                # SÃ©parer les arÃªtes intra-cluster (self-loops) et inter-cluster
                intra_edges = []
                inter_edges = []
                intra_weights = []
                inter_weights = []

                for u, v in G_anon.edges():
                    weight = G_anon[u][v].get('weight', 1)
                    if u == v:  # Self-loop (arÃªtes intra-cluster)
                        intra_edges.append((u, v))
                        intra_weights.append(weight)
                    else:  # ArÃªtes inter-cluster
                        inter_edges.append((u, v))
                        inter_weights.append(weight)

                # Calculer les poids max pour normalisation
                max_intra_weight = max(intra_weights) if intra_weights else 1
                max_inter_weight = max(inter_weights) if inter_weights else 1

                # Dessiner les arÃªtes INTRA-cluster (self-loops)
                if intra_edges:
                    for (u, v), weight in zip(intra_edges, intra_weights):
                        # Dessiner un cercle autour du nÅ“ud pour reprÃ©senter les arÃªtes internes
                        node_pos = pos_anon[u]
                        radius = 0.08 + 0.12 * (weight / max_intra_weight)
                        circle = plt.Circle(node_pos, radius, color='green',
                                          fill=False, linewidth=2 + 3*(weight/max_intra_weight),
                                          linestyle='solid', alpha=0.6)
                        ax2.add_patch(circle)

                # Dessiner les arÃªtes INTER-cluster
                if inter_edges:
                    for (u, v), weight in zip(inter_edges, inter_weights):
                        width = 1.5 + 4.5 * (weight / max_inter_weight)
                        nx.draw_networkx_edges(G_anon, pos_anon, [(u, v)], ax=ax2,
                                              width=width, alpha=0.7, edge_color='purple',
                                              style='solid')

                # Labels avec dÃ©tails : ID + taille + poids intra
                labels = {}
                for n in G_anon.nodes():
                    size = G_anon.nodes[n].get('size', '?')
                    intra_weight = 0
                    if G_anon.has_edge(n, n):
                        intra_weight = G_anon[n][n].get('weight', 0)
                    labels[n] = f"C{n}\n[{size}n]\n{intra_weight}i"

                nx.draw_networkx_labels(G_anon, pos_anon, labels, ax=ax2,
                                       font_size=9, font_weight='bold',
                                       bbox=dict(boxstyle='round,pad=0.3',
                                                facecolor='white', alpha=0.8,
                                                edgecolor='black'))

                # LÃ©gende explicative
                from matplotlib.lines import Line2D
                legend_elements = [
                    Line2D([0], [0], color='green', linewidth=3, linestyle='solid',
                          label='ArÃªtes intra-cluster (vert)'),
                    Line2D([0], [0], color='purple', linewidth=3, linestyle='solid',
                          label='ArÃªtes inter-cluster (violet)'),
                ]
                ax2.legend(handles=legend_elements, loc='upper right', fontsize=9, framealpha=0.9)

                # Titre avec statistiques complÃ¨tes
                total_intra = sum(intra_weights)
                total_inter = sum(inter_weights)
                ax2.set_title(f'Super-Graphe - {method_name}\n{G_anon.number_of_nodes()} clusters | {int(total_intra)} arÃªtes intra | {int(total_inter)} arÃªtes inter',
                             fontsize=13, fontweight='bold')
            else:
                # Graphe normal avec nÅ“uds diffÃ©rents
                nx.draw_networkx_nodes(G_anon, pos_anon, ax=ax2, node_color='lightgreen',
                                      node_size=500, alpha=0.9)
                nx.draw_networkx_edges(G_anon, pos_anon, ax=ax2, edge_color='gray',
                                      width=1.5, alpha=0.6)
                nx.draw_networkx_labels(G_anon, pos_anon, ax=ax2, font_size=8, font_weight='bold')
                ax2.set_title(f'Graphe AnonymisÃ© - {method_name}\n{G_anon.number_of_nodes()} nÅ“uds',
                             fontsize=14, fontweight='bold')
        else:
            pos_anon = pos

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # DÃ‰TECTION DE GRAPHE PROBABILISTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # VÃ©rifier si le graphe a des arÃªtes avec des probabilitÃ©s
            has_probabilities = False
            if G_anon.number_of_edges() > 0:
                first_edge = list(G_anon.edges())[0]
                has_probabilities = 'probability' in G_anon[first_edge[0]][first_edge[1]]

            if has_probabilities:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # VISUALISATION PROBABILISTE AMÃ‰LIORÃ‰E
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                plot_probabilistic_graph(G_anon, G_orig, method_name, ax2)
            else:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # VISUALISATION CLASSIQUE (graphes dÃ©terministes)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                orig_edges = set(G_orig.edges())

                # Dessiner les nÅ“uds
                nx.draw_networkx_nodes(G_anon, pos_anon, ax=ax2, node_color='lightgreen',
                                      node_size=500, alpha=0.9)

                # Dessiner les arÃªtes par type
                preserved_edges = [(u,v) for u,v in G_anon.edges()
                                  if (u,v) in orig_edges or (v,u) in orig_edges]
                added_edges = [(u,v) for u,v in G_anon.edges()
                              if (u,v) not in orig_edges and (v,u) not in orig_edges]

                if preserved_edges:
                    nx.draw_networkx_edges(G_anon, pos_anon, preserved_edges, ax=ax2,
                                          edge_color='blue', width=1.5, alpha=0.6,
                                          style='solid', label='ArÃªtes prÃ©servÃ©es')
                if added_edges:
                    nx.draw_networkx_edges(G_anon, pos_anon, added_edges, ax=ax2,
                                          edge_color='red', width=1.5, alpha=0.6,
                                          style='dashed', label='ArÃªtes ajoutÃ©es')

                nx.draw_networkx_labels(G_anon, pos_anon, ax=ax2, font_size=8, font_weight='bold')
                ax2.legend(loc='upper right')

                ax2.set_title(f'Graphe AnonymisÃ© - {method_name}\n{G_anon.number_of_nodes()} nÅ“uds, {G_anon.number_of_edges()} arÃªtes',
                             fontsize=14, fontweight='bold')
    else:
        ax2.text(0.5, 0.5, 'Graphe non visualisable\n(format incompatible)',
                ha='center', va='center', fontsize=12)
        ax2.set_title(f'Graphe AnonymisÃ© - {method_name}', fontsize=14, fontweight='bold')

    ax2.axis('off')

    plt.tight_layout()
    return fig


def plot_degree_distribution(G_orig, G_anon, method_name):
    """
    Compare les distributions de degrÃ©s.

    GÃ¨re 3 cas spÃ©ciaux :
    1. Graphe probabiliste â†’ Ã‰chantillonner avant de calculer
    2. Super-graphe â†’ Tirage uniforme depuis les clusters
    3. Graphe classique â†’ Distribution standard
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Distribution originale
    degrees_orig = [d for n, d in G_orig.degree()]
    ax1.hist(degrees_orig, bins=range(max(degrees_orig)+2),
            alpha=0.7, color='blue', edgecolor='black', rwidth=0.8)
    ax1.set_xlabel('DegrÃ©', fontsize=12)
    ax1.set_ylabel('Nombre de nÅ“uds', fontsize=12)
    ax1.set_title('Distribution des degrÃ©s - Original', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.set_axisbelow(True)

    # Distribution anonymisÃ©e
    if isinstance(G_anon, nx.Graph) and G_anon.number_of_nodes() > 0:
        # CAS 1 : DÃ©tecter si c'est un super-graphe (gÃ©nÃ©ralisation)
        is_super_graph = False
        if G_anon.number_of_nodes() > 0:
            first_node = list(G_anon.nodes())[0]
            node_data = G_anon.nodes[first_node]
            is_super_graph = 'cluster_size' in node_data

        if is_super_graph:
            # TIRAGE UNIFORME depuis le super-graphe pour recrÃ©er une distribution
            degrees_reconstructed = []

            # Pour chaque cluster
            for cluster_id in G_anon.nodes():
                cluster_size = G_anon.nodes[cluster_id]['cluster_size']
                internal_edges = G_anon.nodes[cluster_id]['internal_edges']

                # Compter les arÃªtes inter-cluster pour ce cluster
                inter_edges_count = 0
                for neighbor in G_anon.neighbors(cluster_id):
                    if neighbor != cluster_id:  # Exclure self-loops
                        inter_edges_count += G_anon[cluster_id][neighbor]['weight']

                # Estimer le degrÃ© moyen dans ce cluster
                # DegrÃ© interne moyen â‰ˆ 2 Ã— internal_edges / cluster_size
                avg_internal_degree = (2 * internal_edges) / cluster_size if cluster_size > 0 else 0

                # DegrÃ© externe moyen â‰ˆ inter_edges_count / cluster_size
                avg_external_degree = inter_edges_count / cluster_size if cluster_size > 0 else 0

                # DegrÃ© total moyen pour les nÅ“uds de ce cluster
                avg_degree = avg_internal_degree + avg_external_degree

                # Tirer des degrÃ©s avec une petite variance (Â± 20%)
                for _ in range(cluster_size):
                    # Ajouter un peu de bruit pour simuler la variabilitÃ©
                    degree = int(max(0, avg_degree + np.random.normal(0, avg_degree * 0.2)))
                    degrees_reconstructed.append(degree)

            ax2.hist(degrees_reconstructed, bins=range(max(degrees_reconstructed)+2) if degrees_reconstructed else [0],
                    alpha=0.7, color='orange', edgecolor='black', rwidth=0.8)
            ax2.set_xlabel('DegrÃ©', fontsize=12)
            ax2.set_ylabel('Nombre de nÅ“uds', fontsize=12)
            ax2.set_title(f'Distribution des degrÃ©s - {method_name}\n(Tirage uniforme depuis clusters)',
                         fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3, linestyle='--')
            ax2.set_axisbelow(True)

        else:
            # CAS 2 : DÃ©tecter si c'est un graphe probabiliste
            is_probabilistic = False
            if G_anon.number_of_edges() > 0:
                first_edge = list(G_anon.edges())[0]
                is_probabilistic = 'probability' in G_anon[first_edge[0]][first_edge[1]]

            if is_probabilistic:
                # Ã‰CHANTILLONNER le graphe probabiliste
                G_sample = sample_from_probabilistic_graph(G_anon)
                degrees_anon = [d for n, d in G_sample.degree()]

                ax2.hist(degrees_anon, bins=range(max(degrees_anon)+2) if degrees_anon else [0],
                        alpha=0.7, color='purple', edgecolor='black', rwidth=0.8)
                ax2.set_xlabel('DegrÃ©', fontsize=12)
                ax2.set_ylabel('Nombre de nÅ“uds', fontsize=12)
                ax2.set_title(f'Distribution des degrÃ©s - {method_name}\n(Ã‰chantillon)',
                             fontsize=14, fontweight='bold')
                ax2.grid(True, alpha=0.3, linestyle='--')
                ax2.set_axisbelow(True)

            # CAS 3 : Graphe classique
            elif set(G_anon.nodes()).issubset(set(G_orig.nodes())):
                degrees_anon = [d for n, d in G_anon.degree()]
                ax2.hist(degrees_anon, bins=range(max(degrees_anon)+2),
                        alpha=0.7, color='green', edgecolor='black', rwidth=0.8)
                ax2.set_xlabel('DegrÃ©', fontsize=12)
                ax2.set_ylabel('Nombre de nÅ“uds', fontsize=12)
                ax2.set_title(f'Distribution des degrÃ©s - {method_name}', fontsize=14, fontweight='bold')
                ax2.grid(True, alpha=0.3, linestyle='--')
                ax2.set_axisbelow(True)
            else:
                ax2.text(0.5, 0.5, 'Distribution non comparable\n(nÅ“uds diffÃ©rents)',
                        ha='center', va='center', fontsize=12)
                ax2.axis('off')
    else:
        ax2.text(0.5, 0.5, 'Pas de distribution\n(format non standard)',
                ha='center', va='center', fontsize=12)
        ax2.axis('off')

    plt.tight_layout()
    return fig


def simulate_degree_attack(G_orig, G_anon, target_node=0):
    """
    Simule une ATTAQUE PAR DEGRÃ‰ (Degree Attack) sur le graphe anonymisÃ©.

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRINCIPE DE L'ATTAQUE :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    L'adversaire connaÃ®t le DEGRÃ‰ (nombre d'amis/connexions) d'un nÅ“ud cible
    dans le graphe original et tente de le retrouver dans le graphe anonymisÃ©
    en cherchant les nÅ“uds ayant le mÃªme degrÃ©.

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    EXEMPLE CONCRET (Karate Club) :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    - Alice (instructrice) a 16 amis dans le club (information publique)
    - L'attaquant cherche dans le graphe anonymisÃ© tous les nÅ“uds de degrÃ© 16
    - Si UN SEUL nÅ“ud a degrÃ© 16 â†’ Alice est rÃ©-identifiÃ©e avec 100% de certitude
    - Si k nÅ“uds ont degrÃ© 16 â†’ ProbabilitÃ© de rÃ©-identification = 1/k

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    MÃ‰TRIQUES DE PRIVACY CALCULÃ‰ES :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    1. INCORRECTNESS : Nombre de fausses suppositions de l'attaquant
       â†’ Plus cette valeur est Ã©levÃ©e, meilleure est la privacy
       â†’ Incorrectness = k - 1 (k candidats signifie k-1 erreurs potentielles)

    2. MIN ENTROPY : logâ‚‚(k) bits
       â†’ Mesure l'incertitude de l'attaquant
       â†’ 0 bits = aucune privacy (1 candidat)
       â†’ 1 bit = privacy faible (2 candidats)
       â†’ 3 bits = privacy moyenne (8 candidats)
       â†’ 5+ bits = bonne privacy (32+ candidats)

    3. PROBABILITÃ‰ DE RÃ‰-IDENTIFICATION : 1/k
       â†’ Chance que l'attaquant devine correctement au hasard

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PARAMÃˆTRES :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    G_orig : networkx.Graph
        Graphe original (avant anonymisation)
    G_anon : networkx.Graph ou autre
        Graphe aprÃ¨s anonymisation
    target_node : int
        NÅ“ud que l'attaquant cherche Ã  rÃ©-identifier (par dÃ©faut : nÅ“ud 0)

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    RETOURNE :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    dict contenant:
        - attack_type : Type d'attaque ('Degree Attack')
        - target_node : NÅ“ud ciblÃ©
        - target_degree : DegrÃ© du nÅ“ud cible
        - candidates : Liste des nÅ“uds candidats dans le graphe anonymisÃ©
        - success : True si rÃ©-identification rÃ©ussie (1 seul candidat)
        - re_identification_probability : 1/nombre_candidats
        - incorrectness : Nombre de fausses suppositions (k-1)
        - min_entropy_bits : logâ‚‚(k) bits de privacy
        - explanation : Explication textuelle du rÃ©sultat
    """

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # INITIALISATION DES RÃ‰SULTATS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    results = {
        'attack_type': 'Degree Attack',
        'target_node': target_node,
        'success': False,
        'candidates': [],
        'explanation': ''
    }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VÃ‰RIFICATION : L'attaque n'est possible que sur des graphes classiques
    # (pas sur les super-nÅ“uds de la gÃ©nÃ©ralisation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if not isinstance(G_anon, nx.Graph):
        results['explanation'] = "âš ï¸ Attaque impossible sur ce type de graphe (super-nodes). La gÃ©nÃ©ralisation dÃ©truit les degrÃ©s individuels."
        results['incorrectness'] = float('inf')  # Privacy parfaite
        results['min_entropy_bits'] = float('inf')
        results['re_identification_probability'] = 0.0
        return results

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 1 : CONNAISSANCE DE L'ATTAQUANT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # L'attaquant connaÃ®t le degrÃ© du nÅ“ud cible dans le graphe ORIGINAL
    # (par exemple via un profil public, un annuaire, ou sa propre connaissance)
    target_degree = G_orig.degree(target_node)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 2 : RECHERCHE DES CANDIDATS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # L'attaquant cherche TOUS les nÅ“uds du graphe anonymisÃ© ayant le mÃªme degrÃ©
    candidates = [n for n in G_anon.nodes() if G_anon.degree(n) == target_degree]

    k = len(candidates)  # Nombre de nÅ“uds indistinguables (anonymity set size)

    results['candidates'] = candidates
    results['target_degree'] = target_degree

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 3 : CALCUL DES MÃ‰TRIQUES DE PRIVACY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    if k == 1:
        # CAS 1 : RÃ‰-IDENTIFICATION RÃ‰USSIE (Privacy = 0)
        # Un seul candidat â†’ L'attaquant est CERTAIN de l'identitÃ©
        results['success'] = True
        results['re_identified_node'] = candidates[0]
        results['re_identification_probability'] = 1.0  # 100% de certitude
        results['incorrectness'] = 0  # Aucune fausse supposition possible
        results['min_entropy_bits'] = 0.0  # Aucune incertitude (logâ‚‚(1) = 0)
        results['explanation'] = (
            f"âœ… **RÃ‰-IDENTIFICATION RÃ‰USSIE !**\n\n"
            f"Le nÅ“ud cible {target_node} a un degrÃ© UNIQUE ({target_degree} connexions).\n"
            f"Un seul nÅ“ud dans le graphe anonymisÃ© a ce degrÃ©.\n\n"
            f"ğŸ“Š **MÃ©triques de Privacy** :\n"
            f"- ProbabilitÃ© de rÃ©-identification : **100%** (certitude absolue)\n"
            f"- Incorrectness : **0** (aucune erreur possible)\n"
            f"- Min Entropy : **0 bits** (aucune privacy)\n\n"
            f"ğŸ”´ **DANGER** : L'attaquant peut maintenant dÃ©couvrir toutes les connexions du nÅ“ud {target_node} !"
        )

    elif k == 0:
        # CAS 2 : AUCUN CANDIDAT TROUVÃ‰
        # Le degrÃ© a Ã©tÃ© modifiÃ© par l'anonymisation (randomisation, DP, etc.)
        results['success'] = False
        results['re_identification_probability'] = 0.0
        results['incorrectness'] = float('inf')  # Protection parfaite
        results['min_entropy_bits'] = float('inf')
        results['explanation'] = (
            f"âŒ **ATTAQUE Ã‰CHOUÃ‰E - Aucun candidat**\n\n"
            f"Aucun nÅ“ud avec degrÃ© {target_degree} trouvÃ© dans le graphe anonymisÃ©.\n"
            f"Le degrÃ© du nÅ“ud cible a Ã©tÃ© modifiÃ© par l'anonymisation.\n\n"
            f"ğŸ“Š **MÃ©triques de Privacy** :\n"
            f"- ProbabilitÃ© de rÃ©-identification : **0%**\n"
            f"- Incorrectness : **âˆ** (impossible de deviner)\n"
            f"- Min Entropy : **âˆ bits** (privacy maximale)\n\n"
            f"ğŸŸ¢ **SÃ‰CURITÃ‰** : Excellente protection contre cette attaque !"
        )

    else:
        # CAS 3 : RÃ‰-IDENTIFICATION AMBIGUÃ‹ (k-anonymity)
        # Plusieurs candidats â†’ L'attaquant doit deviner parmi k nÅ“uds
        results['success'] = False
        results['re_identification_probability'] = 1.0 / k
        results['incorrectness'] = k - 1  # Nombre de fausses suppositions
        results['min_entropy_bits'] = np.log2(k)  # Bits de privacy

        # Ã‰valuation qualitative de la privacy
        if k >= 10:
            privacy_level = "ğŸŸ¢ FORTE"
            privacy_comment = "Excellente protection"
        elif k >= 5:
            privacy_level = "ğŸŸ¡ MOYENNE"
            privacy_comment = "Protection acceptable"
        else:
            privacy_level = "ğŸŸ  FAIBLE"
            privacy_comment = "Protection limitÃ©e"

        results['explanation'] = (
            f"âš ï¸ **RÃ‰-IDENTIFICATION AMBIGUÃ‹**\n\n"
            f"{k} nÅ“uds ont le degrÃ© {target_degree} dans le graphe anonymisÃ©.\n"
            f"L'attaquant doit deviner au hasard parmi ces {k} candidats.\n\n"
            f"ğŸ“Š **MÃ©triques de Privacy** :\n"
            f"- ProbabilitÃ© de rÃ©-identification : **{1/k*100:.1f}%** (1/{k})\n"
            f"- Incorrectness : **{k-1}** fausses suppositions possibles\n"
            f"- Min Entropy : **{np.log2(k):.2f} bits** de privacy\n"
            f"- Niveau de privacy : {privacy_level}\n\n"
            f"ğŸ’¡ **InterprÃ©tation** : {privacy_comment}. "
            f"Le graphe satisfait la **{k}-anonymitÃ©** pour ce nÅ“ud."
        )

    return results


def simulate_subgraph_attack(G_orig, G_anon, target_node=0):
    """
    Simule une ATTAQUE PAR SOUS-GRAPHE (Subgraph/Neighborhood Attack) sur le graphe.

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRINCIPE DE L'ATTAQUE :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    L'adversaire connaÃ®t la STRUCTURE LOCALE autour du nÅ“ud cible, notamment :
    - Le nombre de connexions (degrÃ©)
    - Les TRIANGLES formÃ©s avec ses voisins (amis communs)
    - Le coefficient de clustering (densitÃ© du voisinage)

    Cette attaque est BEAUCOUP PLUS PUISSANTE que l'attaque par degrÃ© seul,
    car elle exploite des MOTIFS STRUCTURELS (patterns) qui sont souvent uniques.

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    EXEMPLE CONCRET (Karate Club) :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    - Mr. Hi (nÅ“ud 0) a 16 amis ET forme 45 triangles avec eux
    - Pattern : [degrÃ©=16, triangles=45]
    - L'attaquant cherche ce pattern dans le graphe anonymisÃ©
    - Ce pattern est souvent UNIQUE â†’ RÃ©-identification rÃ©ussie

    Comparaison avec Degree Attack :
    - Degree Attack : Cherche seulement "degrÃ© = 16"
      â†’ Peut trouver plusieurs candidats (k-anonymity)
    - Subgraph Attack : Cherche "degrÃ© = 16 ET 45 triangles"
      â†’ Pattern beaucoup plus distinctif â†’ Moins de candidats

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PROTECTION CONTRE CETTE ATTAQUE :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ğŸŸ¢ GÃ‰NÃ‰RALISATION (Super-nÅ“uds) : â˜…â˜…â˜…â˜…â˜…
       â†’ DÃ©truit complÃ¨tement les motifs locaux en regroupant les nÅ“uds
       â†’ L'attaque devient IMPOSSIBLE

    ğŸŸ¢ DIFFERENTIAL PRIVACY : â˜…â˜…â˜…â˜…â˜†
       â†’ Ajoute/supprime des triangles de maniÃ¨re alÃ©atoire
       â†’ Brouille les patterns structurels

    ğŸŸ  RANDOMISATION : â˜…â˜…â˜†â˜†â˜†
       â†’ Peut prÃ©server certains triangles
       â†’ Protection limitÃ©e

    ğŸ”´ k-DEGREE ANONYMITY : â˜…â˜†â˜†â˜†â˜†
       â†’ Ne protÃ¨ge que le degrÃ©, pas les triangles
       â†’ VULNÃ‰RABLE Ã  cette attaque

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    MÃ‰TRIQUES DE PRIVACY CALCULÃ‰ES :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    1. STRUCTURAL QUERY H2 : (degrÃ©, nombre_triangles)
       â†’ DÃ©crit la structure locale du nÅ“ud

    2. INCORRECTNESS : k - 1 (nombre de fausses suppositions)

    3. MIN ENTROPY : logâ‚‚(k) bits d'incertitude

    4. PROBABILITÃ‰ DE RÃ‰-IDENTIFICATION : 1/k

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PARAMÃˆTRES :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    G_orig : networkx.Graph
        Graphe original (avant anonymisation)
    G_anon : networkx.Graph ou autre
        Graphe aprÃ¨s anonymisation
    target_node : int
        NÅ“ud que l'attaquant cherche Ã  rÃ©-identifier

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    RETOURNE :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    dict contenant:
        - attack_type : 'Subgraph Attack'
        - target_node : NÅ“ud ciblÃ©
        - target_degree : DegrÃ© du nÅ“ud cible
        - target_triangles : Nombre de triangles formÃ©s par le nÅ“ud
        - clustering_coefficient : Coefficient de clustering
        - candidates : Liste des nÅ“uds candidats
        - success : True si rÃ©-identification unique
        - re_identification_probability : 1/k
        - incorrectness : k-1
        - min_entropy_bits : logâ‚‚(k)
        - explanation : Explication dÃ©taillÃ©e
    """

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # INITIALISATION DES RÃ‰SULTATS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    results = {
        'attack_type': 'Subgraph Attack',
        'target_node': target_node,
        'success': False,
        'candidates': [],
        'explanation': ''
    }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VÃ‰RIFICATION : L'attaque nÃ©cessite des graphes avec structure locale
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if not isinstance(G_anon, nx.Graph):
        results['explanation'] = (
            "âš ï¸ **Attaque impossible sur ce type de graphe (super-nodes)**\n\n"
            "La GÃ‰NÃ‰RALISATION dÃ©truit les motifs locaux (triangles, voisinages).\n"
            "C'est justement la FORCE de cette mÃ©thode contre les attaques structurelles !\n\n"
            "ğŸŸ¢ Protection : **EXCELLENTE** (â˜…â˜…â˜…â˜…â˜…)"
        )
        results['incorrectness'] = float('inf')
        results['min_entropy_bits'] = float('inf')
        results['re_identification_probability'] = 0.0
        return results

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 1 : ANALYSER LA STRUCTURE LOCALE DU NÅ’UD CIBLE (Graphe Original)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Compter les TRIANGLES dont le nÅ“ud fait partie
    # Un triangle = 3 nÅ“uds tous connectÃ©s entre eux (A-B, B-C, A-C)
    target_triangles = []
    for u, v in G_orig.edges(target_node):
        # Si u et v sont aussi connectÃ©s â†’ triangle [target, u, v]
        if G_orig.has_edge(u, v):
            target_triangles.append(sorted([target_node, u, v]))

    # Si le nÅ“ud n'a aucun triangle, l'attaque structurelle est limitÃ©e
    if not target_triangles:
        results['explanation'] = (
            f"âš ï¸ Le nÅ“ud {target_node} ne fait partie d'AUCUN triangle.\n\n"
            f"Cette attaque nÃ©cessite des motifs structurels (triangles).\n"
            f"Utilisez plutÃ´t une **Degree Attack** pour ce nÅ“ud."
        )
        return results

    # CaractÃ©ristiques structurelles du nÅ“ud cible
    target_degree = G_orig.degree(target_node)
    target_triangle_count = len(target_triangles)

    # Coefficient de clustering : proportion de voisins connectÃ©s entre eux
    try:
        target_clustering = nx.clustering(G_orig, target_node)
    except:
        target_clustering = 0.0

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 2 : RECHERCHER LE PATTERN STRUCTUREL DANS LE GRAPHE ANONYMISÃ‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # L'attaquant cherche les nÅ“uds ayant le MÃŠME PATTERN : (degrÃ©, triangles)

    candidates = []
    for n in G_anon.nodes():
        # Filtrer d'abord par degrÃ© (critÃ¨re rapide)
        if G_anon.degree(n) == target_degree:
            # Compter les triangles pour ce nÅ“ud candidat
            node_triangles = 0
            for u, v in G_anon.edges(n):
                if G_anon.has_edge(u, v):
                    node_triangles += 1

            # Si le nombre de triangles correspond aussi â†’ Candidat potentiel !
            if node_triangles == target_triangle_count:
                candidates.append(n)

    k = len(candidates)  # Taille de l'ensemble d'anonymat

    results['candidates'] = candidates
    results['target_degree'] = target_degree
    results['target_triangles'] = target_triangle_count
    results['clustering_coefficient'] = target_clustering

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 3 : Ã‰VALUATION DU SUCCÃˆS DE L'ATTAQUE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    if k == 1:
        # CAS 1 : PATTERN UNIQUE â†’ RÃ‰-IDENTIFICATION RÃ‰USSIE
        results['success'] = True
        results['re_identified_node'] = candidates[0]
        results['re_identification_probability'] = 1.0
        results['incorrectness'] = 0
        results['min_entropy_bits'] = 0.0
        results['explanation'] = (
            f"âœ… **RÃ‰-IDENTIFICATION RÃ‰USSIE !**\n\n"
            f"Le pattern structurel du nÅ“ud {target_node} est UNIQUE :\n"
            f"- DegrÃ© : **{target_degree}** connexions\n"
            f"- Triangles : **{target_triangle_count}**\n"
            f"- Clustering : **{target_clustering:.3f}**\n\n"
            f"Un seul nÅ“ud dans le graphe anonymisÃ© possÃ¨de ce pattern.\n\n"
            f"ğŸ“Š **MÃ©triques de Privacy** :\n"
            f"- ProbabilitÃ© de rÃ©-identification : **100%**\n"
            f"- Incorrectness : **0** (certitude absolue)\n"
            f"- Min Entropy : **0 bits**\n\n"
            f"ğŸ”´ **DANGER** : L'attaque structurelle est PLUS PUISSANTE que l'attaque par degrÃ©.\n"
            f"ğŸ’¡ **Protection recommandÃ©e** : GÃ©nÃ©ralisation ou Differential Privacy"
        )

    elif k == 0:
        # CAS 2 : AUCUN CANDIDAT â†’ STRUCTURE MODIFIÃ‰E
        results['success'] = False
        results['re_identification_probability'] = 0.0
        results['incorrectness'] = float('inf')
        results['min_entropy_bits'] = float('inf')
        results['explanation'] = (
            f"âŒ **ATTAQUE Ã‰CHOUÃ‰E - Structure modifiÃ©e**\n\n"
            f"Aucun nÅ“ud ne correspond au pattern recherchÃ© :\n"
            f"- DegrÃ© : {target_degree}\n"
            f"- Triangles : {target_triangle_count}\n\n"
            f"L'anonymisation a modifiÃ© la structure locale du graphe.\n\n"
            f"ğŸ“Š **MÃ©triques de Privacy** :\n"
            f"- ProbabilitÃ© de rÃ©-identification : **0%**\n"
            f"- Incorrectness : **âˆ**\n"
            f"- Min Entropy : **âˆ bits**\n\n"
            f"ğŸŸ¢ **SÃ‰CURITÃ‰** : Excellente protection contre cette attaque structurelle !"
        )

    else:
        # CAS 3 : PLUSIEURS CANDIDATS â†’ AMBIGUÃTÃ‰
        results['success'] = False
        results['re_identification_probability'] = 1.0 / k
        results['incorrectness'] = k - 1
        results['min_entropy_bits'] = np.log2(k)

        # Ã‰valuation qualitative
        if k >= 8:
            privacy_level = "ğŸŸ¢ FORTE"
            protection_comment = "Excellente protection structurelle"
        elif k >= 4:
            privacy_level = "ğŸŸ¡ MOYENNE"
            protection_comment = "Protection acceptable"
        else:
            privacy_level = "ğŸŸ  FAIBLE"
            protection_comment = "Protection limitÃ©e - Pattern encore distinctif"

        results['explanation'] = (
            f"âš ï¸ **RÃ‰-IDENTIFICATION AMBIGUÃ‹**\n\n"
            f"{k} nÅ“uds partagent le pattern structurel :\n"
            f"- DegrÃ© : **{target_degree}**\n"
            f"- Triangles : **{target_triangle_count}**\n"
            f"- Clustering : **{target_clustering:.3f}**\n\n"
            f"L'attaquant doit deviner parmi {k} candidats.\n\n"
            f"ğŸ“Š **MÃ©triques de Privacy** :\n"
            f"- ProbabilitÃ© de rÃ©-identification : **{1/k*100:.1f}%** (1/{k})\n"
            f"- Incorrectness : **{k-1}** fausses suppositions\n"
            f"- Min Entropy : **{np.log2(k):.2f} bits**\n"
            f"- Niveau de privacy : {privacy_level}\n\n"
            f"ğŸ’¡ **InterprÃ©tation** : {protection_comment}.\n\n"
            f"âš ï¸ **Note** : Cette attaque est plus discriminante qu'une simple Degree Attack.\n"
            f"Pour une meilleure protection, utilisez la GÃ©nÃ©ralisation ou Differential Privacy."
        )

    return results


def calculate_supergraph_metrics(G_orig, G_super):
    """
    Calcule les mÃ©triques d'utilitÃ© pour un SUPER-GRAPHE (GÃ©nÃ©ralisation).

    Le super-graphe a une structure diffÃ©rente :
    - Chaque NÅ’UD = un CLUSTER (super-nÅ“ud)
    - Attributs des nÅ“uds : 'cluster_size', 'internal_edges'
    - ArÃªtes INTRA-cluster : self-loops avec poids = nb d'arÃªtes internes
    - ArÃªtes INTER-cluster : arÃªtes normales avec poids = nb d'arÃªtes entre clusters

    On calcule les mÃ©triques directement Ã  partir de ces informations.
    """
    metrics = {
        'type': 'super-graph',
        'comparable': True,
        'is_super_graph': True
    }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TRIQUES DE BASE (Structure du Super-Graphe)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    num_clusters = G_super.number_of_nodes()
    metrics['num_clusters'] = num_clusters

    # Extraire les tailles de clusters
    cluster_sizes = []
    total_internal_edges = 0

    for node in G_super.nodes():
        node_data = G_super.nodes[node]
        size = node_data.get('cluster_size', 0)
        internal = node_data.get('internal_edges', 0)

        cluster_sizes.append(size)
        total_internal_edges += internal

    metrics['num_nodes'] = sum(cluster_sizes)  # Total de nÅ“uds originaux
    metrics['min_cluster_size'] = min(cluster_sizes) if cluster_sizes else 0
    metrics['max_cluster_size'] = max(cluster_sizes) if cluster_sizes else 0
    metrics['avg_cluster_size'] = np.mean(cluster_sizes) if cluster_sizes else 0
    metrics['cluster_size_variance'] = np.var(cluster_sizes) if cluster_sizes else 0

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TRIQUES D'ARÃŠTES (Intra vs Inter)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # ArÃªtes intra-cluster (depuis les self-loops ou attributs de nÅ“uds)
    metrics['intra_cluster_edges'] = total_internal_edges

    # ArÃªtes inter-cluster (depuis les arÃªtes entre clusters)
    inter_cluster_edges = 0
    for u, v in G_super.edges():
        if u != v:  # Exclure les self-loops
            weight = G_super[u][v].get('weight', 1)
            inter_cluster_edges += weight

    metrics['inter_cluster_edges'] = inter_cluster_edges
    metrics['num_edges'] = total_internal_edges + inter_cluster_edges

    # Ratio intra/total
    total_edges = metrics['num_edges']
    if total_edges > 0:
        metrics['intra_ratio'] = total_internal_edges / total_edges
        metrics['inter_ratio'] = inter_cluster_edges / total_edges
    else:
        metrics['intra_ratio'] = 0
        metrics['inter_ratio'] = 0

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PERTE D'INFORMATION (Information Loss)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Comparer avec le graphe original
    orig_edges = G_orig.number_of_edges()
    orig_nodes = G_orig.number_of_nodes()

    # Perte de granularitÃ© : passage de n nÅ“uds Ã  k clusters
    metrics['node_compression_ratio'] = num_clusters / orig_nodes if orig_nodes > 0 else 0
    metrics['information_loss'] = 1 - metrics['node_compression_ratio']

    # Conservation des arÃªtes
    if orig_edges > 0:
        metrics['edge_preservation_ratio'] = total_edges / orig_edges
    else:
        metrics['edge_preservation_ratio'] = 0

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TRIQUES STRUCTURELLES (sur le super-graphe lui-mÃªme)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # DensitÃ© du super-graphe (sans compter les self-loops)
    super_graph_no_loops = G_super.copy()
    super_graph_no_loops.remove_edges_from(nx.selfloop_edges(super_graph_no_loops))
    metrics['super_graph_density'] = nx.density(super_graph_no_loops)

    # DegrÃ© moyen des clusters (nb de clusters voisins)
    super_degrees = [d for n, d in super_graph_no_loops.degree()]
    metrics['avg_cluster_degree'] = np.mean(super_degrees) if super_degrees else 0
    metrics['max_cluster_degree'] = max(super_degrees) if super_degrees else 0

    # ConnectivitÃ© du super-graphe
    metrics['super_graph_connected'] = nx.is_connected(super_graph_no_loops)

    if metrics['super_graph_connected']:
        try:
            metrics['super_graph_diameter'] = nx.diameter(super_graph_no_loops)
        except:
            metrics['super_graph_diameter'] = None
    else:
        metrics['super_graph_diameter'] = None

    return metrics


def calculate_utility_metrics(G_orig, G_anon):
    """
    Calcule les mÃ©triques d'utilitÃ© selon la thÃ¨se (Section 3.5.2).

    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    MÃ‰TRIQUES D'UTILITÃ‰ (selon la thÃ¨se, lignes 2503-2636) :
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    3 GROUPES DE STATISTIQUES :

    1. DEGREE-BASED (statistiques basÃ©es sur les degrÃ©s) :
       - Nombre d'arÃªtes (S_NE)
       - DegrÃ© moyen (S_AD)
       - DegrÃ© maximal (S_MD)
       - Variance des degrÃ©s (S_DV)
       - Exposant power-law (S_PL)

    2. SHORTEST PATH-BASED (statistiques basÃ©es sur les chemins) :
       - Distance moyenne (S_APD)
       - DiamÃ¨tre effectif - 90e percentile (S_EDiam)
       - Longueur de connectivitÃ© - moyenne harmonique (S_CL)
       - DiamÃ¨tre (S_Diam)

    3. CLUSTERING :
       - Coefficient de clustering (S_CC) = 3 Ã— triangles / triples connectÃ©s

    CAS SPÃ‰CIAUX :
    - Graphes PROBABILISTES : Calculer sur Ã‰CHANTILLONS (sample graphs)
    - GÃ‰NÃ‰RALISATION (super-graphe) : MÃ©triques adaptÃ©es au format cluster
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    metrics = {}

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAS 1 : SUPER-GRAPHE (GÃ©nÃ©ralisation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if not isinstance(G_anon, nx.Graph):
        # Pour la gÃ©nÃ©ralisation, on ne peut pas comparer
        return {'type': 'super-graph', 'comparable': False}

    # VÃ©rifier si c'est un super-graphe (a des attributs cluster)
    is_super_graph = False
    if G_anon.number_of_nodes() > 0:
        first_node = list(G_anon.nodes())[0]
        node_data = G_anon.nodes[first_node]
        is_super_graph = 'cluster_size' in node_data

    if is_super_graph:
        # MÃ‰TRIQUES SPÃ‰CIFIQUES POUR LE SUPER-GRAPHE
        return calculate_supergraph_metrics(G_orig, G_anon)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CAS 2 : GRAPHE PROBABILISTE â†’ Ã‰chantillonner d'abord
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    is_probabilistic = False
    if G_anon.number_of_edges() > 0:
        first_edge = list(G_anon.edges())[0]
        is_probabilistic = 'probability' in G_anon[first_edge[0]][first_edge[1]]

    if is_probabilistic:
        # GÃ©nÃ©rer un Ã©chantillon dÃ©terministe depuis le graphe probabiliste
        G_sample = sample_from_probabilistic_graph(G_anon)
        metrics['is_sample'] = True
        metrics['probabilistic_edges'] = G_anon.number_of_edges()
    else:
        G_sample = G_anon
        metrics['is_sample'] = False

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GROUPE 1 : DEGREE-BASED STATISTICS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # S_NE : Nombre d'arÃªtes
    metrics['num_edges'] = G_sample.number_of_edges()
    metrics['num_nodes'] = G_sample.number_of_nodes()

    # Calculer les degrÃ©s
    degrees = [d for n, d in G_sample.degree()]

    # S_AD : DegrÃ© moyen
    metrics['avg_degree'] = np.mean(degrees) if degrees else 0

    # S_MD : DegrÃ© maximal
    metrics['max_degree'] = max(degrees) if degrees else 0

    # S_DV : Variance des degrÃ©s
    metrics['degree_variance'] = np.var(degrees) if degrees else 0

    # S_PL : Exposant power-law
    # On estime Î³ (gamma) de la distribution P(k) âˆ k^(-Î³)
    try:
        from scipy.stats import linregress
        # Compter la distribution des degrÃ©s
        degree_counts = Counter(degrees)
        degrees_unique = sorted([d for d in degree_counts.keys() if d > 0])
        counts = [degree_counts[d] for d in degrees_unique]

        if len(degrees_unique) >= 3:
            # RÃ©gression log-log : log(P(k)) = -Î³ Ã— log(k) + C
            log_degrees = np.log(degrees_unique)
            log_counts = np.log(counts)
            slope, intercept, r_value, p_value, std_err = linregress(log_degrees, log_counts)
            metrics['power_law_exponent'] = -slope  # Î³ = -slope
            metrics['power_law_r_squared'] = r_value ** 2
        else:
            metrics['power_law_exponent'] = None
            metrics['power_law_r_squared'] = None
    except:
        metrics['power_law_exponent'] = None
        metrics['power_law_r_squared'] = None

    # DensitÃ© (mÃ©trique additionnelle)
    metrics['density'] = nx.density(G_sample)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GROUPE 2 : SHORTEST PATH-BASED STATISTICS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    try:
        if nx.is_connected(G_sample):
            # S_Diam : DiamÃ¨tre
            metrics['diameter'] = nx.diameter(G_sample)

            # S_APD : Distance moyenne
            metrics['avg_shortest_path'] = nx.average_shortest_path_length(G_sample)

            # S_EDiam : DiamÃ¨tre effectif (90e percentile)
            # Calculer toutes les distances
            all_paths = dict(nx.all_pairs_shortest_path_length(G_sample))
            all_distances = []
            for source in all_paths:
                for target, dist in all_paths[source].items():
                    if source != target:
                        all_distances.append(dist)

            if all_distances:
                metrics['effective_diameter'] = np.percentile(all_distances, 90)

                # S_CL : Longueur de connectivitÃ© (moyenne harmonique)
                # CL = n(n-1) / Î£(1/d(u,v))
                harmonic_sum = sum([1.0/d for d in all_distances if d > 0])
                n = G_sample.number_of_nodes()
                metrics['connectivity_length'] = n * (n-1) / harmonic_sum if harmonic_sum > 0 else None
            else:
                metrics['effective_diameter'] = None
                metrics['connectivity_length'] = None
        else:
            # Prendre la plus grande composante connexe
            largest_cc = max(nx.connected_components(G_sample), key=len)
            subgraph = G_sample.subgraph(largest_cc)

            metrics['diameter'] = nx.diameter(subgraph)
            metrics['avg_shortest_path'] = nx.average_shortest_path_length(subgraph)

            # Pour effective diameter et connectivity length sur la LCC
            all_paths = dict(nx.all_pairs_shortest_path_length(subgraph))
            all_distances = []
            for source in all_paths:
                for target, dist in all_paths[source].items():
                    if source != target:
                        all_distances.append(dist)

            if all_distances:
                metrics['effective_diameter'] = np.percentile(all_distances, 90)
                harmonic_sum = sum([1.0/d for d in all_distances if d > 0])
                n_lcc = subgraph.number_of_nodes()
                metrics['connectivity_length'] = n_lcc * (n_lcc-1) / harmonic_sum if harmonic_sum > 0 else None
            else:
                metrics['effective_diameter'] = None
                metrics['connectivity_length'] = None
    except:
        metrics['diameter'] = None
        metrics['avg_shortest_path'] = None
        metrics['effective_diameter'] = None
        metrics['connectivity_length'] = None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GROUPE 3 : CLUSTERING COEFFICIENT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # S_CC : Clustering coefficient = 3 Ã— triangles / triples connectÃ©s
    try:
        # Compter les triangles
        triangles = sum(nx.triangles(G_sample).values()) / 3  # DivisÃ© par 3 car chaque triangle comptÃ© 3 fois

        # Compter les triples connectÃ©s (chemins de longueur 2)
        connected_triples = 0
        for node in G_sample.nodes():
            degree = G_sample.degree(node)
            # Chaque nÅ“ud de degrÃ© k crÃ©e k(k-1)/2 triples
            connected_triples += degree * (degree - 1) / 2

        if connected_triples > 0:
            metrics['clustering_coefficient'] = (3 * triangles) / connected_triples
        else:
            metrics['clustering_coefficient'] = 0

        # Clustering moyen (mÃ©trique alternative)
        metrics['avg_clustering'] = nx.average_clustering(G_sample)
    except:
        metrics['clustering_coefficient'] = None
        metrics['avg_clustering'] = None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TRIQUES ADDITIONNELLES : PrÃ©servation de la structure
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # CorrÃ©lation des sÃ©quences de degrÃ©s (Spearman)
    orig_degrees = sorted([d for n, d in G_orig.degree()])
    sample_degrees = sorted([d for n, d in G_sample.degree()])

    if len(orig_degrees) == len(sample_degrees):
        from scipy.stats import spearmanr
        try:
            corr, _ = spearmanr(orig_degrees, sample_degrees)
            metrics['degree_correlation'] = corr
        except:
            metrics['degree_correlation'] = None
    else:
        metrics['degree_correlation'] = None

    # Erreur relative moyenne (comme dans la thÃ¨se)
    # rel.err = |S(G0) - S(G)| / S(G0)
    metrics['comparable'] = True

    return metrics


def calculate_privacy_metrics_separated(G_orig, G_anon, method_key, method_params):
    """Calcule les mÃ©triques de privacy sÃ©parÃ©es"""
    metrics = {}

    if method_key == "k-degree anonymity":
        degrees = dict(G_anon.degree()) if isinstance(G_anon, nx.Graph) else {}
        degree_counts = Counter(degrees.values()) if degrees else Counter()
        k_value = method_params.get('k', 2)
        min_count = min(degree_counts.values()) if degree_counts else 0

        metrics['k_value'] = k_value
        metrics['min_anonymity_set'] = min_count
        metrics['satisfies_k_anonymity'] = min_count >= k_value
        metrics['re_identification_probability'] = 1/min_count if min_count > 0 else 1.0

    elif method_key in ["EdgeFlip", "Laplace"]:
        epsilon = method_params.get('epsilon', 1.0)
        metrics['epsilon_budget'] = epsilon
        metrics['privacy_loss_bound'] = np.exp(epsilon)
        metrics['privacy_level'] = "Forte (Îµ<1)" if epsilon < 1.0 else ("Moyenne (1â‰¤Îµ<2)" if epsilon < 2.0 else "Faible (Îµâ‰¥2)")

        if method_key == "EdgeFlip":
            s = 1 - np.exp(-epsilon)
            metrics['flip_probability'] = s
            metrics['expected_noise_edges'] = int(G_orig.number_of_edges() * s / 2)

    elif method_key == "Probabilistic":
        k = method_params.get('k', 5)
        eps = method_params.get('epsilon', 0.3)
        metrics['k_candidates'] = k
        metrics['epsilon_tolerance'] = eps
        metrics['min_entropy'] = np.log(k) - eps
        metrics['confusion_factor'] = k

    elif method_key == "MaxVar":
        num_pot = method_params.get('num_potential_edges', 50)
        metrics['num_potential_edges'] = num_pot

        # Analyser les probabilitÃ©s pour vÃ©rifier la dispersion
        if G_anon.number_of_edges() > 0:
            # SÃ©parer arÃªtes existantes vs potentielles
            existing_probs = []
            potential_probs = []
            all_probs = []

            for u, v in G_anon.edges():
                prob = G_anon[u][v].get('probability', 1.0)
                is_original = G_anon[u][v].get('is_original', False)
                all_probs.append(prob)

                if is_original:
                    existing_probs.append(prob)
                else:
                    potential_probs.append(prob)

            # MÃ©triques globales
            metrics['avg_probability'] = np.mean(all_probs)
            metrics['std_probability'] = np.std(all_probs)
            metrics['min_probability'] = np.min(all_probs)
            metrics['max_probability'] = np.max(all_probs)

            # MÃ©triques pour arÃªtes existantes
            if existing_probs:
                metrics['existing_avg_prob'] = np.mean(existing_probs)
                metrics['existing_std_prob'] = np.std(existing_probs)

            # MÃ©triques pour arÃªtes potentielles
            if potential_probs:
                metrics['potential_avg_prob'] = np.mean(potential_probs)
                metrics['potential_std_prob'] = np.std(potential_probs)

            # Calculer la variance totale (objectif maximisÃ©)
            total_variance = sum(p * (1 - p) for p in all_probs)
            metrics['total_variance'] = total_variance
            metrics['avg_edge_variance'] = total_variance / len(all_probs) if all_probs else 0

            # Tester la rÃ©sistance au seuillage
            threshold = 0.5
            reconstructed = sum(1 for p in all_probs if p > threshold)
            original_edges_count = len(existing_probs)
            if original_edges_count > 0:
                reconstruction_rate = sum(1 for p in existing_probs if p > threshold) / original_edges_count
                metrics['threshold_resistance'] = 1 - reconstruction_rate  # Plus proche de 1 = meilleur
                metrics['reconstruction_rate'] = reconstruction_rate

    elif method_key == "Generalization":
        if hasattr(G_anon, 'graph') and 'cluster_to_nodes' in G_anon.graph:
            cluster_sizes = [len(nodes) for nodes in G_anon.graph['cluster_to_nodes'].values()]
            metrics['min_cluster_size'] = min(cluster_sizes) if cluster_sizes else 0
            metrics['avg_cluster_size'] = np.mean(cluster_sizes) if cluster_sizes else 0
            metrics['max_privacy'] = 1/min(cluster_sizes) if cluster_sizes else 1.0

    return metrics


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DICTIONNAIRE DES DÃ‰FINITIONS ET MÃ‰THODES DE CALCUL (pour tooltips)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

METRIC_DEFINITIONS = {
    # MÃ©triques de base
    'num_nodes': {
        'name': 'Nombre de NÅ“uds',
        'definition': "Nombre total de nÅ“uds dans le graphe",
        'formula': "n = |V|",
        'interpretation': "Plus Ã©levÃ© = graphe plus grand"
    },
    'num_edges': {
        'name': 'Nombre d\'ArÃªtes',
        'definition': "Nombre total d'arÃªtes dans le graphe",
        'formula': "m = |E|",
        'interpretation': "Plus Ã©levÃ© = graphe plus connectÃ©"
    },
    'density': {
        'name': 'DensitÃ©',
        'definition': "Proportion d'arÃªtes existantes par rapport au maximum possible",
        'formula': "D = 2m / (n(n-1))",
        'interpretation': "0 = vide, 1 = complet, ~0.1 = Ã©pars, ~0.5 = dense"
    },

    # Groupe 1: Degree-based
    'avg_degree': {
        'name': 'DegrÃ© Moyen (S_AD)',
        'definition': "Nombre moyen de voisins par nÅ“ud",
        'formula': r"d_{avg} = \frac{1}{n} \sum_{v \in V} \deg(v)",
        'interpretation': "Mesure la connectivitÃ© moyenne du graphe"
    },
    'max_degree': {
        'name': 'DegrÃ© Maximal (S_MD)',
        'definition': "Plus grand nombre de voisins d'un nÅ“ud",
        'formula': r"d_{max} = \max_{v \in V} \deg(v)",
        'interpretation': "Identifie les hubs (nÅ“uds trÃ¨s connectÃ©s)"
    },
    'degree_variance': {
        'name': 'Variance des DegrÃ©s (S_DV)',
        'definition': "Dispersion des degrÃ©s autour de la moyenne",
        'formula': r"\sigma^2 = \frac{1}{n} \sum_{v \in V} (\deg(v) - d_{avg})^2",
        'interpretation': "Ã‰levÃ©e = hÃ©tÃ©rogÃ©nÃ©itÃ© (hubs + nÅ“uds peu connectÃ©s)"
    },
    'power_law_exponent': {
        'name': 'Exposant Power-Law (S_PL)',
        'definition': "CaractÃ©rise la distribution des degrÃ©s pour les rÃ©seaux scale-free",
        'formula': r"P(k) \propto k^{-\gamma} \text{ oÃ¹ } \gamma \text{ est estimÃ© par rÃ©gression log-log}",
        'interpretation': r"\gamma \in [2,3] \text{ typique pour rÃ©seaux sociaux (loi de puissance)}"
    },

    # Groupe 2: Shortest path-based
    'diameter': {
        'name': 'DiamÃ¨tre (S_Diam)',
        'definition': "Plus grande distance entre deux nÅ“uds connectÃ©s",
        'formula': r"D = \max_{u,v \in V} d(u,v)",
        'interpretation': "Borne supÃ©rieure sur toutes les distances"
    },
    'avg_shortest_path': {
        'name': 'Distance Moyenne (S_APD)',
        'definition': "Longueur moyenne des plus courts chemins entre tous les couples",
        'formula': r"L = \frac{2}{n(n-1)} \sum_{u,v \in V} d(u,v)",
        'interpretation': "Mesure la compacitÃ© du rÃ©seau (propriÃ©tÃ© small-world)"
    },
    'effective_diameter': {
        'name': 'DiamÃ¨tre Effectif (S_EDiam)',
        'definition': "90e percentile des distances (plus robuste que le diamÃ¨tre)",
        'formula': r"D_{eff} = \text{Percentile}_{90}\{d(u,v)\}",
        'interpretation': r"90\% \text{ des nÅ“uds sont Ã  distance } \leq D_{eff}"
    },
    'connectivity_length': {
        'name': 'Longueur de ConnectivitÃ© (S_CL)',
        'definition': "Moyenne harmonique des distances (privilÃ©gie les courtes distances)",
        'formula': r"CL = \frac{n(n-1)}{\sum_{u,v} \frac{1}{d(u,v)}}",
        'interpretation': "Plus faible = meilleure connectivitÃ© locale"
    },

    # Groupe 3: Clustering
    'clustering_coefficient': {
        'name': 'Coefficient de Clustering (S_CC)',
        'definition': "Mesure la tendance Ã  former des triangles (cliques locales)",
        'formula': "CC = (3 Ã— nb_triangles) / nb_triples_connectÃ©s",
        'interpretation': "Ã‰levÃ© = forte transitivitÃ© (ami de mes amis = mon ami)"
    },
    'avg_clustering': {
        'name': 'Clustering Moyen',
        'definition': "Moyenne des coefficients de clustering locaux",
        'formula': r"C_{avg} = \frac{1}{n} \sum_{v \in V} C(v) \text{ oÃ¹ } C(v) = \frac{\text{triangles}(v)}{\text{triples}(v)}",
        'interpretation': "Mesure alternative du clustering (locale â†’ globale)"
    },

    # MÃ©triques de prÃ©servation
    'degree_correlation': {
        'name': 'CorrÃ©lation des DegrÃ©s',
        'definition': "CorrÃ©lation de Spearman entre sÃ©quences de degrÃ©s (original vs anonymisÃ©)",
        'formula': "Ï = Spearman(deg_orig, deg_anon)",
        'interpretation': ">0.9 = excellente prÃ©servation, >0.7 = bonne, <0.7 = faible"
    },

    # MÃ©triques pour super-graphe (gÃ©nÃ©ralisation)
    'num_clusters': {
        'name': 'Nombre de Clusters',
        'definition': "Nombre de super-nÅ“uds dans le graphe de gÃ©nÃ©ralisation",
        'formula': "k = nombre de clusters",
        'interpretation': "Plus faible = plus de privacy, moins d'utilitÃ©"
    },
    'min_cluster_size': {
        'name': 'Taille Min. Cluster',
        'definition': "Plus petit nombre de nÅ“uds dans un cluster",
        'formula': r"\min_i |C_i|",
        'interpretation': r"\text{Doit Ãªtre } \geq k \text{ pour garantir k-anonymity}"
    },
    'avg_cluster_size': {
        'name': 'Taille Moy. Cluster',
        'definition': "Nombre moyen de nÅ“uds par cluster",
        'formula': r"\text{avg}\{|C_i|\} = \frac{n}{k}",
        'interpretation': "Plus Ã©levÃ© = clusters plus gros = plus de privacy"
    },
    'intra_cluster_edges': {
        'name': 'ArÃªtes Intra-Cluster',
        'definition': "Nombre d'arÃªtes Ã  l'intÃ©rieur des clusters",
        'formula': "Somme des arÃªtes internes de chaque cluster",
        'interpretation': "ReprÃ©sentent la structure locale prÃ©servÃ©e"
    },
    'inter_cluster_edges': {
        'name': 'ArÃªtes Inter-Cluster',
        'definition': "Nombre d'arÃªtes entre diffÃ©rents clusters",
        'formula': "ArÃªtes reliant des nÅ“uds de clusters diffÃ©rents",
        'interpretation': "ReprÃ©sentent les connexions globales"
    },
    'intra_ratio': {
        'name': 'Ratio Intra/Total',
        'definition': "Proportion d'arÃªtes intra-cluster par rapport au total",
        'formula': "ratio = intra_edges / (intra_edges + inter_edges)",
        'interpretation': "Ã‰levÃ© = structure locale bien prÃ©servÃ©e"
    },
    'information_loss': {
        'name': 'Perte d\'Information',
        'definition': "Proportion de granularitÃ© perdue lors du clustering",
        'formula': "loss = 1 - (k_clusters / n_nodes)",
        'interpretation': "0 = aucune perte, 1 = perte totale (1 seul cluster)"
    },
    'edge_preservation_ratio': {
        'name': 'Taux de PrÃ©servation des ArÃªtes',
        'definition': "Proportion d'arÃªtes prÃ©servÃ©es aprÃ¨s anonymisation",
        'formula': "ratio = edges_anon / edges_orig",
        'interpretation': "1 = toutes prÃ©servÃ©es, <1 = pertes, >1 = arÃªtes ajoutÃ©es"
    },
    'super_graph_density': {
        'name': 'DensitÃ© du Super-Graphe',
        'definition': "DensitÃ© du graphe des clusters (sans self-loops)",
        'formula': "D_super = 2m_inter / (k(k-1))",
        'interpretation': "Mesure la connectivitÃ© entre clusters"
    },
}


def get_metric_tooltip(metric_key):
    """
    GÃ©nÃ¨re un tooltip formatÃ© pour une mÃ©trique donnÃ©e.

    Args:
        metric_key: ClÃ© de la mÃ©trique dans METRIC_DEFINITIONS

    Returns:
        String formatÃ© pour le paramÃ¨tre 'help' de st.metric()
    """
    if metric_key not in METRIC_DEFINITIONS:
        return None

    info = METRIC_DEFINITIONS[metric_key]

    tooltip = (
        f"ğŸ“– **DÃ©finition**: {info['definition']}\n\n"
        f"ğŸ“ **Formule**: {info['formula']}\n\n"
        f"ğŸ’¡ **InterprÃ©tation**: {info['interpretation']}"
    )

    return tooltip


def main():
    """Application principale Streamlit"""

    # En-tÃªte
    st.markdown('<p class="main-header">ğŸ”’ Anonymisation de Graphes Sociaux</p>',
                unsafe_allow_html=True)
    st.markdown("---")

    st.markdown("""
    ### Application Interactive basÃ©e sur la thÃ¨se de NGUYEN Huu-Hiep (2016)

    Cette application dÃ©montre les **5 types de mÃ©thodes d'anonymisation** de graphes sociaux
    avec explications mathÃ©matiques dÃ©taillÃ©es et mÃ©triques d'anonymisation.
    """)

    # Sidebar - SÃ©lection de la mÃ©thode
    st.sidebar.title("âš™ï¸ Configuration")

    st.sidebar.markdown("### ğŸ“Š Graphe de Test")
    graph_choice = st.sidebar.selectbox(
        "Choisir un graphe",
        ["Karate Club (34 nÅ“uds)", "Graphe AlÃ©atoire Petit (20 nÅ“uds)",
         "Graphe AlÃ©atoire Moyen (50 nÅ“uds)"]
    )

    # Charger le graphe
    if "Karate" in graph_choice:
        G = nx.karate_club_graph()
        st.sidebar.success(f"âœ“ Graphe Karate Club chargÃ©: {G.number_of_nodes()} nÅ“uds, {G.number_of_edges()} arÃªtes")
    elif "Petit" in graph_choice:
        G = nx.erdos_renyi_graph(20, 0.15, seed=42)
        st.sidebar.success(f"âœ“ Graphe alÃ©atoire chargÃ©: {G.number_of_nodes()} nÅ“uds, {G.number_of_edges()} arÃªtes")
    else:
        G = nx.erdos_renyi_graph(50, 0.1, seed=42)
        st.sidebar.success(f"âœ“ Graphe alÃ©atoire chargÃ©: {G.number_of_nodes()} nÅ“uds, {G.number_of_edges()} arÃªtes")

    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ”¬ MÃ©thode d'Anonymisation")

    method_key = st.sidebar.selectbox(
        "Choisir une mÃ©thode",
        list(METHODS.keys()),
        format_func=lambda x: METHODS[x]["name"]
    )

    method = METHODS[method_key]

    st.sidebar.markdown(f"**CatÃ©gorie** : {method['category']}")
    st.sidebar.markdown(f"**Description** : {method['description_short']}")

    # Section de paramÃ¨tres modulables
    st.sidebar.markdown("---")
    st.sidebar.markdown("### âš™ï¸ Budget de Privacy (Modulable)")

    # ParamÃ¨tres dynamiques selon la mÃ©thode
    dynamic_params = {}

    if method_key in ["Random Add/Del", "Random Switch"]:
        k_value = st.sidebar.slider(
            "k = Nombre de modifications",
            min_value=5,
            max_value=50,
            value=method['params']['k'],
            step=5,
            help="Nombre d'arÃªtes Ã  modifier (ajout/suppression ou Ã©change)"
        )
        dynamic_params['k'] = k_value

    elif method_key == "k-degree anonymity":
        k_value = st.sidebar.slider(
            "k = Taille minimale des groupes",
            min_value=2,
            max_value=10,
            value=method['params']['k'],
            step=1,
            help="Nombre minimum de nÅ“uds ayant le mÃªme degrÃ©"
        )
        dynamic_params['k'] = k_value

    elif method_key == "Generalization":
        k_value = st.sidebar.slider(
            "k = Taille minimale des clusters",
            min_value=2,
            max_value=10,
            value=method['params']['k'],
            step=1,
            help="Nombre minimum de nÅ“uds dans chaque cluster"
        )
        dynamic_params['k'] = k_value

    elif method_key == "Probabilistic":
        k_value = st.sidebar.slider(
            "k = Nombre de graphes candidats",
            min_value=3,
            max_value=15,
            value=method['params']['k'],
            step=1,
            help="Nombre minimum de graphes plausibles"
        )
        epsilon_value = st.sidebar.slider(
            "Îµ = Taux de transfert de probabilitÃ©",
            min_value=0.1,
            max_value=10.0,
            value=method['params']['epsilon'],
            step=0.1,
            help="âš ï¸ ATTENTION : Plus Îµ est GRAND, plus on transfÃ¨re de probabilitÃ© des arÃªtes existantes vers les potentielles â†’ PLUS DE PRIVACY ! Avec Îµ grand (ex: 5.0), p_exist diminue et p_potential augmente, rendant le graphe plus anonymisÃ©."
        )
        dynamic_params['k'] = k_value
        dynamic_params['epsilon'] = epsilon_value

    elif method_key == "MaxVar":
        num_pot_edges = st.sidebar.slider(
            "Nombre d'arÃªtes potentielles",
            min_value=20,
            max_value=100,
            value=method['params']['num_potential_edges'],
            step=10,
            help="Nombre d'arÃªtes potentielles (nearby, distance=2) Ã  ajouter avant l'optimisation. Plus ce nombre est Ã©levÃ©, plus la variance peut Ãªtre maximisÃ©e, mais le calcul est plus long."
        )
        dynamic_params['num_potential_edges'] = num_pot_edges

    elif method_key in ["EdgeFlip", "Laplace"]:
        epsilon_value = st.sidebar.slider(
            "Îµ = Budget de Privacy",
            min_value=0.1,
            max_value=3.0,
            value=method['params']['epsilon'],
            step=0.1,
            help="""ğŸ“– Budget de Privacy DiffÃ©rentielle (Îµ-DP)

FORMULE CORRECTE : s = 2/(e^Îµ + 1), flip_probability = s/2

Trade-off Privacy-UtilitÃ© :
â€¢ Îµ PETIT = FORTE privacy (beaucoup de modifications)
â€¢ Îµ GRAND = FAIBLE privacy (peu de modifications)

Exemples concrets :
â€¢ Îµ = 0.1 (petit): flip_prob = 47.5% â†’ graphe trÃ¨s diffÃ©rent â†’ FORTE privacy âœ“
â€¢ Îµ = 1.0 (moyen): flip_prob = 26.9% â†’ changements modÃ©rÃ©s â†’ privacy moyenne
â€¢ Îµ = 3.0 (grand): flip_prob = 4.7% â†’ graphe proche â†’ FAIBLE privacy

En DP, epsilon mesure la "perte de privacy" : plus c'est petit, mieux c'est !"""
        )
        dynamic_params['epsilon'] = epsilon_value

        # Afficher l'impact du budget AVEC LA FORMULE CORRECTE
        privacy_loss = np.exp(epsilon_value)
        s = 2 / (np.exp(epsilon_value) + 1)  # FORMULE CORRECTE
        flip_prob = s / 2

        if epsilon_value < 1.0:
            st.sidebar.success(f"âœ… Privacy Forte (Îµ={epsilon_value:.1f})")
            st.sidebar.caption(f"Flip: {flip_prob*100:.1f}% | Graphe trÃ¨s modifiÃ©")
        elif epsilon_value < 2.0:
            st.sidebar.warning(f"âš ï¸ Privacy Moyenne (Îµ={epsilon_value:.1f})")
            st.sidebar.caption(f"Flip: {flip_prob*100:.1f}% | Modifications modÃ©rÃ©es")
        else:
            st.sidebar.error(f"âŒ Privacy Faible (Îµ={epsilon_value:.1f})")
            st.sidebar.caption(f"Flip: {flip_prob*100:.1f}% | Graphe proche de l'original")

    # Bouton pour anonymiser
    st.sidebar.markdown("---")
    if st.sidebar.button("ğŸš€ Anonymiser le Graphe", type="primary"):
        st.session_state.anonymized = True
        st.session_state.method_key = method_key
        st.session_state.method_params = dynamic_params  # Sauvegarder les paramÃ¨tres utilisÃ©s
        st.session_state.show_sample = False  # RÃ©initialiser l'affichage d'Ã©chantillon

        # Anonymiser
        anonymizer = GraphAnonymizer(G)

        with st.spinner('Anonymisation en cours...'):
            node_to_cluster = None
            if method_key == "Random Add/Del":
                G_anon = anonymizer.random_add_del(**dynamic_params)
            elif method_key == "Random Switch":
                G_anon = anonymizer.random_switch(**dynamic_params)
            elif method_key == "k-degree anonymity":
                G_anon = anonymizer.k_degree_anonymity(**dynamic_params)
            elif method_key == "Generalization":
                G_anon, node_to_cluster = anonymizer.generalization(**dynamic_params)
                st.session_state.node_to_cluster = node_to_cluster
            elif method_key == "Probabilistic":
                G_anon = anonymizer.probabilistic_obfuscation(**dynamic_params)
            elif method_key == "MaxVar":
                G_anon = anonymizer.maxvar_obfuscation(**dynamic_params)
            elif method_key == "EdgeFlip":
                G_anon = anonymizer.differential_privacy_edgeflip(**dynamic_params)
            elif method_key == "Laplace":
                G_anon = anonymizer.differential_privacy_laplace(**dynamic_params)

            st.session_state.G_anon = G_anon
            st.session_state.G_orig = G
            if node_to_cluster is None:
                st.session_state.node_to_cluster = None

    # Bouton pour tirer un Ã©chantillon (seulement pour graphes probabilistes)
    if 'anonymized' in st.session_state and st.session_state.anonymized:
        if st.session_state.method_key in ["Probabilistic", "MaxVar"]:
            st.sidebar.markdown("---")
            st.sidebar.markdown("### ğŸ² Ã‰chantillonnage")
            st.sidebar.caption("Les mÃ©thodes probabilistes publient un graphe incertain. Tirez un Ã©chantillon dÃ©terministe!")

            if st.sidebar.button("ğŸ² Tirer un Ã‰chantillon", type="secondary"):
                with st.spinner('Tirage d\'Ã©chantillon en cours...'):
                    G_sample = sample_from_probabilistic_graph(st.session_state.G_anon)
                    st.session_state.G_sample = G_sample
                    st.session_state.show_sample = True

            if st.session_state.get('show_sample', False):
                st.sidebar.success("âœ… Ã‰chantillon tirÃ©!")
                st.sidebar.caption(f"NÅ“uds: {st.session_state.G_sample.number_of_nodes()}, ArÃªtes: {st.session_state.G_sample.number_of_edges()}")

                if st.sidebar.button("ğŸ”„ Afficher graphe incertain", type="secondary"):
                    st.session_state.show_sample = False

    # Affichage des rÃ©sultats
    if 'anonymized' in st.session_state and st.session_state.anonymized:
        G_orig = st.session_state.G_orig
        G_anon = st.session_state.G_anon

        # Utiliser l'Ã©chantillon si disponible pour l'affichage
        G_display = st.session_state.get('G_sample', G_anon) if st.session_state.get('show_sample', False) else G_anon

        current_method = METHODS[st.session_state.method_key]

        # Onglets - VERSION AMÃ‰LIORÃ‰E avec 8 onglets
        tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
            "ğŸ“Š RÃ©sultats",
            "ğŸ“– DÃ©finitions",
            "ğŸ“ˆ MÃ©triques UtilitÃ©",
            "ğŸ”’ MÃ©triques Privacy",
            "ğŸ¯ Simulations d'Attaques",
            "ğŸ›¡ï¸ Attaques & Garanties",
            "ğŸ“š Dict. Attaques",
            "ğŸ” Dict. PropriÃ©tÃ©s"
        ])

        with tab1:
            st.markdown("## ğŸ“Š RÃ©sultats de l'Anonymisation")

            # Indicateur si on affiche un Ã©chantillon
            if st.session_state.get('show_sample', False):
                st.info("ğŸ² **Affichage d'un graphe Ã©chantillon** tirÃ© depuis le graphe incertain. Les probabilitÃ©s ont Ã©tÃ© converties en arÃªtes dÃ©terministes.")

            col1, col2 = st.columns(2)

            with col1:
                st.metric("NÅ“uds Originaux", G_orig.number_of_nodes())
                st.metric("ArÃªtes Originales", G_orig.number_of_edges())

            with col2:
                if isinstance(G_display, nx.Graph):
                    label = "Ã‰chantillon" if st.session_state.get('show_sample', False) else "AnonymisÃ©s"
                    st.metric(f"NÅ“uds {label}", G_display.number_of_nodes())
                    st.metric(f"ArÃªtes {label}", G_display.number_of_edges(),
                             delta=f"{G_display.number_of_edges() - G_orig.number_of_edges():+d}")
                else:
                    st.info("Format de graphe non standard (super-nodes)")

            st.markdown("---")
            st.markdown("### Comparaison Visuelle")

            node_to_cluster = st.session_state.get('node_to_cluster', None)
            # Utiliser G_display pour la visualisation
            fig = plot_graph_comparison(G_orig, G_display, current_method['name'], node_to_cluster)
            st.pyplot(fig)

            # Afficher les statistiques spÃ©cifiques aux super-nodes
            if st.session_state.method_key == "Generalization" and hasattr(G_anon, 'graph'):
                st.markdown("---")
                st.markdown("### ğŸ“Š Statistiques des Super-Nodes")

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ArÃªtes Intra-Cluster", G_anon.graph.get('intra_edges', 'N/A'),
                             help="ArÃªtes Ã  l'intÃ©rieur des clusters (vert)")
                with col2:
                    st.metric("ArÃªtes Inter-Cluster", G_anon.graph.get('inter_edges', 'N/A'),
                             help="ArÃªtes entre diffÃ©rents clusters (rouge)")
                with col3:
                    total = G_anon.graph.get('intra_edges', 0) + G_anon.graph.get('inter_edges', 0)
                    ratio = G_anon.graph.get('intra_edges', 0) / total * 100 if total > 0 else 0
                    st.metric("Ratio Intra/Total", f"{ratio:.1f}%")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # SECTION SPÃ‰CIALE : TIRAGE D'Ã‰CHANTILLONS (Graphes Probabilistes)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if st.session_state.method_key == "Probabilistic" and isinstance(G_anon, nx.Graph):
                # VÃ©rifier si c'est un graphe probabiliste
                if G_anon.number_of_edges() > 0:
                    first_edge = list(G_anon.edges())[0]
                    if 'probability' in G_anon[first_edge[0]][first_edge[1]]:
                        st.markdown("---")
                        st.markdown("### ğŸ² Tirage d'Ã‰chantillons depuis le Graphe Probabiliste")

                        st.info("""
                        **ğŸ’¡ Principe du Tirage (Sampling)** :

                        Dans un graphe probabiliste (k,Îµ)-obfuscation, chaque arÃªte a une **probabilitÃ© d'existence**.
                        Au lieu de publier le graphe probabiliste directement, on peut publier des **graphes Ã©chantillons**
                        tirÃ©s au sort selon ces probabilitÃ©s.

                        - **ArÃªtes Ã  haute probabilitÃ©** (â‰ˆ 95%) : Apparaissent presque toujours
                        - **ArÃªtes Ã  faible probabilitÃ©** (â‰ˆ 10%) : Apparaissent rarement

                        Cliquez sur le bouton ci-dessous pour gÃ©nÃ©rer 3 Ã©chantillons diffÃ©rents !
                        """)

                        # Bouton pour gÃ©nÃ©rer des Ã©chantillons
                        if st.button("ğŸ² GÃ©nÃ©rer 3 Ã‰chantillons AlÃ©atoires", key="sample_btn"):
                            st.markdown("#### Ã‰chantillons GÃ©nÃ©rÃ©s :")

                            cols = st.columns(3)
                            for i, col in enumerate(cols):
                                with col:
                                    # GÃ©nÃ©rer un Ã©chantillon
                                    sampled_graph = sample_from_probabilistic_graph(G_anon)

                                    # CrÃ©er une figure pour cet Ã©chantillon
                                    fig_sample, ax_sample = plt.subplots(1, 1, figsize=(6, 6))

                                    pos = nx.spring_layout(G_orig, seed=42, k=0.5, iterations=50)

                                    # Dessiner les nÅ“uds
                                    nx.draw_networkx_nodes(sampled_graph, pos, ax=ax_sample,
                                                          node_color='lightyellow',
                                                          node_size=400, alpha=0.9,
                                                          edgecolors='orange', linewidths=2)

                                    # Dessiner les arÃªtes
                                    nx.draw_networkx_edges(sampled_graph, pos, ax=ax_sample,
                                                          edge_color='gray', width=1.5, alpha=0.6)

                                    # Labels
                                    nx.draw_networkx_labels(sampled_graph, pos, ax=ax_sample,
                                                           font_size=7, font_weight='bold')

                                    ax_sample.set_title(f'Ã‰chantillon #{i+1}\n{sampled_graph.number_of_edges()} arÃªtes',
                                                       fontsize=12, fontweight='bold')
                                    ax_sample.axis('off')

                                    plt.tight_layout()
                                    st.pyplot(fig_sample)
                                    plt.close(fig_sample)

                                    # Afficher les stats
                                    st.caption(f"**{sampled_graph.number_of_nodes()}** nÅ“uds | **{sampled_graph.number_of_edges()}** arÃªtes")

                        st.markdown("""
                        **ğŸ” Observation** : Chaque Ã©chantillon est diffÃ©rent ! C'est cette variabilitÃ© qui crÃ©e
                        de l'incertitude pour l'attaquant. Il ne peut pas savoir quel Ã©chantillon correspond au graphe original.
                        """)

            st.markdown("---")
            st.markdown("### Distribution des DegrÃ©s")

            fig_dist = plot_degree_distribution(G_orig, G_anon, current_method['name'])
            st.pyplot(fig_dist)

            # Explication de la mÃ©thode actuelle (dÃ©placÃ©e depuis tab2)
            st.markdown("---")
            st.markdown(f"### ğŸ”¬ Explication : {current_method['name']}")

            with st.expander("ğŸ“š DÃ©tails de la mÃ©thode", expanded=False):
                st.markdown(current_method['description'])
                st.markdown("**Formule** :")
                st.latex(current_method['formula'])

                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**ğŸ”’ Niveau de Privacy**")
                    st.info(current_method['privacy_level'])
                with col2:
                    st.markdown("**ğŸ“Š PrÃ©servation de l'UtilitÃ©**")
                    st.info(current_method['utility_preservation'])

        with tab2:
            st.markdown("## ğŸ“– DÃ©finitions des Concepts d'Anonymisation")

            st.markdown("""
            Cette section prÃ©sente les dÃ©finitions formelles et intuitions pour chaque type d'anonymisation.
            Choisissez un concept ci-dessous pour voir sa dÃ©finition complÃ¨te.
            """)

            st.markdown("---")

            # SÃ©lecteur de concept
            concept_keys = list(ANONYMIZATION_DEFINITIONS.keys())
            concept_names = [ANONYMIZATION_DEFINITIONS[k]['name'] for k in concept_keys]

            selected_concept_name = st.selectbox(
                "Choisir un concept Ã  explorer",
                concept_names
            )

            # Trouver la clÃ© correspondante
            selected_concept_key = concept_keys[concept_names.index(selected_concept_name)]
            concept = ANONYMIZATION_DEFINITIONS[selected_concept_key]

            st.markdown(f"### {concept['name']}")

            with st.expander("ğŸ“ DÃ©finition Formelle", expanded=True):
                st.markdown(concept['definition'])
                st.markdown("**Formule mathÃ©matique** :")
                st.latex(concept['math_formula'])

            with st.expander("ğŸ’¡ Intuition (Explication en langage naturel)", expanded=True):
                st.markdown(concept['intuition'])

            with st.expander("ğŸ”’ Garantie de Privacy"):
                st.info(f"**Garantie** : {concept['privacy_guarantee']}")

            with st.expander("âš™ï¸ Signification des ParamÃ¨tres"):
                st.markdown(concept['parameter_meaning'])

        with tab3:
            st.markdown("## ğŸ“ˆ MÃ©triques d'UtilitÃ© du Graphe")

            st.markdown("""
            Ces mÃ©triques mesurent la **prÃ©servation de l'utilitÃ©** du graphe aprÃ¨s anonymisation.
            Plus ces mÃ©triques sont proches du graphe original, mieux l'utilitÃ© est prÃ©servÃ©e.

            ğŸ’¡ **Astuce** : Passez votre souris sur le â„¹ï¸ Ã  cÃ´tÃ© de chaque mÃ©trique pour voir sa dÃ©finition et mÃ©thode de calcul.
            """)

            utility_metrics = calculate_utility_metrics(G_orig, G_anon)

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CAS 1 : SUPER-GRAPHE (GÃ©nÃ©ralisation)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if utility_metrics.get('is_super_graph', False):
                st.info("ğŸ” **Type de graphe** : Super-Graphe (GÃ©nÃ©ralisation) - MÃ©triques adaptÃ©es au format cluster")

                st.markdown("### ğŸ˜ï¸ MÃ©triques de Clustering")

                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("Nombre de Clusters",
                             utility_metrics.get('num_clusters', 'N/A'),
                             help=get_metric_tooltip('num_clusters'))
                with col2:
                    st.metric("Taille Min. Cluster",
                             utility_metrics.get('min_cluster_size', 'N/A'),
                             help=get_metric_tooltip('min_cluster_size'))
                with col3:
                    st.metric("Taille Moy. Cluster",
                             f"{utility_metrics.get('avg_cluster_size', 0):.1f}",
                             help=get_metric_tooltip('avg_cluster_size'))
                with col4:
                    st.metric("Taille Max. Cluster",
                             utility_metrics.get('max_cluster_size', 'N/A'))

                st.markdown("---")
                st.markdown("### ğŸ”— MÃ©triques d'ArÃªtes")

                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric("ArÃªtes Intra-Cluster",
                             utility_metrics.get('intra_cluster_edges', 'N/A'),
                             help=get_metric_tooltip('intra_cluster_edges'))
                with col2:
                    st.metric("ArÃªtes Inter-Cluster",
                             utility_metrics.get('inter_cluster_edges', 'N/A'),
                             help=get_metric_tooltip('inter_cluster_edges'))
                with col3:
                    intra_ratio = utility_metrics.get('intra_ratio', 0)
                    st.metric("Ratio Intra/Total",
                             f"{intra_ratio*100:.1f}%",
                             help=get_metric_tooltip('intra_ratio'))

                st.markdown("---")
                st.markdown("### ğŸ“Š Perte d'Information")

                col1, col2, col3 = st.columns(3)

                with col1:
                    info_loss = utility_metrics.get('information_loss', 0)
                    st.metric("Perte d'Information",
                             f"{info_loss*100:.1f}%",
                             help=get_metric_tooltip('information_loss'))
                with col2:
                    edge_pres = utility_metrics.get('edge_preservation_ratio', 0)
                    st.metric("PrÃ©servation des ArÃªtes",
                             f"{edge_pres*100:.1f}%",
                             help=get_metric_tooltip('edge_preservation_ratio'))
                with col3:
                    super_density = utility_metrics.get('super_graph_density', 0)
                    st.metric("DensitÃ© Super-Graphe",
                             f"{super_density:.3f}",
                             help=get_metric_tooltip('super_graph_density'))

                # Afficher un rÃ©sumÃ© comparatif
                st.markdown("---")
                st.markdown("### ğŸ“‰ Comparaison Original â†” AnonymisÃ©")

                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("**Graphe Original**")
                    st.metric("NÅ“uds", G_orig.number_of_nodes())
                    st.metric("ArÃªtes", G_orig.number_of_edges())

                with col2:
                    st.markdown("**Super-Graphe**")
                    st.metric("Clusters (super-nÅ“uds)", utility_metrics.get('num_clusters', 'N/A'))
                    st.metric("ArÃªtes Totales", utility_metrics.get('num_edges', 'N/A'))

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CAS 2 : GRAPHE CLASSIQUE ou PROBABILISTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            elif utility_metrics.get('comparable', True):
                if utility_metrics.get('is_sample', False):
                    st.info("ğŸ² **Type de graphe** : Ã‰chantillon tirÃ© depuis un graphe probabiliste")

                st.markdown("### ğŸ“Š MÃ©triques de Base")

                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("NÅ“uds",
                             utility_metrics.get('num_nodes', 'N/A'),
                             help=get_metric_tooltip('num_nodes'))
                with col2:
                    st.metric("ArÃªtes",
                             utility_metrics.get('num_edges', 'N/A'),
                             help=get_metric_tooltip('num_edges'))
                with col3:
                    orig_density = nx.density(G_orig)
                    anon_density = utility_metrics.get('density', 0)
                    delta_density = anon_density - orig_density
                    st.metric("DensitÃ©",
                             f"{anon_density:.3f}",
                             delta=f"{delta_density:+.3f}",
                             help=get_metric_tooltip('density'))
                with col4:
                    if utility_metrics.get('avg_clustering') is not None:
                        orig_clust = nx.average_clustering(G_orig)
                        anon_clust = utility_metrics['avg_clustering']
                        delta_clust = anon_clust - orig_clust
                        st.metric("Clustering Moyen",
                                 f"{anon_clust:.3f}",
                                 delta=f"{delta_clust:+.3f}",
                                 help=get_metric_tooltip('avg_clustering'))

                st.markdown("---")
                st.markdown("### ğŸŒ MÃ©triques Globales")

                col1, col2, col3 = st.columns(3)

                with col1:
                    if utility_metrics.get('diameter') is not None:
                        try:
                            if nx.is_connected(G_orig):
                                orig_diam = nx.diameter(G_orig)
                            else:
                                largest_cc = max(nx.connected_components(G_orig), key=len)
                                orig_diam = nx.diameter(G_orig.subgraph(largest_cc))
                            delta_diam = utility_metrics['diameter'] - orig_diam
                            st.metric("DiamÃ¨tre",
                                     utility_metrics['diameter'],
                                     delta=f"{delta_diam:+d}",
                                     help=get_metric_tooltip('diameter'))
                        except:
                            st.metric("DiamÃ¨tre",
                                     utility_metrics['diameter'],
                                     help=get_metric_tooltip('diameter'))

                with col2:
                    if utility_metrics.get('avg_shortest_path') is not None:
                        try:
                            if nx.is_connected(G_orig):
                                orig_asp = nx.average_shortest_path_length(G_orig)
                            else:
                                largest_cc = max(nx.connected_components(G_orig), key=len)
                                orig_asp = nx.average_shortest_path_length(G_orig.subgraph(largest_cc))
                            delta_asp = utility_metrics['avg_shortest_path'] - orig_asp
                            st.metric("Chemin Moyen",
                                     f"{utility_metrics['avg_shortest_path']:.2f}",
                                     delta=f"{delta_asp:+.2f}",
                                     help=get_metric_tooltip('avg_shortest_path'))
                        except:
                            st.metric("Chemin Moyen",
                                     f"{utility_metrics['avg_shortest_path']:.2f}",
                                     help=get_metric_tooltip('avg_shortest_path'))

                with col3:
                    if utility_metrics.get('degree_correlation') is not None:
                        st.metric("CorrÃ©lation des DegrÃ©s",
                                 f"{utility_metrics['degree_correlation']:.3f}",
                                 help=get_metric_tooltip('degree_correlation'))

                st.markdown("---")
                st.markdown("### ğŸ“‰ Trade-off UtilitÃ© vs Modifications")

                metrics = calculate_anonymization_metrics(G_orig, G_anon)

                if metrics:
                    col1, col2 = st.columns(2)

                    with col1:
                        st.markdown("**Modifications des ArÃªtes**")
                        added = metrics.get('edges_added', 0)
                        removed = metrics.get('edges_removed', 0)
                        preserved = metrics.get('edges_preserved', 0)

                        df_edges = pd.DataFrame({
                            'Type': ['PrÃ©servÃ©es', 'AjoutÃ©es', 'SupprimÃ©es'],
                            'Nombre': [preserved, added, removed]
                        })
                        st.bar_chart(df_edges.set_index('Type'))

                    with col2:
                        st.markdown("**Taux de Modification**")
                        rate = metrics.get('modification_rate', 0)
                        st.progress(min(rate, 1.0))
                        st.metric("Taux de modification", f"{rate*100:.1f}%")

                        if rate < 0.1:
                            st.success("âœ… UtilitÃ© trÃ¨s bien prÃ©servÃ©e")
                        elif rate < 0.3:
                            st.info("â„¹ï¸ UtilitÃ© correctement prÃ©servÃ©e")
                        else:
                            st.warning("âš ï¸ Modifications importantes")

            else:
                st.warning("âš ï¸ Type de graphe non reconnu - impossible de calculer les mÃ©triques")

        with tab4:
            st.markdown("## ğŸ”’ MÃ©triques de Privacy")

            st.markdown("""
            Ces mÃ©triques quantifient la **protection de la vie privÃ©e** offerte par l'anonymisation.
            Plus ces valeurs sont Ã©levÃ©es, meilleure est la protection.
            """)

            method_params = st.session_state.get('method_params', {})
            privacy_metrics = calculate_privacy_metrics_separated(G_orig, G_anon, st.session_state.method_key, method_params)

            if privacy_metrics:
                st.markdown("### ğŸ›¡ï¸ Garanties de Privacy")

                if 'k_value' in privacy_metrics:
                    # k-anonymity
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("k requis", privacy_metrics['k_value'])
                    with col2:
                        st.metric("Ensemble d'anonymat min.", privacy_metrics['min_anonymity_set'])
                    with col3:
                        satisfies = privacy_metrics['satisfies_k_anonymity']
                        if satisfies:
                            st.success(f"âœ… {privacy_metrics['k_value']}-anonymitÃ© satisfaite")
                        else:
                            st.error(f"âŒ {privacy_metrics['k_value']}-anonymitÃ© NON satisfaite")

                    st.markdown("---")
                    prob = privacy_metrics['re_identification_probability']
                    st.markdown(f"**ProbabilitÃ© de rÃ©-identification** : {prob:.3f} ({prob*100:.1f}%)")

                    st.progress(1 - prob)

                    if prob < 0.2:
                        st.success("âœ… Risque de rÃ©-identification faible")
                    elif prob < 0.5:
                        st.warning("âš ï¸ Risque de rÃ©-identification modÃ©rÃ©")
                    else:
                        st.error("âŒ Risque de rÃ©-identification Ã©levÃ©")

                elif 'epsilon_budget' in privacy_metrics:
                    # Differential Privacy
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        eps = privacy_metrics['epsilon_budget']
                        st.metric("Îµ (epsilon) Budget", f"{eps:.2f}")

                    with col2:
                        loss = privacy_metrics['privacy_loss_bound']
                        st.metric("Borne de perte de privacy", f"e^{eps:.2f} = {loss:.2f}x")

                    with col3:
                        level = privacy_metrics['privacy_level']
                        if "Forte" in level:
                            st.success(f"âœ… {level}")
                        elif "Moyenne" in level:
                            st.warning(f"âš ï¸ {level}")
                        else:
                            st.error(f"âŒ {level}")

                    st.markdown("---")

                    if 'flip_probability' in privacy_metrics:
                        st.markdown("### ğŸ² EdgeFlip - ParamÃ¨tres de Randomisation")
                        col1, col2 = st.columns(2)

                        with col1:
                            flip_prob = privacy_metrics['flip_probability']
                            st.metric("ProbabilitÃ© de flip", f"{flip_prob:.3f}")

                        with col2:
                            expected_noise = privacy_metrics['expected_noise_edges']
                            st.metric("ArÃªtes bruitÃ©es (attendu)", expected_noise)

                elif 'k_candidates' in privacy_metrics:
                    # Probabilistic
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("k graphes candidats", privacy_metrics['k_candidates'])

                    with col2:
                        st.metric("Îµ tolÃ©rance", f"{privacy_metrics['epsilon_tolerance']:.2f}")

                    with col3:
                        entropy = privacy_metrics['min_entropy']
                        st.metric("Entropie minimale", f"{entropy:.2f}")

                    st.markdown("---")
                    confusion = privacy_metrics['confusion_factor']
                    st.info(f"**Facteur de confusion** : {confusion} graphes plausibles")

                elif 'num_potential_edges' in privacy_metrics:
                    # MaxVar
                    st.markdown("### ğŸ”’ MaxVar - MÃ©triques de Variance et Dispersion")

                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("ArÃªtes potentielles ajoutÃ©es", privacy_metrics['num_potential_edges'])

                    with col2:
                        if 'total_variance' in privacy_metrics:
                            var = privacy_metrics['total_variance']
                            st.metric("Variance totale", f"{var:.2f}",
                                     help="Plus Ã©levÃ©e = meilleure dispersion des probabilitÃ©s")

                    with col3:
                        if 'avg_edge_variance' in privacy_metrics:
                            avg_var = privacy_metrics['avg_edge_variance']
                            st.metric("Variance moyenne/arÃªte", f"{avg_var:.3f}")

                    st.markdown("---")
                    st.markdown("### ğŸ“Š Analyse des ProbabilitÃ©s")

                    col1, col2 = st.columns(2)

                    with col1:
                        st.markdown("**ArÃªtes existantes (originales)**")
                        if 'existing_avg_prob' in privacy_metrics:
                            st.metric("ProbabilitÃ© moyenne", f"{privacy_metrics['existing_avg_prob']:.3f}")
                            if 'existing_std_prob' in privacy_metrics:
                                st.metric("Ã‰cart-type", f"{privacy_metrics['existing_std_prob']:.3f}",
                                         help="Plus Ã©levÃ© = probabilitÃ©s plus dispersÃ©es")

                    with col2:
                        st.markdown("**ArÃªtes potentielles (ajoutÃ©es)**")
                        if 'potential_avg_prob' in privacy_metrics:
                            st.metric("ProbabilitÃ© moyenne", f"{privacy_metrics['potential_avg_prob']:.3f}")
                            if 'potential_std_prob' in privacy_metrics:
                                st.metric("Ã‰cart-type", f"{privacy_metrics['potential_std_prob']:.3f}")

                    st.markdown("---")
                    st.markdown("### ğŸ›¡ï¸ RÃ©sistance au Seuillage")

                    if 'threshold_resistance' in privacy_metrics:
                        resistance = privacy_metrics['threshold_resistance']
                        reconstruction = privacy_metrics.get('reconstruction_rate', 0)

                        col1, col2 = st.columns(2)

                        with col1:
                            st.metric("Taux de rÃ©sistance", f"{resistance*100:.1f}%",
                                     help="% d'arÃªtes originales non rÃ©cupÃ©rables par seuillage Ã  0.5")

                        with col2:
                            st.metric("Taux de reconstruction", f"{reconstruction*100:.1f}%",
                                     help="% d'arÃªtes originales rÃ©cupÃ©rables par seuillage Ã  0.5")

                        st.progress(resistance)

                        if resistance > 0.2:
                            st.success(f"âœ… Bonne rÃ©sistance au seuillage ({resistance*100:.1f}%)")
                        elif resistance > 0.1:
                            st.warning(f"âš ï¸ RÃ©sistance modÃ©rÃ©e ({resistance*100:.1f}%)")
                        else:
                            st.error(f"âŒ Faible rÃ©sistance - vulnÃ©rable au seuillage ({resistance*100:.1f}%)")

                        st.caption("ğŸ’¡ Un attaquant qui applique un seuil Ã  0.5 ne rÃ©cupÃ¨re que "
                                  f"{reconstruction*100:.1f}% des arÃªtes originales (contre 100% pour (k,Îµ)-obf)")

                elif 'min_cluster_size' in privacy_metrics:
                    # Generalization
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("Taille min. cluster", int(privacy_metrics['min_cluster_size']))

                    with col2:
                        st.metric("Taille moy. cluster", f"{privacy_metrics['avg_cluster_size']:.1f}")

                    with col3:
                        max_priv = privacy_metrics['max_privacy']
                        st.metric("Prob. max rÃ©-identification", f"{max_priv:.3f}")

                st.markdown("---")

                # Garanties globales
                guarantees = calculate_privacy_guarantees(G_orig, G_anon, st.session_state.method_key, method_params)

                if guarantees:
                    st.markdown("### ğŸ“‹ Garanties DÃ©taillÃ©es")

                    with st.expander("Voir toutes les garanties"):
                        for key, value in guarantees.items():
                            st.text(f"{key}: {value}")

            else:
                st.info("Aucune mÃ©trique de privacy spÃ©cifique pour cette mÃ©thode")

        with tab5:
            st.markdown("## ğŸ¯ Simulations d'Attaques RÃ©elles")

            st.markdown("""
            Cette section simule des attaques de **rÃ©-identification** sur le graphe anonymisÃ©.
            Ces simulations montrent concrÃ¨tement si un adversaire peut retrouver des nÅ“uds spÃ©cifiques.
            """)

            st.markdown("---")

            # SÃ©lection du nÅ“ud cible
            st.markdown("### ğŸ¯ Configuration de l'Attaque")

            col1, col2 = st.columns(2)

            with col1:
                target_node = st.number_input(
                    "NÅ“ud cible Ã  retrouver",
                    min_value=0,
                    max_value=G_orig.number_of_nodes()-1,
                    value=0,
                    help="Le nÅ“ud que l'adversaire essaie de rÃ©-identifier"
                )

            with col2:
                attack_type = st.selectbox(
                    "Type d'attaque",
                    ["Degree Attack", "Subgraph Attack (Triangles)"]
                )

            st.markdown("---")

            if st.button("ğŸš€ Lancer l'Attaque"):
                st.markdown("### ğŸ“Š RÃ©sultats de l'Attaque")

                with st.spinner("Simulation en cours..."):
                    if attack_type == "Degree Attack":
                        results = simulate_degree_attack(G_orig, G_anon, target_node)
                    else:
                        results = simulate_subgraph_attack(G_orig, G_anon, target_node)

                if results['success']:
                    st.error("### âš ï¸ Attaque RÃ©ussie !")
                    st.markdown(results['explanation'])

                    st.markdown(f"**NÅ“ud rÃ©-identifiÃ©** : {results.get('re_identified_node', 'N/A')}")

                else:
                    st.success("### âœ… Attaque Ã‰chouÃ©e / Partiellement RÃ©ussie")
                    st.markdown(results['explanation'])

                st.markdown("---")
                st.markdown("### ğŸ“ˆ DÃ©tails Techniques")

                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("**NÅ“ud cible** :")
                    st.info(f"NÅ“ud {target_node}")

                    if 'target_degree' in results:
                        st.markdown("**DegrÃ© du nÅ“ud** :")
                        st.info(f"DegrÃ© = {results['target_degree']}")

                    if 'target_triangles' in results:
                        st.markdown("**Triangles** :")
                        st.info(f"{results['target_triangles']} triangles")

                with col2:
                    st.markdown("**Candidats trouvÃ©s** :")
                    if results['candidates']:
                        st.info(f"{len(results['candidates'])} nÅ“uds : {results['candidates'][:10]}")
                    else:
                        st.info("Aucun candidat")

                    if len(results['candidates']) > 1:
                        prob_success = 1 / len(results['candidates'])
                        st.markdown("**ProbabilitÃ© de succÃ¨s** :")
                        st.warning(f"{prob_success*100:.1f}%")

            st.markdown("---")

            # Section Ã©ducative
            with st.expander("ğŸ“š En savoir plus sur ces attaques"):
                st.markdown("""
                ### Degree Attack (Attaque par DegrÃ©)

                L'adversaire connaÃ®t le degrÃ© (nombre de connexions) du nÅ“ud cible et cherche
                dans le graphe anonymisÃ© tous les nÅ“uds ayant ce degrÃ©.

                **Protection** :
                - k-degree anonymity garantit au moins k nÅ“uds par degrÃ©
                - Randomisation modifie les degrÃ©s
                - Differential Privacy ajoute du bruit

                ### Subgraph Attack (Attaque par Sous-graphe)

                L'adversaire connaÃ®t la structure locale autour du nÅ“ud (ex: triangles, motifs).
                Cette attaque est plus puissante car elle exploite plus d'information.

                **Protection** :
                - Generalization dÃ©truit les motifs locaux
                - Differential Privacy ajoute/supprime des triangles fictifs
                - Randomisation casse certains motifs
                """)

        with tab6:
            st.markdown(f"## ğŸ›¡ï¸ Attaques et Garanties : {current_method['name']}")

            method_details = ATTACKS_AND_GUARANTEES.get(st.session_state.method_key, {})

            if method_details:
                # Attaques protÃ©gÃ©es
                st.markdown("### âœ… Attaques contre lesquelles la mÃ©thode protÃ¨ge")
                attacks_protected = method_details.get('attacks_protected', [])
                for attack in attacks_protected:
                    with st.expander(f"ğŸ›¡ï¸ {attack['name']}", expanded=False):
                        st.markdown(attack['description'])

                # Attaques vulnÃ©rables
                st.markdown("---")
                st.markdown("### âš ï¸ VulnÃ©rabilitÃ©s et Limitations")
                attacks_vulnerable = method_details.get('attacks_vulnerable', [])
                for attack in attacks_vulnerable:
                    with st.expander(f"ğŸš¨ {attack['name']}", expanded=False):
                        st.markdown(attack['description'])

                # Avantages
                st.markdown("---")
                st.markdown("### âœ… Avantages de la MÃ©thode")
                advantages = method_details.get('advantages', [])
                for adv in advantages:
                    st.markdown(adv)

                # InconvÃ©nients
                st.markdown("---")
                st.markdown("### âŒ InconvÃ©nients et Limitations")
                disadvantages = method_details.get('disadvantages', [])
                for dis in disadvantages:
                    st.markdown(dis)

                # Exemple Karate
                st.markdown("---")
                st.markdown("### ğŸ¥‹ Exemple Concret : Graphe Karate Club")
                karate_example = method_details.get('karate_example', '')
                if karate_example:
                    st.markdown(karate_example)
                else:
                    st.info("Exemple Ã  venir pour cette mÃ©thode.")
            else:
                st.warning("Informations dÃ©taillÃ©es non disponibles pour cette mÃ©thode.")

        with tab7:
            st.markdown("## ğŸ“š Dictionnaire des Attaques de RÃ©-Identification")

            st.markdown("""
            Ce dictionnaire prÃ©sente **toutes les attaques connues** contre les graphes anonymisÃ©s,
            avec des exemples concrets et des explications dÃ©taillÃ©es.
            """)

            st.markdown("---")

            # Liste des attaques
            attack_names = [ATTACKS_DICTIONARY[k]['name'] for k in ATTACKS_DICTIONARY.keys()]

            selected_attack_name = st.selectbox(
                "Choisir une attaque Ã  explorer",
                attack_names
            )

            # Trouver l'attaque correspondante
            selected_attack_key = list(ATTACKS_DICTIONARY.keys())[attack_names.index(selected_attack_name)]
            attack = ATTACKS_DICTIONARY[selected_attack_key]

            st.markdown(f"### {attack['name']}")

            col1, col2 = st.columns([2, 1])

            with col1:
                with st.expander("ğŸ“ Description de l'Attaque", expanded=True):
                    st.markdown(attack['description'])

                with st.expander("ğŸ’¡ Exemple Concret"):
                    st.markdown(attack['example'])

            with col2:
                st.markdown("**âš ï¸ SÃ©vÃ©ritÃ©**")
                severity = attack['severity']
                if "TrÃ¨s Ã©levÃ©e" in severity or "Ã‰levÃ©e" in severity:
                    st.error(severity)
                elif "Moyenne" in severity:
                    st.warning(severity)
                else:
                    st.info(severity)

                st.markdown("**ğŸ›¡ï¸ Protection**")
                st.success(attack['protection'])

            st.markdown("---")

            # Exemples concrets sur Karate Club
            st.markdown("### ğŸ¥‹ Exemples Concrets sur Karate Club")

            example_keys = list(CONCRETE_ATTACK_EXAMPLES.keys())

            for example_key in example_keys:
                example = CONCRETE_ATTACK_EXAMPLES[example_key]

                with st.expander(f"ğŸ“– {example['title']}"):
                    st.markdown(f"**ScÃ©nario** : {example['scenario']}")

                    st.markdown("**Ã‰tapes de l'attaque** :")
                    for step in example['steps']:
                        st.markdown(f"- {step}")

                    st.markdown("---")
                    st.markdown("**Taux de SuccÃ¨s** :")

                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("Sans protection", example.get('success_rate_no_protection', 'N/A'))

                    with col2:
                        if 'success_rate_k_anonymity' in example:
                            st.metric("Avec k-anonymity", example['success_rate_k_anonymity'])
                        elif 'success_rate_randomization' in example:
                            st.metric("Avec randomization", example['success_rate_randomization'])

                    with col3:
                        if 'success_rate_differential_privacy' in example:
                            st.metric("Avec Diff. Privacy", example['success_rate_differential_privacy'])
                        elif 'success_rate_generalization' in example:
                            st.metric("Avec Generalization", example['success_rate_generalization'])

                    if 'code_simulation' in example:
                        with st.expander("ğŸ’» Code de Simulation"):
                            st.code(example['code_simulation'], language='python')

        with tab8:
            st.markdown("## ğŸ” Dictionnaire des PropriÃ©tÃ©s de Graphes")

            st.markdown("""
            Ce dictionnaire explique **toutes les propriÃ©tÃ©s de graphes** utilisÃ©es en anonymisation,
            leur importance pour l'utilitÃ©, et leur risque pour la privacy.
            """)

            st.markdown("---")

            # Liste des propriÃ©tÃ©s
            property_names = [GRAPH_PROPERTIES[k]['name'] for k in GRAPH_PROPERTIES.keys()]

            selected_property_name = st.selectbox(
                "Choisir une propriÃ©tÃ© Ã  explorer",
                property_names
            )

            # Trouver la propriÃ©tÃ© correspondante
            selected_property_key = list(GRAPH_PROPERTIES.keys())[property_names.index(selected_property_name)]
            prop = GRAPH_PROPERTIES[selected_property_key]

            st.markdown(f"### {prop['name']}")

            col1, col2 = st.columns(2)

            with col1:
                with st.expander("ğŸ“ DÃ©finition", expanded=True):
                    st.markdown(prop['definition'])

                with st.expander("ğŸ”¢ Formule"):
                    st.latex(prop['formula'])

                with st.expander("ğŸ’¡ Exemple"):
                    st.info(prop['example'])

            with col2:
                st.markdown("**ğŸ“Š Importance pour l'UtilitÃ©**")
                importance = prop['utility_importance']
                if "Critique" in importance or "Ã‰levÃ©e" in importance:
                    st.success(importance)
                else:
                    st.info(importance)

                st.markdown("**âš ï¸ Risque pour la Privacy**")
                risk = prop['privacy_risk']
                if "Ã‰levÃ©" in risk:
                    st.error(risk)
                elif "Moyen" in risk:
                    st.warning(risk)
                else:
                    st.success(risk)

            st.markdown("---")

            # Calcul des propriÃ©tÃ©s sur le graphe actuel
            if isinstance(G_anon, nx.Graph):
                st.markdown("### ğŸ“Š Valeurs pour le Graphe Actuel")

                try:
                    if selected_property_key == 'degree':
                        degrees = dict(G_anon.degree())
                        st.metric("DegrÃ© moyen", f"{np.mean(list(degrees.values())):.2f}")
                        st.metric("DegrÃ© max", max(degrees.values()))

                    elif selected_property_key == 'clustering_coefficient':
                        clustering = nx.average_clustering(G_anon)
                        st.metric("Coefficient de clustering moyen", f"{clustering:.3f}")

                    elif selected_property_key == 'density':
                        density = nx.density(G_anon)
                        st.metric("DensitÃ©", f"{density:.3f}")

                    elif selected_property_key == 'diameter':
                        if nx.is_connected(G_anon):
                            diameter = nx.diameter(G_anon)
                            st.metric("DiamÃ¨tre", diameter)
                        else:
                            st.info("Graphe non connexe, diamÃ¨tre non dÃ©fini")

                    elif selected_property_key == 'average_path_length':
                        if nx.is_connected(G_anon):
                            apl = nx.average_shortest_path_length(G_anon)
                            st.metric("Longueur moyenne des chemins", f"{apl:.2f}")
                        else:
                            st.info("Graphe non connexe, calculÃ© sur la plus grande composante")

                except Exception as e:
                    st.warning(f"Calcul non disponible pour ce graphe")

    else:
        st.info("ğŸ‘ˆ SÃ©lectionnez une mÃ©thode et cliquez sur 'Anonymiser le Graphe' pour commencer")

        # Afficher un aperÃ§u du graphe original
        st.markdown("### ğŸ“Š AperÃ§u du Graphe Original")

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("NÅ“uds", G.number_of_nodes())
        with col2:
            st.metric("ArÃªtes", G.number_of_edges())
        with col3:
            st.metric("DegrÃ© Moyen", f"{sum(d for n, d in G.degree()) / G.number_of_nodes():.2f}")

        fig, ax = plt.subplots(figsize=(10, 8))
        pos = nx.spring_layout(G, seed=42, k=0.5, iterations=50)
        nx.draw_networkx_nodes(G, pos, ax=ax, node_color='lightblue', node_size=500, alpha=0.9)
        nx.draw_networkx_edges(G, pos, ax=ax, edge_color='gray', width=1.5, alpha=0.6)
        nx.draw_networkx_labels(G, pos, ax=ax, font_size=8, font_weight='bold')
        ax.set_title('Graphe Original', fontsize=16, fontweight='bold')
        ax.axis('off')
        st.pyplot(fig)


if __name__ == "__main__":
    main()
